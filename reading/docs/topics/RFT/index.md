# Reinforcement Learning & Fine-Tuning

Deep dive into RLHF (Reinforcement Learning from Human Feedback) and efficient fine-tuning techniques for LLMs.

## Contents

- **[RLHF Overview](notes.md)** - Process, benefits, and challenges of reinforcement learning for language model fine-tuning
- **[Paper Reviews](paper-review.md)** - LIMA paper on alignment with minimal data

---

*Explore alignment techniques, RLHF processes, and the surprising effectiveness of small, high-quality datasets.*
