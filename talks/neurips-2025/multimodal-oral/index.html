<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Personal learning notes on Large Language Models, AI Agents, RAG, and more"><meta name=author content="Jackie Yin"><link href=https://jackie-jiaqi-yin.github.io/llm-learning-journey/talks/neurips-2025/multimodal-oral/ rel=canonical><link href=../workshop-dl4c/ rel=prev><link href=../../../courses/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.0"><title>Multimodal Oral Session - LLM Learning Journey</title><link rel=stylesheet href=../../../assets/stylesheets/main.618322db.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../css/timeago.css><link rel=stylesheet href=../../../stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#multimodal-oral class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="LLM Learning Journey" class="md-header__button md-logo" aria-label="LLM Learning Journey" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 2a2 2 0 0 1 2 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 0 1 7 7h1a1 1 0 0 1 1 1v3a1 1 0 0 1-1 1h-1v1a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-1H2a1 1 0 0 1-1-1v-3a1 1 0 0 1 1-1h1a7 7 0 0 1 7-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 0 1 2-2M7.5 13A2.5 2.5 0 0 0 5 15.5 2.5 2.5 0 0 0 7.5 18a2.5 2.5 0 0 0 2.5-2.5A2.5 2.5 0 0 0 7.5 13m9 0a2.5 2.5 0 0 0-2.5 2.5 2.5 2.5 0 0 0 2.5 2.5 2.5 2.5 0 0 0 2.5-2.5 2.5 2.5 0 0 0-2.5-2.5"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> LLM Learning Journey </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Multimodal Oral Session </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/jackie-jiaqi-yin/llm-learning-journey title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> llm-learning-journey </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../../../topics/ class=md-tabs__link> Topics </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Talks & Seminars </a> </li> <li class=md-tabs__item> <a href=../../../courses/ class=md-tabs__link> Online Courses </a> </li> <li class=md-tabs__item> <a href=../../../tags/ class=md-tabs__link> Tags </a> </li> <li class=md-tabs__item> <a href=../../../about/ class=md-tabs__link> About </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="LLM Learning Journey" class="md-nav__button md-logo" aria-label="LLM Learning Journey" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 2a2 2 0 0 1 2 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 0 1 7 7h1a1 1 0 0 1 1 1v3a1 1 0 0 1-1 1h-1v1a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-1H2a1 1 0 0 1-1-1v-3a1 1 0 0 1 1-1h1a7 7 0 0 1 7-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 0 1 2-2M7.5 13A2.5 2.5 0 0 0 5 15.5 2.5 2.5 0 0 0 7.5 18a2.5 2.5 0 0 0 2.5-2.5A2.5 2.5 0 0 0 7.5 13m9 0a2.5 2.5 0 0 0-2.5 2.5 2.5 2.5 0 0 0 2.5 2.5 2.5 2.5 0 0 0 2.5-2.5 2.5 2.5 0 0 0-2.5-2.5"/></svg> </a> LLM Learning Journey </label> <div class=md-nav__source> <a href=https://github.com/jackie-jiaqi-yin/llm-learning-journey title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> llm-learning-journey </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=../../../topics/ class="md-nav__link "> <span class=md-ellipsis> Topics </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Topics </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_2> <div class="md-nav__link md-nav__container"> <a href=../../../topics/AI-Agent/ class="md-nav__link "> <span class=md-ellipsis> AI Agents </span> </a> <label class="md-nav__link " for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> AI Agents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../topics/AI-Agent/notes/ class=md-nav__link> <span class=md-ellipsis> Overview & Frameworks </span> </a> </li> <li class=md-nav__item> <a href=../../../topics/AI-Agent/paper-review/ class=md-nav__link> <span class=md-ellipsis> Paper Reviews </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_3> <div class="md-nav__link md-nav__container"> <a href=../../../topics/RAG/ class="md-nav__link "> <span class=md-ellipsis> RAG (Retrieval-Augmented Generation) </span> </a> <label class="md-nav__link " for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> RAG (Retrieval-Augmented Generation) </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../topics/RAG/notes/ class=md-nav__link> <span class=md-ellipsis> RAG Guide & Best Practices </span> </a> </li> <li class=md-nav__item> <a href=../../../topics/RAG/paper-review/ class=md-nav__link> <span class=md-ellipsis> Paper Reviews </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_4> <div class="md-nav__link md-nav__container"> <a href=../../../topics/RFT/ class="md-nav__link "> <span class=md-ellipsis> Reinforcement Learning & Fine-Tuning </span> </a> <label class="md-nav__link " for=__nav_2_4 id=__nav_2_4_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Reinforcement Learning & Fine-Tuning </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../topics/RFT/notes/ class=md-nav__link> <span class=md-ellipsis> RLHF Overview </span> </a> </li> <li class=md-nav__item> <a href=../../../topics/RFT/paper-review/ class=md-nav__link> <span class=md-ellipsis> Paper Reviews </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_5> <div class="md-nav__link md-nav__container"> <a href=../../../topics/Prompt/ class="md-nav__link "> <span class=md-ellipsis> Prompt Engineering </span> </a> <label class="md-nav__link " for=__nav_2_5 id=__nav_2_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_5_label aria-expanded=false> <label class=md-nav__title for=__nav_2_5> <span class="md-nav__icon md-icon"></span> Prompt Engineering </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../topics/Prompt/notes/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering Guide </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_6> <div class="md-nav__link md-nav__container"> <a href=../../../topics/evaluation/ class="md-nav__link "> <span class=md-ellipsis> Evaluation </span> </a> <label class="md-nav__link " for=__nav_2_6 id=__nav_2_6_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_6_label aria-expanded=false> <label class=md-nav__title for=__nav_2_6> <span class="md-nav__icon md-icon"></span> Evaluation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../topics/evaluation/notes/ class=md-nav__link> <span class=md-ellipsis> Evaluation Methods & Metrics </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_7> <div class="md-nav__link md-nav__container"> <a href=../../../topics/compressing/ class="md-nav__link "> <span class=md-ellipsis> Model Compression </span> </a> <label class="md-nav__link " for=__nav_2_7 id=__nav_2_7_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_7_label aria-expanded=false> <label class=md-nav__title for=__nav_2_7> <span class="md-nav__icon md-icon"></span> Model Compression </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../topics/compressing/notes/ class=md-nav__link> <span class=md-ellipsis> Compression Techniques </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_8> <div class="md-nav__link md-nav__container"> <a href=../../../topics/RecSys/ class="md-nav__link "> <span class=md-ellipsis> Recommendation Systems </span> </a> <label class="md-nav__link " for=__nav_2_8 id=__nav_2_8_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_8_label aria-expanded=false> <label class=md-nav__title for=__nav_2_8> <span class="md-nav__icon md-icon"></span> Recommendation Systems </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../topics/RecSys/notes/ class=md-nav__link> <span class=md-ellipsis> RecSys Notes </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_9> <div class="md-nav__link md-nav__container"> <a href=../../../topics/Others/ class="md-nav__link "> <span class=md-ellipsis> Others </span> </a> <label class="md-nav__link " for=__nav_2_9 id=__nav_2_9_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_9_label aria-expanded=false> <label class=md-nav__title for=__nav_2_9> <span class="md-nav__icon md-icon"></span> Others </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../topics/Others/notes/ class=md-nav__link> <span class=md-ellipsis> Miscellaneous Notes </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Talks & Seminars </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Talks & Seminars </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_2 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> NeurIPS 2025 </span> </a> <label class="md-nav__link " for=__nav_3_2 id=__nav_3_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=true> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> NeurIPS 2025 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2_2> <label class=md-nav__link for=__nav_3_2_2 id=__nav_3_2_2_label tabindex=0> <span class=md-ellipsis> Invited Talks </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2_2> <span class="md-nav__icon md-icon"></span> Invited Talks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../invited-talk-cognitive-capability-evaluation/ class=md-nav__link> <span class=md-ellipsis> Cognitive Capability Evaluation </span> </a> </li> <li class=md-nav__item> <a href=../invited-talk-ai-reasoning/ class=md-nav__link> <span class=md-ellipsis> The Art of Reasoning </span> </a> </li> <li class=md-nav__item> <a href=../Invited-talk-problem-finding-in-ai/ class=md-nav__link> <span class=md-ellipsis> Problem Finding in AI </span> </a> </li> <li class=md-nav__item> <a href=../invited-talk-wrong-nightmares-ai/ class=md-nav__link> <span class=md-ellipsis> Wrong Nightmares About AI </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2_3> <label class=md-nav__link for=__nav_3_2_3 id=__nav_3_2_3_label tabindex=0> <span class=md-ellipsis> Tutorials </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2_3> <span class="md-nav__icon md-icon"></span> Tutorials </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../xAI/ class=md-nav__link> <span class=md-ellipsis> Explainable AI (xAI) </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2_4> <label class=md-nav__link for=__nav_3_2_4 id=__nav_3_2_4_label tabindex=0> <span class=md-ellipsis> Panels & Workshops </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2_4> <span class="md-nav__icon md-icon"></span> Panels & Workshops </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../rai-pannel/ class=md-nav__link> <span class=md-ellipsis> Responsible AI & Unlearning </span> </a> </li> <li class=md-nav__item> <a href=../agentic-development-frontier/ class=md-nav__link> <span class=md-ellipsis> Agentic Development </span> </a> </li> <li class=md-nav__item> <a href=../workshop-dl4c/ class=md-nav__link> <span class=md-ellipsis> Deep Learning for Coding </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_2_5 checked> <label class=md-nav__link for=__nav_3_2_5 id=__nav_3_2_5_label tabindex=0> <span class=md-ellipsis> Oral Presentations </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_3_2_5_label aria-expanded=true> <label class=md-nav__title for=__nav_3_2_5> <span class="md-nav__icon md-icon"></span> Oral Presentations </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Multimodal Oral Session </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Multimodal Oral Session </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#dynam3d class=md-nav__link> <span class=md-ellipsis> Dynam3D </span> </a> </li> <li class=md-nav__item> <a href=#perception-encoder-the-best-visual-embeddings-are-not-at-the-output-of-the-network class=md-nav__link> <span class=md-ellipsis> Perception Encoder: The best visual embeddings are not at the output of the network </span> </a> </li> <li class=md-nav__item> <a href=#interactive-cross-modal-learning-for-text-3d-scene-retrieval class=md-nav__link> <span class=md-ellipsis> Interactive Cross-modal Learning for Text-3D Scene Retrieval </span> </a> </li> <li class=md-nav__item> <a href=#coralvqa-a-large-scale-visual-question-answering-dataset-for-coral-reef-image-understanding class=md-nav__link> <span class=md-ellipsis> CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding </span> </a> </li> <li class=md-nav__item> <a href=#openhoi-open-world-hand-object-interaction-synthesis-with-multimodal-large-language-model class=md-nav__link> <span class=md-ellipsis> OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_4> <div class="md-nav__link md-nav__container"> <a href=../../../courses/ class="md-nav__link "> <span class=md-ellipsis> Online Courses </span> </a> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Online Courses </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../tags/ class=md-nav__link> <span class=md-ellipsis> Tags </span> </a> </li> <li class=md-nav__item> <a href=../../../about/ class=md-nav__link> <span class=md-ellipsis> About </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <nav class=md-path aria-label=Navigation> <ol class=md-path__list> <li class=md-path__item> <a href=../../.. class=md-path__link> <span class=md-ellipsis> Home </span> </a> </li> <li class=md-path__item> <a href=../../ class=md-path__link> <span class=md-ellipsis> Talks & Seminars </span> </a> </li> <li class=md-path__item> <a href=../ class=md-path__link> <span class=md-ellipsis> NeurIPS 2025 </span> </a> </li> <li class=md-path__item> <a href=./ class=md-path__link> <span class=md-ellipsis> Oral Presentations </span> </a> </li> </ol> </nav> <article class="md-content__inner md-typeset"> <h1 id=multimodal-oral>Multimodal Oral<a class=headerlink href=#multimodal-oral title="Permanent link">&para;</a></h1> <h2 id=dynam3d><strong>Dynam3D</strong><a class=headerlink href=#dynam3d title="Permanent link">&para;</a></h2> <p><strong>Dynam3D</strong></p> <p><a href=https://github.com/MrZihan/Dynam3D>GitHub Repository</a></p> <p>Slides: </p> <p>Vision and Language Navigation (VLN) - agents follow natural language instructions to navigate 3D environments</p> <p><strong>Existing approaches</strong></p> <p><strong>The Video Tape Approach</strong></p> <ul> <li>Treats world as stream of frames processed by video VLM</li> <li>Issues:<ul> <li>Spatial amnesia - forgets objects once they leave frame</li> <li>Geometry blindness - 2D video lacks explicit 3D structure, leads to poor planning and collisions</li> </ul> </li> </ul> <p><strong>The Frozen Map Approach</strong></p> <ul> <li>Builds explicit 3D map and reasons with 3D VLM</li> <li>Issues:<ul> <li>Assumes static world - fails when objects move</li> <li>Granularity efficiency trade-off - dense maps too slow, sparse maps lose semantic details</li> </ul> </li> </ul> <p><strong>Five Core Requirements for Good VLN Model:</strong></p> <ol> <li>Explicit 3D structure (not just 2D appearances)</li> <li>Multiple semantic scales (fine details to objects and rooms)</li> <li>Compact enough to fit VLM context window</li> <li>Update online as world changes</li> <li>Bind language directly to 3D objects and regions</li> </ol> <p><strong>Semantic Pyramid Tokenization</strong></p> <p>Compresses millions of dense 3D feature points into manageable token set with three-level hierarchy:</p> <p><strong>Patch Level:</strong></p> <ul> <li>Lifts CLIP patch features into 3D using DAPF</li> <li>Preserves fine-grained texture, edges, precise geometry</li> <li>~557 tokens per image</li> </ul> <p><strong>Instance Level:</strong></p> <ul> <li>Groups patch features using FastSAM mask</li> <li>Aggregates into persistent 3D object instances</li> <li>Reduces to ~300 tokens</li> <li>Tracks objects instead of fixed points</li> </ul> <p><strong>Zone Level:</strong></p> <ul> <li>Divides 3D space into uniform cubic zones</li> <li>Aggregates instances within zones into room-level tokens</li> <li>Final count: just few thousand tokens</li> </ul> <p><strong>Online 3D Instance Construction</strong></p> <p><strong>Process:</strong></p> <ol> <li>Generate 2D instances using FastSAM and instance encoder</li> <li>Project into 3D and compare against existing instance memory</li> <li>Get top K 3D instance matches based on feature distances</li> <li>Pass candidates through learned merging discriminator<ul> <li>Selects best 2D to 3D matches using feature similarity and geometric distance</li> <li>If object exists: update instance in 3D memory</li> <li>If new: create new 3D instance</li> </ul> </li> </ol> <p><strong>Result:</strong> Persistent, compact, language-grounded object memory that updates continuously</p> <p><strong>Adapt to the Dynamic World</strong></p> <p><strong>Dynamic Frustum Coordinating:</strong></p> <ul> <li>Adds new features when surfaces become newly visible</li> <li>Removes outdated features based on camera frustums and depth when objects move</li> <li>Prevents long-term accumulation of outdated information</li> <li>Keeps 3D memory physically consistent over time</li> <li>Essential for handling moving objects in real environments</li> </ul> <p><strong>Dataset for Training</strong></p> <p><strong>Scale:</strong></p> <ul> <li>1,800+ object categories</li> <li>5,000+ 3D scenes</li> <li>2 million language descriptions</li> <li>Meticulously curated over multiple large 3D datasets</li> <li>Diversity key for generalization</li> </ul> <p><strong>Contrastive Learning for Semantic Alignment</strong></p> <p><strong>Three Complementary Losses:</strong></p> <ol> <li><strong>Segmentation Loss:</strong> Ensures accurate 2D to 3D instance grouping</li> <li><strong>Distillation Loss:</strong> Transfers CLIP knowledge into 3D instance and zone tokens</li> <li><strong>Alignment Loss:</strong> Directly grounds 3D tokens to natural language descriptions</li> </ol> <p><strong>Multi-view Consistency:</strong></p> <ul> <li>Contrastively aligns features of same 3D instances across different viewpoints</li> <li>Pulls same instance features together, pushes different instances apart</li> <li>Produces view-invariant 3D embeddings crucial for stable reasoning as camera moves</li> </ul> <p><strong>Multimodal Reasoning and Action Prediction</strong></p> <p><strong>Architecture:</strong></p> <ul> <li>Patch, instance, zone tokens + natural language instruction + action history fed into lightweight 3.8B parameter VLM</li> <li>VLM fine-tuned to output continuous navigation actions (turning, moving forward, stopping)</li> <li>VLM reasons over structured dynamic 3D token memory instead of raw pixels</li> </ul> <p><strong>Local Minima Mitigation:</strong></p> <ul> <li>Since no global map maintained, planner susceptible to local minima</li> <li>Maintains historical record of robot actions to mitigate this issue</li> </ul> <p><strong>Evaluation Results:</strong></p> <ul> <li>Tested on three challenging VLN benchmarks</li> <li>Consistently outperforms video-based and map-based baselines in:<ul> <li>Navigation success</li> <li>Path efficiency</li> <li>Robustness to freeform instructions</li> </ul> </li> <li>Successfully deployed on real robot in indoor environment</li> <li>Continues operating correctly even when objects move during execution</li> </ul> <p><strong>Conclusion</strong></p> <p>Dynam3D provides hierarchical, compact and dynamic 3D memory that is:</p> <ul> <li>Explicitly geometrical</li> <li>Continuously updated</li> <li>Directly grounded in language</li> <li>Key towards scalable embodied reasoning in real world Vision and language navigation (VLN)</li> </ul> <h2 id=perception-encoder-the-best-visual-embeddings-are-not-at-the-output-of-the-network><strong>Perception Encoder: The best visual embeddings are not at the output of the network</strong><a class=headerlink href=#perception-encoder-the-best-visual-embeddings-are-not-at-the-output-of-the-network title="Permanent link">&para;</a></h2> <p><a href=https://arxiv.org/abs/2504.13181>Perception Encoder Paper</a></p> <p><strong>PE Core</strong></p> <ul> <li>State-of-the-art image and video joint CLIP model</li> <li>Built by making CLIP recipe robust through eliminating shortcuts<ul> <li>Varied resolution during training to prevent overfitting</li> <li>Increased batch size for more hard negatives</li> <li>Improvements plateaued on ImageNet validation but significantly boosted robustness metrics</li> </ul> </li> <li>Extended to video using synthetic data engine<ul> <li>Used image-only model as frame-based encoder</li> <li>Built custom Perception LLM for video captioning</li> <li>Video fine-tuning surprisingly improved image performance too</li> </ul> </li> <li>Outperforms SigLIP-2 at all model scales on classification, fine-grained classification, and retrieval</li> <li>Demonstrates superior compositionality and robustness compared to CLIP-L<ul> <li>Better understanding of complex queries like “orange berries” and “blue street sign with white text”</li> <li>More robust to image variations (night vision example with raccoons vs opossums)</li> </ul> </li> </ul> <p><strong>PE Lang (alignment method)</strong></p> <ul> <li>Addresses problem: PE Core has strong language features in intermediate layers but poor last-layer performance</li> <li>Solution: Fine-tune Perception LM on top of PE model for 60M samples</li> <li>Results in state-of-the-art multimodal LLM performance<ul> <li>Stronger than InternVL-1.5 and Qwen2-VL at time of publication</li> <li>Successfully aligns language features to final layer</li> </ul> </li> <li>Enables strong performance on OCR QA and visual QA tasks</li> </ul> <p><strong>PE Spatial</strong></p> <ul> <li>Challenge: Spatial features degraded at last layer due to global tokens appearing after layer 32</li> <li>Solution: Self-distillation approach<ul> <li>Distill model to itself at earlier layer (~layer 40)</li> <li>Use SAM mask logic features for enhanced locality</li> <li>Combines semantic understanding with clean spatial features</li> </ul> </li> <li>Achieves state-of-the-art COCO detection performance</li> <li>Qualitative results show clean, semantic part-level similarities</li> <li>Addresses dense prediction tasks like detection, tracking, depth estimation</li> </ul> <h2 id=interactive-cross-modal-learning-for-text-3d-scene-retrieval><strong>Interactive Cross-modal Learning for Text-3D Scene Retrieval</strong><a class=headerlink href=#interactive-cross-modal-learning-for-text-3d-scene-retrieval title="Permanent link">&para;</a></h2> <p><strong>Overview</strong></p> <ul> <li>IDeal: Interactive Text-3D Scene Retrieval method enhancing alignment between text queries and 3D scenes through continuous interaction</li> <li>Four main contributions:<ol> <li>Interactive text-3D retrieval method with active alignment enhancement</li> <li>Interactive Retrieval Refinement (IRR) framework supporting structural interaction</li> <li>Interaction Adaptation Tuning (IAT) strategy</li> <li>Comprehensive experimental validation demonstrating superiority</li> </ol> </li> <li><a href=https://arxiv.org/pdf/2502.19128>Paper</a> available.</li> </ul> <p><strong>Open World Obstacles</strong></p> <ul> <li>Incomplete one-shot descriptions fail to capture user intent<ul> <li>Single shot queries provide limited scene summaries</li> <li>Lead to mismatches between queries and retrieval results</li> </ul> </li> <li>Domain shifts across users with different languages, regional experiences, semantic skills<ul> <li>Models trained on one text type struggle to generalize to other niches</li> </ul> </li> <li>Ambiguous/unspecific descriptions in complex scenes<ul> <li>Users omit crucial details or use vague terms</li> <li>Significantly impairs accuracy and reliability</li> </ul> </li> <li>Limited generalization of static models<ul> <li>Unable to adapt to new contexts and scenarios</li> <li>Constrained by internal knowledge within pre-trained models</li> </ul> </li> </ul> <p><strong>Motivation</strong></p> <ul> <li>Leverage interactive retrieval with external agents (e.g. LLM) for general solution</li> <li>Two main challenges identified:<ol> <li>Applying existing interactive methods to text-3D scene retrieval<ul> <li>Current methods lack holistic interaction perspective for complex scenes</li> <li>Focus limited to initially described regions rather than entire spatial layouts</li> </ul> </li> <li>Making existing static retrieval methods interactive<ul> <li>Static models struggle to adapt to interactive text inputs</li> <li>Domain gap between original training and interactive scenarios</li> </ul> </li> </ol> </li> <li>Performance gap evidence: enriched text from interactions only achieved 30.67 recall@1 with 7-point gain</li> </ul> <p><strong>Method</strong></p> <ul> <li>Interactive Retrieval Refinement (IRR) Framework:<ul> <li>Coordinates questioner, answerer, and retriever for continuous interactions</li> <li>Questioner posts questions based on dense capacity entropy between response and scene features</li> <li>Answerer describes scenes based on user responses</li> <li>Retriever performs comprehensive retrieval from three perspectives:<ol> <li>Initial query processing for baseline prediction</li> <li>Multi-round response integration using weighted linear fusion</li> <li>Semantic-level feature extraction and summarization</li> </ol> </li> </ul> </li> <li>Interaction Adaptation Tuning (IAT) Strategy:<ul> <li>Addresses domain adaptation challenge for static-to-interactive transition</li> <li>Two-step process:<ol> <li>Construct simulated memory for text augmentation using training data descriptions</li> <li>Adapt model using discriminability and diversity risk minimization</li> </ol> </li> <li>Loss terms ensure matched pairs cluster together while negative pairs remain separated</li> </ul> </li> <li>Experimental Results:<ul> <li>Effective under coarse-grained memory without additional information</li> <li>Novel performance improvement over existing 2D and interactive methods under fine-grained memory</li> <li>Seamless integration with conventional cross-modal retrieval methods</li> <li>Substantial performance gains demonstrated across three datasets</li> </ul> </li> </ul> <h2 id=coralvqa-a-large-scale-visual-question-answering-dataset-for-coral-reef-image-understanding><strong>CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding</strong><a class=headerlink href=#coralvqa-a-large-scale-visual-question-answering-dataset-for-coral-reef-image-understanding title="Permanent link">&para;</a></h2> <p><a href=https://arxiv.org/abs/2507.10449>Paper</a></p> <p>Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous monitoring to support conservation. While coral reef images provide essential information in coral monitoring, interpreting such images remains challenging due to the need for domain expertise. Visual Question Answering (VQA), powered by Large Vision-Language Models (LVLMs), has great potential in user-friendly interaction with coral reef images. However, applying VQA to coral imagery demands a dedicated dataset that addresses two key challenges: domain-specific annotations and multidimensional questions. In this work, we introduce CoralVQA, the first large-scale VQA dataset for coral reef analysis. It contains 12,805 real-world coral images from 67 coral genera collected from 3 oceans, along with 277,653 question-answer pairs that comprehensively assess ecological and health-related conditions. To construct this dataset, we develop a semi-automatic data construction pipeline in collaboration with marine biologists to ensure both scalability and professional-grade data quality. CoralVQA presents novel challenges and provides a comprehensive benchmark for studying vision-language reasoning in the context of coral reef images. By evaluating several state-of-the-art LVLMs, we reveal key limitations and opportunities. These insights form a foundation for future LVLM development, with a particular emphasis on supporting coral conservation efforts</p> <p><strong>Background</strong></p> <ul> <li>Coral reefs are vital yet vulnerable ecosystems requiring continuous monitoring for conservation</li> <li>Current global mass coral bleaching event affecting 44% of world’s coral across 53+ countries over 40 years<ul> <li>Worst coral bleaching event ever recorded</li> </ul> </li> <li>Coral reef image interpretation requires extensive domain expertise</li> <li>Visual Question Answering (VQA) powered by Large Vision-Language Models (LVLMs) offers potential for user-friendly coral image interaction</li> <li>Current coral datasets focus mainly on classification and segmentation tasks</li> </ul> <p><strong>Find interesting ways for meaningful things</strong></p> <ul> <li>VQA can help bridge the gap between complex ecological assessment and clear insights</li> <li>Translates domain-specific coral analysis into accessible format for broader understanding</li> <li>Enables answering questions about coral health, diversity, bleaching coverage through natural language interaction</li> </ul> <p><strong>Problem</strong></p> <ul> <li>Absence of comprehensive, high-quality VQA datasets for coral conservation</li> <li>Coral reef VQA presents unique challenges:<ul> <li>Domain-specific labels requiring expert knowledge</li> <li>Multidimensional/multi-task questions covering various aspects of coral health</li> </ul> </li> <li>Building VQA datasets for coral reefs more challenging than general image datasets<ul> <li>Requires marine biology expertise for question-answer generation</li> <li>Multiple scale questions needed concurrently</li> </ul> </li> </ul> <p><strong>Work</strong></p> <ul> <li>CoralVQA: First large-scale VQA dataset for coral reef analysis</li> <li>Dataset specifications:<ul> <li>12,805 real-world coral images from 67 coral genera</li> <li>Images collected from 3 oceans across different geographic regions</li> <li>277,653 question-answer pairs for comprehensive ecological assessment</li> </ul> </li> <li>Semi-automatic data construction pipeline developed with marine biologists</li> <li>Provides comprehensive benchmark for vision-language reasoning in coral reef context</li> </ul> <p><strong>Data pipeline</strong></p> <ul> <li>Six-stage process:<ol> <li>Data collection → 2. Label cleaning → 3. Textual attribute extraction → 4. Prompt engineering → 5. Question-answer generation → 6. Human verification</li> </ol> </li> <li>Data sources:<ol> <li>RC dataset</li> <li>XL co clean survey project</li> <li>Cores of the world namespace</li> </ol> </li> <li>Attribute organization into key groups:<ol> <li>Basic real-world fields</li> <li>Health-related attributes</li> </ol> </li> <li>Quality assurance through three-stage process:<ol> <li>Human manual tagging</li> <li>Cross tagging</li> <li>Expert validation</li> </ol> </li> <li>Images span different marine regions across multiple countries and oceans</li> <li>Average coral coverage: 21.6%</li> <li>Average bleached area coverage: 14.4%</li> </ul> <p><strong>Evaluation</strong></p> <ul> <li>Three evaluation subsets created:<ol> <li><strong>Test dataset</strong> - Standard performance evaluation</li> <li><strong>Cross-region dataset</strong> - Tests model generalization across different geographic locations</li> <li><strong>Bleaching-coverage dataset</strong> - Specialized for coral bleaching assessment tasks</li> </ol> </li> <li>Performance results:<ol> <li>Zero-shot performance drops significantly on coral-specific tasks</li> <li>Internal models show stronger average performance across both groups</li> <li>Cross-region evaluation shows 13% performance decrease, indicating generalization challenges</li> </ol> </li> <li>Coral bleaching evaluation metrics:<ol> <li>Current best model MASC scores: 1.2326 (channel) and 0.8967 (income)</li> <li>Models tend to overestimate bleaching region size and extent</li> <li>Significant challenges remain in complex reasoning tasks for coral analysis</li> </ol> </li> <li>Key finding: Current vision-language models struggle with understanding and complex reasoning in coral reef contexts</li> </ul> <h2 id=openhoi-open-world-hand-object-interaction-synthesis-with-multimodal-large-language-model><strong>OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model</strong><a class=headerlink href=#openhoi-open-world-hand-object-interaction-synthesis-with-multimodal-large-language-model title="Permanent link">&para;</a></h2> <p><a href=https://arxiv.org/abs/2505.18947>Paper</a></p> <p><strong>OpenHOI Framework Overview</strong></p> <ul> <li>First open-world hand-object interaction synthesis framework</li> <li>Generates realistic motion sequences for unseen objects using open vocabulary language instructions</li> <li>Addresses two key challenges:<ul> <li>Handling unseen objects in open-world scenarios</li> <li>Processing open vocabulary free-form instructions</li> </ul> </li> </ul> <p><strong>Technical Architecture</strong></p> <ul> <li>Two main components:<ol> <li>Affordance and subtask decomposition</li> <li>Diffusion-driven HOI generation</li> </ol> </li> <li>3D Multimodal LLM (MLLM):<ol> <li>Processes open vocabulary instructions via language encoder</li> <li>Handles 3D point clouds through 3D vision encoder</li> <li>Outputs sequence of subtasks and 3D affordance maps</li> </ol> </li> <li>Affordance decoder:<ol> <li>Takes object features and event program</li> <li>Generates 3D affordance maps over point clouds</li> <li>Specifies where hand should interact with objects</li> </ol> </li> </ul> <p><strong>Training and Generation Process</strong></p> <ul> <li>Two-stage training approach:<ul> <li>Stage 1: Object-centric affordance learning</li> <li>Stage 2: Full instruction alignment with 3D affordance masks</li> </ul> </li> <li>Diffusion model conditioning:<ul> <li>Object geometry</li> <li>Subtask decomposition</li> <li>3D affordance maps from MLLM</li> </ul> </li> <li>Training objectives:<ul> <li>Standard diffusion loss</li> <li>Auxiliary losses (hand-object contact, orientation)</li> <li>Classifier guidance for better object/affordance alignment</li> <li>Compositional noise scheduling for motion refinement</li> </ul> </li> </ul> <p><strong>Applications and Results</strong></p> <ul> <li>Example interaction: “I want to write a paper using my laptop”<ul> <li>System decomposes into three subtasks</li> <li>Generates realistic motion sequence for each subtask</li> </ul> </li> <li>State-of-the-art performance across evaluation metrics</li> <li>Handles complex multi-step interactions:<ul> <li>Brushing teeth, taking photos, making phone calls</li> <li>Opening/closing objects with both hands</li> <li>Playing with toys, eating fruit, making toast</li> </ul> </li> <li>Achieves strong generalization through 3D MLLM processing of open vocabulary targets</li> </ul> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="December 10, 2025 08:14:55 UTC"><span class=timeago datetime=2025-12-10T08:14:55+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="December 10, 2025 08:14:55 UTC">2025-12-10</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="December 10, 2025 08:14:55 UTC"><span class=timeago datetime=2025-12-10T08:14:55+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="December 10, 2025 08:14:55 UTC">2025-12-10</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../workshop-dl4c/ class="md-footer__link md-footer__link--prev" aria-label="Previous: Deep Learning for Coding"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Deep Learning for Coding </div> </div> </a> <a href=../../../courses/ class="md-footer__link md-footer__link--next" aria-label="Next: Online Courses"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Online Courses </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2025 Jackie Yin | Shared for educational purposes </div> </div> <div class=md-social> <a href=https://jackie-jiaqi-yin.github.io target=_blank rel=noopener title="Personal Website" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M277.8 8.6c-12.3-11.4-31.3-11.4-43.5 0l-224 208c-9.6 9-12.8 22.9-8 35.1S18.8 272 32 272h16v176c0 35.3 28.7 64 64 64h288c35.3 0 64-28.7 64-64V272h16c13.2 0 25-8.1 29.8-20.3s1.6-26.2-8-35.1zM240 320h32c26.5 0 48 21.5 48 48v96H192v-96c0-26.5 21.5-48 48-48"/></svg> </a> <a href=https://github.com/jackie-jiaqi-yin target=_blank rel=noopener title="GitHub Profile" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"annotate": null, "base": "../../..", "features": ["navigation.instant", "navigation.instant.prefetch", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.path", "navigation.indexes", "navigation.top", "navigation.footer", "toc.follow", "toc.integrate", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.tabs.link", "content.tooltips"], "search": "../../../assets/javascripts/workers/search.7a47a382.min.js", "tags": {"AI Agent": "ai-agent", "Evaluation": "evaluation", "Model Compression": "compression", "Prompt Engineering": "prompt", "RAG": "rag", "RLHF": "rlhf", "RecSys": "recsys"}, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.e71a0d61.min.js></script> <script src=../../../js/timeago.min.js></script> <script src=../../../js/timeago_mkdocs_material.js></script> <script src=../../../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>