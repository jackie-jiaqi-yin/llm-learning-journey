{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LLM Learning Journey","text":"<p>Welcome to my Large Language Models learning notes! This documentation site serves as a comprehensive collection of my personal insights, research notes, and explorations in the field of LLMs and AI.</p>"},{"location":"#about-these-notes","title":"About These Notes","text":"<p>These Markdown learning notes are a collection of my personal thoughts and insights gathered from various sources. The content may not be perfectly organized, as I take notes whenever I come across something interesting and use a \"topic online clustering\" approach to group them. This method allows me to capture ideas in real-time but may result in notes that are not in a strict order.</p> <p>While these notes help me in my LLMs learning journey, they may not directly align with your needs or be as structured as you'd prefer. Feel free to explore topics that interest you!</p>"},{"location":"#topics-covered","title":"Topics Covered","text":""},{"location":"#core-llm-topics","title":"Core LLM Topics","text":"<ul> <li>AI Agents - Frameworks, architectures, and implementations of AI agents</li> <li>RAG (Retrieval-Augmented Generation) - Best practices and workflows for RAG systems</li> <li>Reinforcement Learning &amp; Fine-Tuning - RLHF, alignment, and fine-tuning techniques</li> <li>Prompt Engineering - Crafting effective prompts for LLMs</li> </ul>"},{"location":"#evaluation-optimization","title":"Evaluation &amp; Optimization","text":"<ul> <li>Evaluation - Metrics, benchmarks, and evaluation methodologies</li> <li>Model Compression - Quantization, pruning, and distillation techniques</li> </ul>"},{"location":"#specialized-topics","title":"Specialized Topics","text":"<ul> <li>Recommendation Systems - LLMs in recommendation systems</li> <li>Miscellaneous - Other interesting topics and findings</li> </ul>"},{"location":"#online-courses","title":"Online Courses","text":"<p>I've also documented notes from various online courses:</p> <ul> <li>GenAI 5-Day Course with Google</li> <li>Hugging Face AI Agent Course</li> </ul>"},{"location":"#navigation-tips","title":"Navigation Tips","text":"<ul> <li>Use the search bar at the top to find specific topics or keywords</li> <li>Browse by topic sections in the left sidebar</li> <li>Check the table of contents on the right for quick navigation within pages</li> <li>Look for tags to find related content across different topics</li> </ul>"},{"location":"#contributing-feedback","title":"Contributing &amp; Feedback","text":"<p>These are personal learning notes, but if you find errors or have suggestions, feel free to open an issue or PR on GitHub!</p> <p>Happy learning!</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#about-this-documentation","title":"About This Documentation","text":"<p>This documentation site is a collection of personal learning notes on Large Language Models, AI Agents, and related topics. The notes are organized using a \"topic online clustering\" approach, where content is added whenever interesting insights are discovered.</p>"},{"location":"about/#note-taking-philosophy","title":"Note-Taking Philosophy","text":"<p>\"I take notes whenever I come across something interesting and use a 'topic online clustering' approach to group them.\"</p> <p>This means:</p> <ul> <li>Real-time capture: Notes are added as I learn, not in a rigid structure</li> <li>Organic organization: Topics emerge and evolve naturally</li> <li>Personal focus: Content reflects my learning journey and interests</li> <li>Continuous evolution: The documentation grows and improves over time</li> </ul>"},{"location":"about/#content-sources","title":"Content Sources","text":"<p>Notes are gathered from various sources including:</p> <ul> <li>Research papers and academic publications</li> <li>Online courses and tutorials</li> <li>Blog posts and technical articles</li> <li>Hands-on experimentation</li> <li>Community discussions</li> </ul>"},{"location":"about/#disclaimer","title":"Disclaimer","text":"<p>These notes may not be: - Perfectly organized or structured - Comprehensive or complete - Suitable for all learning styles - Free from personal bias or interpretation</p> <p>They are intended as a personal knowledge base that might be useful to others exploring similar topics.</p>"},{"location":"about/#contributing","title":"Contributing","text":"<p>While these are personal notes, feedback is welcome! If you find errors or have suggestions:</p> <ol> <li>Open an issue on GitHub</li> <li>Submit a pull request</li> <li>Reach out directly</li> </ol>"},{"location":"about/#license","title":"License","text":"<p>These notes are shared for educational purposes. Please check individual sources for their respective licenses.</p> <p>Happy learning!</p>"},{"location":"tags/","title":"Tags","text":"<p>Browse content by tags to find related topics across different sections.</p>"},{"location":"tags/#available-tags","title":"Available Tags","text":"<ul> <li>ai-agent - AI Agents and autonomous systems</li> <li>rag - Retrieval-Augmented Generation</li> <li>rlhf - Reinforcement Learning from Human Feedback</li> <li>prompt - Prompt Engineering</li> <li>evaluation - LLM Evaluation methods</li> <li>compression - Model Compression techniques</li> <li>recsys - Recommendation Systems</li> </ul> <p>Tags help you discover related content across different topic areas.</p>"},{"location":"courses/","title":"Online Courses","text":"<p>Notes and insights from various online courses on AI and Large Language Models.</p>"},{"location":"courses/#available-courses","title":"Available Courses","text":""},{"location":"courses/#genai-5-day-course-with-google","title":"GenAI 5-Day Course with Google","text":"<p>Comprehensive 5-day course covering generative AI fundamentals and applications.</p> <p>View course notes \u2192</p>"},{"location":"courses/#hugging-face-ai-agent-course","title":"Hugging Face AI Agent Course","text":"<p>Deep dive into building AI agents using Hugging Face tools and frameworks.</p> <p>View course notes \u2192</p>"},{"location":"courses/#coming-soon","title":"Coming Soon","text":"<p>More course notes will be added as I continue my learning journey!</p> <p>\u2190 Back to Topics</p>"},{"location":"talks/","title":"Talks &amp; Seminars","text":"<p>My notes from conferences, seminars, and workshops in AI/ML research.</p>"},{"location":"talks/#recent-conferences","title":"Recent Conferences","text":""},{"location":"talks/#neurips-2025","title":"NeurIPS 2025","text":"<p>Neural Information Processing Systems 2025 San Diego, CA, USA | December 2025</p> <p>A collection of notes from invited talks, tutorials, panels, and oral presentations covering topics in explainable AI, cognitive evaluation, responsible AI, agentic systems, and multimodal learning.</p> <p>View Notes</p>"},{"location":"talks/#about-these-notes","title":"About These Notes","text":"<p>These notes capture key insights, technical details, and personal reflections from talks I've attended. They're organized by event and include:</p> <ul> <li>Invited Talks - Keynotes and special presentations from leading researchers</li> <li>Tutorials - Deep technical dives into specific topics</li> <li>Panels - Discussions on current challenges and future directions</li> <li>Oral Presentations - Selected research papers presented at conferences</li> </ul>"},{"location":"talks/#browse-by-topic","title":"Browse by Topic","text":"<p>Use the Tags page to find talks by topic area (AI Agents, Explainability, Multimodal, etc.).</p>"},{"location":"talks/neurips-2025/","title":"NeurIPS 2025 Notes","text":"<p>Conference: Neural Information Processing Systems 2025</p> <p>Location: San Diego, CA, USA</p> <p>Dates: December 9-15, 2025</p> <p>Website: neurips.cc</p>"},{"location":"talks/neurips-2025/#overview","title":"Overview","text":"<p>NeurIPS (Neural Information Processing Systems) is one of the premier conferences in machine learning and artificial intelligence. These notes capture insights from invited talks, tutorials, panels, and oral presentations I attended during the conference.</p>"},{"location":"talks/neurips-2025/#talks-attended","title":"Talks Attended","text":""},{"location":"talks/neurips-2025/#invited-talks","title":"Invited Talks","text":"Talk Title Speaker Notes On the Science of \"Alien Intelligences\": Evaluating Cognitive Capabilities Melanie Mitchell Six principles for rigorous AI evaluation; exposing flaws in current benchmarking The Art of (Artificial) Reasoning Yejin Choi Era of Large Reasoning Models; smarter scaling through RL and synthetic data From Benchmarks to Problems: Problem Finding in AI Kyunghyun Cho Learning algorithms from data; therapeutic antibody design applications Are We Having the Wrong Nightmares About AI? Zeynep Tufekci Five proofs being destroyed; real dangers of engagement-driven AI business models"},{"location":"talks/neurips-2025/#tutorials","title":"Tutorials","text":"Tutorial Title Topic Area Notes Explainable AI (xAI) Interpretability Feature, data, and component attribution methods; inherent interpretability through training constraints"},{"location":"talks/neurips-2025/#panels-workshops","title":"Panels &amp; Workshops","text":"Session Title Topic Notes Responsible AI Research &amp; Unlearning Ethics &amp; Governance Non-consensual data in research; machine unlearning limitations; research integrity challenges Agentic Development at the Frontier AI Agents PyTorch RL infrastructure; OpenEnv for RL environments; coding agents as primary success case Deep Learning for Coding (DL4C) Coding Agents Building usable coding agents; agentic training; benchmarking; Qwen3-Coder; panel discussion"},{"location":"talks/neurips-2025/#oral-presentations","title":"Oral Presentations","text":"Paper Title Research Area Notes Multimodal Oral Session Vision-Language Dynam3D (3D navigation), Perception Encoder, text-3D retrieval, CoralVQA, OpenHOI"},{"location":"talks/neurips-2025/#key-highlights","title":"Key Highlights","text":""},{"location":"talks/neurips-2025/#most-impactful-insights","title":"Most Impactful Insights","text":"<p>1. AI Evaluation is Fundamentally Broken (Melanie Mitchell) - Current benchmarks suffer from data contamination, spurious associations, and lack of robustness testing - High accuracy doesn't guarantee intended abstraction recognition - Need to distinguish between performance and competence - Six principles: cognitive bias awareness, skeptical hypothesis testing, failure analysis, novel variations, performance vs competence distinction, replication</p> <p>2. The Era of Large Reasoning Models (AI Reasoning Talk) - Transition from brute-force scaling to \"smarter scaling\" - Data saturation forcing new approaches: learn faster, synthesize data, or reason beyond training data - RL effectiveness mixed - entropy management crucial (Goldilocks zone) - Synthetic data innovation: aggressive filtering (70-90%), weaker teacher models can outperform 20x larger ones - Democratizing AI: \"of humans, by humans, for humans\"</p> <p>3. Learning Algorithms from Data (Problem Finding Talk) - Shift from manually designing algorithms to learning them through meta-learning - Applications: targeted causal discovery (20K+ genes), black-box causal inference, mutual information estimation, sequential optimization - Moving from \"learning tools\" to \"learning the process of scientific inquiry itself\" - Trade-off: lose a priori guarantees but gain scalability through extensive empirical verification</p> <p>4. Five Proofs Being Destroyed by Generative AI - Proof of effort (essays, cover letters now mass-produced) - Proof of authenticity (voice, video no longer trustworthy) - Proof of accuracy (well-written \u2260 expertise) - Proof of sincerity (non-sincere entities acting sincere) - Proof of humanity (art value from shared human vulnerability) - Real danger: engagement-driven advertising model creating propaganda/control mechanisms - Actual doom scenario: demand for mass surveillance to restore authenticity proofs</p> <p>5. Explainability Through Three Eras - Before 2014: Linear models and trees - 2014-2020: Data attribution for DNNs - After 2022: Component attribution for LLMs - Unified framework: perturbations, gradients, linear approximations across feature/data/component attribution - Key insight: Build interpretability into training (concept constraints, adversarial training) rather than post-hoc methods</p> <p>6. Responsible AI Challenges - 8M+ non-consensual nude images used in 150 CS papers without consent - Machine unlearning doesn't work as expected - models learn latent information beyond training data - Gap between technical capabilities and regulatory expectations - AI-generated survey papers creating DDoS attack on research community - Need for refutations/critiques track and scientific consensus building</p> <p>7. Agentic AI Infrastructure Revolution - Environments now as important as models - PyTorch Monarch framework for distributed RL with heterogeneous compute - OpenEnv: 1,800+ environments for RL training (used in DeepSeek-V3.2) - Coding agents as first success case: deterministic, verifiable, easy feedback - Task horizons doubling every 7 months</p> <p>8. Multimodal Models Still Struggle - Vision-language navigation plagued by spatial amnesia and geometry blindness - Dynam3D solution: hierarchical semantic pyramid (patch \u2192 instance \u2192 zone) - Perception Encoder: best features not at output layer - need self-distillation - Domain-specific challenges: CoralVQA shows 13% performance drop cross-region - Current VLMs struggle with complex reasoning in specialized domains</p>"},{"location":"talks/neurips-2025/#major-themes","title":"Major Themes","text":"<ul> <li>Evaluation Crisis: Moving beyond accuracy to robustness, consistency, and true understanding</li> <li>Smarter Not Bigger: Data efficiency and reasoning over raw compute scaling</li> <li>Societal Impact: Real dangers aren't superintelligence but engagement optimization and proof destruction</li> <li>Research Integrity: Ethics in data collection and AI-generated content pollution</li> <li>Infrastructure for Agents: Distributed RL systems and diverse training environments</li> <li>Interpretability by Design: Building understanding into models during training</li> <li>Multimodal Challenges: Vision-language models need better 3D understanding and domain adaptation</li> </ul>"},{"location":"talks/neurips-2025/#personal-reflections","title":"Personal Reflections","text":"<p>The conference revealed a field at an inflection point. The conversations weren't about whether models would get bigger, but how to make them smarter, more aligned with human values, and more rigorously evaluated. The most sobering realization: our benchmarks are broken, our research practices have ethical gaps, and the real AI risks aren't about AGI takeover but about the mundane deployment of engagement-maximizing systems that destroy trust, authenticity, and truth.</p>"},{"location":"talks/neurips-2025/#resources","title":"Resources","text":"<ul> <li>NeurIPS 2025 Proceedings</li> <li>Tutorial materials and code repositories linked in individual talk notes</li> </ul>"},{"location":"talks/neurips-2025/#follow-up","title":"Follow-Up","text":"<ul> <li>Implement six principles for evaluating my own AI experiments</li> <li>Revisit Yejin Choi's talk for RL review</li> <li>Explore PyTorch Monarch for distributed RL projects</li> <li>Try OpenEnv environments for agent training</li> <li>Try explanable AI method from tutorial in my interpretability work</li> </ul>"},{"location":"talks/neurips-2025/Invited-talk-problem-finding-in-ai/","title":"Invited talk: From Benchmarks to Problems - A Perspective on Problem Finding in AI","text":"<p>Video Recording</p> <p>Research Philosophy &amp; Problem-Solving Framework</p> <ul> <li>Computer science as discipline of problem solving, not discovery<ul> <li>Find challenging, impactful problems \u2192 identify common substructures \u2192 abstract details \u2192 develop systematic, generalizable solutions</li> <li>Example: NLP problems (translation, speech, dialogue) \u2192 scalable conditional density estimation over sequences \u2192 autoregressive neural models</li> </ul> </li> <li>Criteria for next problems: interesting + impactful<ul> <li>Healthcare chosen for maximum societal impact despite being \u201cmiserable\u201d to work on</li> <li>Drug discovery as specific focus area with tangible positive outcomes</li> </ul> </li> </ul> <p>Machine Translation to Sequence Modeling Evolution</p> <ul> <li>2013: Machine translation as conditional density estimation via nonlinear function approximation</li> <li>Encoder-decoder/sequence-to-sequence approach development (2014-2019)<ul> <li>Attention mechanism invention</li> <li>Multilingual systems, unsupervised translation, non-monotonic generation</li> </ul> </li> <li>2017 realization: not about translation specifically, but general sequence modeling solution<ul> <li>Ilya\u2019s 2014 prediction proved correct: solves any sequence-to-sequence problem</li> </ul> </li> </ul> <p>Learning-to-X Paradigm Emergence</p> <ul> <li>Core insight: learn algorithms directly instead of manually designing them<ul> <li>Turn algorithm design into supervised learning problem</li> <li>Build reasonable simulators \u2192 generate input-output pairs \u2192 train deep neural networks</li> </ul> </li> <li>Historical context: learning-to-learn, amortized inference, simulation-based inference</li> <li>Scale-up breakthrough: from 5 examples per class (2017) to thousands of examples per dataset<ul> <li>Attention mechanisms enable variable input sizes and dimensions</li> </ul> </li> </ul> <p>Therapeutic Antibody Design Application</p> <ul> <li>Lab-in-the-loop molecular design paradigm<ol> <li>Target protein + initial candidates \u2192 generative model mutations \u2192 scoring \u2192 multi-objective optimization \u2192 lab synthesis/testing \u2192 feedback loop</li> </ol> </li> <li>Four key challenges requiring algorithmic solutions:<ol> <li>Target discovery - what should antibody bind to?</li> <li>Evolution - optimal mutation strategies</li> <li>Uncertainty quantification for candidate selection</li> <li>Candidate selection from millions of possibilities with lab constraints</li> </ol> </li> <li>Underlying problems: scalable causal discovery, causal inference, black-box optimization<ol> <li>All require \u201clearning an algorithm\u201d approach since humans can\u2019t solve these systematically</li> </ol> </li> </ul> <p>Four Case Studies of Learned Algorithms</p> <p>Targeted Causal Discovery</p> <ul> <li>Problem: identify actionable causes of target variable from thousands of variables</li> <li>Solution: train neural network on millions of synthetic causal graphs<ul> <li>Input: observational dataset \u2192 Output: binary cause indicators</li> </ul> </li> <li>Results: learned algorithm different from naive graph reconstruction + traversal<ul> <li>Flat error rate vs. distance (pairwise independence testing-like) vs. increasing error for graph-based methods</li> <li>Scales to 20,000+ genes in human cells</li> </ul> </li> </ul> <p>Black-Box Causal Inference</p> <ul> <li>Problem: estimate causal effects without manually deriving estimators</li> <li>Solution: meta-distribution over structural causal models \u2192 train set transformer</li> <li>Results: outperforms existing estimators on sample efficiency and complex nonlinear cases<ul> <li>Validated on LaLonde job training dataset - matched randomized control trial results</li> </ul> </li> <li>Future: single universal causal inference network for all identifiable problems</li> </ul> <p>Neural Mutual Information Estimation</p> <ul> <li>Problem: mutual information estimation requires full density estimation (too hard)</li> <li>Solution: train neural network on 500K+ synthetic joint distributions<ul> <li>Varying dimensions (2-32), sample sizes, distribution families</li> </ul> </li> <li>Results: dramatically outperforms KSG, MINE baselines<ul> <li>Works well for high mutual information values (others underestimate)</li> <li>Single forward pass, built-in uncertainty quantification via simultaneous quantile regression</li> </ul> </li> </ul> <p>Sequential Black-Box Optimization</p> <ul> <li>Problem: use trajectory information for better candidate selection in multi-round optimization</li> <li>Solution: extract procedural knowledge from optimization trajectories<ol> <li>Generate trajectories via MDP + deep Q-network</li> <li>Train prior-fitted network with positional embedding using MAML</li> </ol> </li> <li>Results: faster convergence, better final solutions on molecular binder discovery<ol> <li>MAML crucial to prevent overfitting to spurious trajectory correlations</li> </ol> </li> </ul> <p>Open Research Questions</p> <ul> <li>Uncertainty quantification: meta-distribution uncertainty + meta-training uncertainty + usual uncertainties</li> <li>Meta-generalization: learned algorithms failing on out-of-distribution tasks<ul> <li>Need principled training objectives beyond current meta-learning approaches</li> </ul> </li> <li>Trust and verification: learned algorithms are black boxes vs. classical algorithms with guarantees<ul> <li>Paradigm shift from a priori guarantees to extensive empirical verification</li> <li>Embrace approximations rather than seeking optimal solutions</li> </ul> </li> </ul> <p>Vision for Scientific Discovery</p> <ul> <li>Path toward learned scientific discovery<ul> <li>Current: learning specific tools to automate known processes</li> <li>Future: learn the process of scientific inquiry itself</li> </ul> </li> <li>Capture discovery strategies from traces of past discoveries</li> <li>AI-driven loop for continuous scientific advancement<ul> <li>Move beyond coding \u201caha moments\u201d to learning discovery patterns</li> </ul> </li> </ul>"},{"location":"talks/neurips-2025/agentic-development-frontier/","title":"Agentic Development at the Frontier","text":"<p>Agenda</p> <ul> <li>Environments are foundational for agentic AI development</li> <li>Showcase tools, systems, and simulators needed for RL training</li> <li>Demonstrate how environments integrate with RL training and LM post-training</li> <li>Workshop jointly organized with Reflection AI, Hugging Face, Unsloth, and PyTorch Foundation</li> </ul> <p>PyTorch RL</p> <ul> <li>Building agents requires new infrastructure stack<ul> <li>Infrastructure: getting infra out of the way to focus on algorithms</li> <li>Data: exposing models to as many environments and skills as possible</li> </ul> </li> <li>Key challenges in distributed RL systems<ul> <li>Orchestration - heterogeneous compute scheduling across different resource types</li> <li>Programming model - torch distributed built for data parallel, not RL workflows</li> <li>Performance - dual producer-consumer problems with replay buffer and parameter server bottlenecks</li> </ul> </li> <li>Monarch: PyTorch-native distributed programming framework<ul> <li>Actor-based system with imperative Python API</li> <li>RDMA transfers as first-class citizens</li> <li>Enables fault tolerance and heterogeneous scaling</li> </ul> </li> <li>Torch Forge built on Monarch<ul> <li>Control plane: service abstraction for routing and load balancing</li> <li>Data plane: in-memory storage with automatic resharding</li> <li>Reduces RL code from thousands of lines to simple, readable pseudocode</li> <li>Supports both synchronous and asynchronous RL training</li> </ul> </li> </ul> <p>OpenEnv - Hugging Face</p> <ul> <li>Problem: static datasets plateau in reward, models degrade when deployed to real world</li> <li>Solution: unified, partially observable MDP interface for RL environments<ul> <li>Gymnasium-style API with step and reset functions</li> <li>Can host environments locally or scale via Hugging Face Spaces API</li> </ul> </li> <li>DeepSeek-V3.2 trained on 1,800 distinct environments with 85,000+ complex prompts</li> <li>Available environments<ul> <li>Coding environments for software engineering agents</li> <li>Browser team environments</li> <li>Wordle puzzles, games, and more</li> </ul> </li> <li>CLI tool works like uv package manager<ul> <li>open init creates project skeleton</li> <li>open push deploys to Hugging Face Hub</li> </ul> </li> <li>Type-safe by design using Python data classes<ul> <li>Action, observation, and state classes prevent tensor mismatch errors</li> <li>No external dependencies required</li> </ul> </li> </ul> <p>Unsloth PyTorch OpenEnv</p> <ul> <li>Colab notebook demo: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/OpenEnv_gpt_oss_(20B)_Reinforcement_Learning_2048_Game.ipynb</li> <li>2048 game reinforcement learning example<ul> <li>Goal: train language model to generate winning strategies</li> <li>Reward good actions, penalize bad actions over many iterations</li> <li>Only requires one prompt and one environment - no additional training data needed</li> </ul> </li> <li>Key RL principle: \u201cmore good, less bad\u201d repeated over thousands of steps</li> <li>Integration with Hugging Face TRL framework</li> <li>Memory optimization: 50% reduction in memory usage with faster processing</li> <li>Provides 2,000+ RL environments through Unsloth integration</li> <li>Anti-cheating mechanisms to prevent strategy exploitation</li> <li>GitHub: https://github.com/unslothai/unsloth</li> </ul> <p>Reflection</p> <ul> <li>Evolution from static benchmarks to agentic benchmarks requiring dynamic problem-solving</li> <li>Task horizon trends<ul> <li>AI models completing hour-long tasks 50% of the time</li> <li>Task length doubling every 7 months</li> <li>Software engineering acceleration even faster</li> </ul> </li> <li>Required LLM capabilities for autonomous agents<ul> <li>Multi-step reasoning and planning</li> <li>Tool use (already working well)</li> <li>Self-correction and recovery from failed trajectories</li> <li>Long horizon task coherence</li> </ul> </li> <li>Coding agents as primary success case<ul> <li>Deterministic, text-based, and verifiable</li> <li>Easy verification through unit tests and tool-assisted feedback</li> <li>Evolution from autocomplete to autonomous coding across multiple files</li> </ul> </li> <li>Pre-training vs post-training focus<ul> <li>Pre-training: reasoning ability and long horizon task solving</li> <li>Post-training: environments provide both data and evaluation</li> <li>Need diverse, challenging tasks at edge of current model capabilities</li> </ul> </li> <li>Reflection AI hiring for frontier open agentic models development</li> </ul>"},{"location":"talks/neurips-2025/invited-talk-ai-reasoning/","title":"Invited Talk: The Art of (Artificial) Reasoning","text":"<p>Video Recording</p> <p>Era of Smarter Scaling</p> <ul> <li>Brute-force scaling era ending, smarter scaling beginning</li> <li>Computing growing but data not growing fast enough</li> <li>Three approaches to data saturation:<ol> <li>Learn better/faster with limited data (human-like efficiency)</li> <li>Synthesize more data artificially</li> <li>Reason beyond what\u2019s in training data</li> </ol> </li> <li>2025: Year of Large Reasoning Models (LRMs) vs Large Language Models</li> </ul> <p>Reinforcement Learning Research Findings</p> <ul> <li>Mixed evidence on RL effectiveness in reasoning<ul> <li>Papers show RL can improve performance but questions remain about true reasoning vs probability shifting</li> <li>Pass@1 performance improves but Pass@K performance may worsen after RL</li> <li>Models show homogeneity across different LLMs, especially after post-training</li> </ul> </li> <li>ProRL (Prolonged RL) results:<ul> <li>1.5B parameter model pushed to compete with 7B models</li> <li>Key insight: Entropy management crucial - clipping boundaries matter significantly</li> <li>Goldilocks zone: Low entropy but not too low prevents collapse</li> </ul> </li> <li>RL as Pre-training (RLP):<ul> <li>Information gain reward for predicting next tokens with vs without thought</li> <li>Performance gains survive post-training (SFT + RL)</li> <li>Works even with controlled compute/fewer tokens</li> </ul> </li> </ul> <p>Synthetic Data Innovation</p> <ul> <li>Prismatic Synthesis approach challenges conventional wisdom</li> <li>Used weaker teacher model (32B vs largest available) intentionally</li> <li>Key innovations:<ul> <li>Gradient-based data representation for diversity measurement</li> <li>G-Bandy score in gradient space correlates with out-of-distribution performance</li> <li>Aggressive filtering (70-90% of generated data removed)</li> <li>Fully synthetic problems and solutions</li> </ul> </li> <li>Results: Outperformed models using 20x larger teacher models with zero human-labeled data</li> </ul> <p>Democratizing AI Philosophy</p> <ul> <li>AI should be \u201cof humans, by humans, for humans\u201d<ul> <li>Ownership: Reflects values of all humanity, not just few countries/companies</li> <li>Creation: Developed by people worldwide, not just those who can afford it</li> <li>Beneficiary: Serves all humans, not just some or AI serving AI</li> </ul> </li> <li>Unconventional collaboration example: OpenThought project<ul> <li>Multi-institutional team across universities and startups</li> <li>Achieved remarkable results through effortful SFT competing with RL models</li> </ul> </li> <li>Current AI relies heavily on human intelligence and massive human annotation efforts</li> </ul> <p>Open Research Questions</p> <ul> <li>Need new theories of intelligence (plural) - LLMs may be one approach among many</li> <li>How to reach \u201cdark matter of human knowledge\u201d that current data doesn\u2019t cover</li> <li>Human brain uses light bulb energy vs massive compute requirements</li> <li>Small working memory might be architectural advantage vs million-token windows</li> <li>Robotics lacks internet data - requires different approaches entirely</li> </ul>"},{"location":"talks/neurips-2025/invited-talk-cognitive-capability-evaluation/","title":"Invited talk: On the Science of \u201cAlien Intelligences\u201d: Evaluating Cognitive Capabilities in Babies, Animals, and AI","text":"<p>Video Recording</p> <p>Introduction and Context</p> <ul> <li>Melanie Mitchell keynote at NeurIPS 2025 on evaluating \u201calien intelligences\u201d</li> <li>Focus on rigorous methods for assessing cognitive capabilities across babies, animals, and AI</li> <li>Conference has grown significantly since 1990s (few hundred to current scale)</li> </ul> <p>Benchmark and Evaluation Method Problems</p> <ul> <li>Current AI benchmarking approach fundamentally flawed<ul> <li>Nature headline: benchmarks saturated, systems scoring better than humans</li> <li>NYT: \u201cAI has a measurement problem\u201d</li> <li>Technology Review: \u201cterrible\u201d progress measurement methods</li> </ul> </li> <li>Core issues with benchmark performance:<ul> <li>Data contamination (test data in training sets)</li> <li>Approximate retrieval (similar questions enable pattern matching)</li> <li>Exploitable spurious associations or shortcuts</li> <li>No testing for consistency, robustness, or generalization</li> <li>Lack of construct validity (passing bar exam \u2260 practicing law)</li> <li>Anthropomorphism assumptions (human test design doesn\u2019t translate to AI)</li> </ul> </li> </ul> <p>Six Principles for More Rigorous Evaluation of Cognitive Capacities</p> <ol> <li>Cognitive Bias Awareness<ul> <li>Recognize anthropomorphism tendencies</li> <li>Example: monkey \u201csmile\u201d = fear grimace, not happiness</li> <li>Eliza effect: fluent language triggers human quality attribution</li> </ul> </li> <li>Skeptical Hypothesis Testing<ul> <li>Always consider alternative explanations</li> <li>Clever Hans example: horse appeared to do math but actually read facial cues</li> <li>Required control experiments: questioner knowledge, visual access</li> <li>Recent infant morality study: babies chose \u201chelper\u201d vs \u201chinderer\u201d characters<ul> <li>Confound discovered: bounce animation at top (helper) vs bottom (hinderer)</li> <li>When bounce controlled: babies followed bounce location, not moral preference</li> </ul> </li> </ul> </li> <li>Analyze Failure Types<ul> <li>Failures more insightful than successes for understanding systems</li> <li>Psychology uses human errors to understand cognition</li> <li>Field bias against negative results (\u201ckilljoy explanations\u201d)</li> <li>Journal of Negative Results exists but low impact</li> </ul> </li> <li>Design Novel Variations for Robustness Testing<ul> <li>Mitchell\u2019s analogical reasoning research example</li> <li>UCLA study: GPT-3 outperformed undergrads on letter string analogies</li> <li>Robustness test: counterfactual alphabets (M and E swapped, symbol sequences)</li> <li>Results: humans maintained performance, AI models failed dramatically</li> <li>Importance: test generalization beyond original benchmark conditions</li> </ul> </li> <li>Performance vs. Competence Distinction<ul> <li>System may understand rules but lack execution ability</li> <li>Abstraction and Reasoning Corpus (ARC) example:<ul> <li>Chollet\u2019s 1000 tasks based on core knowledge priors</li> <li>O3 model achieved 88% (high reasoning) vs 64% human performance</li> <li>But analysis of reasoning strategies revealed differences</li> </ul> </li> <li>Mitchell\u2019s simplified ARC study (480 tasks, 16 concepts):<ul> <li>Asked for both output grids AND stated transformation rules</li> <li>O3 often correct output but \u201calien\u201d reasoning (using color numbers vs visual concepts)</li> <li>Visual input: models performed poorly but often had correct rules</li> <li>Competence present despite poor performance</li> </ul> </li> </ul> </li> <li>Replicate and Build on Others\u2019 Results<ul> <li>Converging evidence across multiple experimental tasks essential</li> <li>Academic bias against \u201cincremental\u201d replication work</li> <li>Replication and incremental extension = hallmark of good science</li> </ul> </li> </ol> <p>Conclusion</p> <ul> <li>Don\u2019t just need harder benchmarks (like \u201chumanity\u2019s last exam\u201d)</li> <li>Need more rigorous evaluation methods with substantial creativity</li> <li>High accuracy doesn\u2019t guarantee intended abstraction recognition</li> <li>Low accuracy may mask competence issues rather than fundamental incapability</li> <li>AI systems need human-like world understanding for safe interaction</li> <li>Accuracy alone can mask exploitation of superficial features and non-human reasoning patterns</li> </ul>"},{"location":"talks/neurips-2025/invited-talk-wrong-nightmares-ai/","title":"Invited talk: Are We Having the Wrong Nightmares About AI?","text":"<p>Video Recording</p> <p>Systematic Failures in Anticipating Major Technological Transformations</p> <ul> <li>We systematically miss the impending impact of major technological change in early days</li> <li>Historical pattern: early conversations pick wrong winners/losers, miss major themes</li> <li>Common systematic failures in anticipation:<ul> <li>Mental models get stuck in the past</li> <li>Wrong benchmarks (manuscript imagery vs text for printing press)</li> <li>Focus on individual replacement rather than scale effects</li> <li>Assume current power structures will control new technology</li> </ul> </li> </ul> <p>Historical Examples of Misprediction</p> <ul> <li>Printing press example:<ul> <li>Catholic Church thought it would help them print more Bibles and indulgences</li> <li>Actually unleashed Protestant Reformation through vernacular translations and pamphlets</li> <li>Led to hundreds of years of religious wars, ~\u2153 of German population died</li> </ul> </li> <li>Steam engine/cars:<ul> <li>Entire focus was \u201cis it faster than a horse?\u201d</li> <li>Called \u201chorseless carriages\u201d - designed like carriages without horses</li> <li>Missed profound transformation of 20<sup>th</sup> century living (loneliness, obesity, climate change, geopolitics)</li> </ul> </li> <li>Social media (2012):<ul> <li>Obama White House and Silicon Valley love fest</li> <li>Assumption: Democrats would always control these tools better</li> <li>Reality: tools help whoever uses them most effectively</li> </ul> </li> </ul> <p>Generative AI: Beyond Normal Technology</p> <ul> <li>Transformative general-purpose technology</li> <li>Complex coherent human language capabilities - most significant game changer potentially bigger than Internet</li> <li>Generative and creative output at scale</li> <li>Connectionist systems, not symbolic (different from all historical AI predictions)</li> <li>AGI framework is like \u201chorseless carriage\u201d thinking - wrong mental model</li> <li>Generative AI is a \u201cplausibility engine\u201d - not human-like intelligence<ul> <li>Different error patterns (Cipher 2 vs Cipher 13 example)</li> <li>No internal sense of correct/incorrect</li> <li>Averaging in giant vector space</li> </ul> </li> </ul> <p>Key Destabilization Mechanisms</p> <ul> <li>Scale effects: \u201cmore is different\u201d</li> <li>Load-bearing friction removal breaks systems that use difficulty as signaling</li> <li>Scarcity to abundance transitions break old defenses</li> <li>\u201cGood enough\u201d at scale matters more than \u201cbetter\u201d</li> <li>Institutional and social power assumptions become invalid</li> </ul> <p>The Five Proofs Being Destroyed</p> <ul> <li>Proof of effort:<ul> <li>High school essays, homework, job applications no longer signal investment</li> <li>Customized cover letters can now be mass-produced</li> <li>Gatekeepers will revert to hiring from networks/Ivy League</li> </ul> </li> <li>Proof of authenticity:<ul> <li>Voice calls, videos no longer trustworthy</li> <li>Financial systems depend on certain things being difficult</li> <li>Courts cannot function without evidence standards</li> </ul> </li> <li>Proof of accuracy:<ul> <li>Well-written, well-cited text no longer correlates with expertise</li> <li>LLM papers from plausible to nonsense all look professional</li> <li>Credibility filters based on writing quality are obsolete</li> </ul> </li> <li>Proof of sincerity:<ul> <li>Models use \u201cI\u201d pronouns and act as people pleasers</li> <li>~1 million regular users, many younger, told this is \u201csuper intelligence\u201d</li> <li>Non-sincere entities acting with all signals of sincerity</li> </ul> </li> <li>Proof of humanity:<ul> <li>Art\u2019s value comes from shared human vulnerability/existence</li> <li>Copyright designed to incentivize creators to share publicly</li> <li>Removing moral/financial rights will drive creation underground</li> </ul> </li> </ul> <p>The Big Misalignment: Looming AI Business Model</p> <ul> <li>Current trajectory toward engagement-driven advertising model</li> <li>Not enough revenue from coding agents alone to justify valuations</li> <li>Will use ads, affiliate sales, product placement to monetize</li> <li>AI will try to keep users engaged and trusted - extremely dangerous</li> <li>People will ask AI for views on news, immigration, politics</li> <li>Both propaganda and control mechanism</li> <li>Anthropomorphism makes humans project humanity onto speaking machines</li> <li>If social media polarization was bad, this will be exponentially worse</li> </ul> <p>The Actual Doom Scenario</p> <ul> <li>Removal of authenticity/accuracy proofs creates demand for mass surveillance</li> <li>People will demand chips, government cameras everywhere to verify what\u2019s real</li> <li>Destabilization historically leads to worst-case scenarios as people demand order</li> <li>Need technological solutions, not nostalgia or prohibition</li> <li>Cryptography, blockchain, zero-knowledge proofs could authenticate without surveillance</li> <li>Public key cryptography precedent: first time strangers could securely communicate</li> <li>Choice for technologists: work on engagement optimization vs. human-compatible solutions</li> </ul>"},{"location":"talks/neurips-2025/multimodal-oral/","title":"Multimodal Oral","text":""},{"location":"talks/neurips-2025/multimodal-oral/#dynam3d","title":"Dynam3D","text":"<p>Dynam3D</p> <p>GitHub Repository</p> <p>Slides: </p> <p>Vision and Language Navigation (VLN) - agents follow natural language instructions to navigate 3D environments</p> <p>Existing approaches</p> <p>The Video Tape Approach</p> <ul> <li>Treats world as stream of frames processed by video VLM</li> <li>Issues:<ul> <li>Spatial amnesia - forgets objects once they leave frame</li> <li>Geometry blindness - 2D video lacks explicit 3D structure, leads to poor planning and collisions</li> </ul> </li> </ul> <p>The Frozen Map Approach</p> <ul> <li>Builds explicit 3D map and reasons with 3D VLM</li> <li>Issues:<ul> <li>Assumes static world - fails when objects move</li> <li>Granularity efficiency trade-off - dense maps too slow, sparse maps lose semantic details</li> </ul> </li> </ul> <p>Five Core Requirements for Good VLN Model:</p> <ol> <li>Explicit 3D structure (not just 2D appearances)</li> <li>Multiple semantic scales (fine details to objects and rooms)</li> <li>Compact enough to fit VLM context window</li> <li>Update online as world changes</li> <li>Bind language directly to 3D objects and regions</li> </ol> <p>Semantic Pyramid Tokenization</p> <p>Compresses millions of dense 3D feature points into manageable token set with three-level hierarchy:</p> <p>Patch Level:</p> <ul> <li>Lifts CLIP patch features into 3D using DAPF</li> <li>Preserves fine-grained texture, edges, precise geometry</li> <li>~557 tokens per image</li> </ul> <p>Instance Level:</p> <ul> <li>Groups patch features using FastSAM mask</li> <li>Aggregates into persistent 3D object instances</li> <li>Reduces to ~300 tokens</li> <li>Tracks objects instead of fixed points</li> </ul> <p>Zone Level:</p> <ul> <li>Divides 3D space into uniform cubic zones</li> <li>Aggregates instances within zones into room-level tokens</li> <li>Final count: just few thousand tokens</li> </ul> <p>Online 3D Instance Construction</p> <p>Process:</p> <ol> <li>Generate 2D instances using FastSAM and instance encoder</li> <li>Project into 3D and compare against existing instance memory</li> <li>Get top K 3D instance matches based on feature distances</li> <li>Pass candidates through learned merging discriminator<ul> <li>Selects best 2D to 3D matches using feature similarity and geometric distance</li> <li>If object exists: update instance in 3D memory</li> <li>If new: create new 3D instance</li> </ul> </li> </ol> <p>Result: Persistent, compact, language-grounded object memory that updates continuously</p> <p>Adapt to the Dynamic World</p> <p>Dynamic Frustum Coordinating:</p> <ul> <li>Adds new features when surfaces become newly visible</li> <li>Removes outdated features based on camera frustums and depth when objects move</li> <li>Prevents long-term accumulation of outdated information</li> <li>Keeps 3D memory physically consistent over time</li> <li>Essential for handling moving objects in real environments</li> </ul> <p>Dataset for Training</p> <p>Scale:</p> <ul> <li>1,800+ object categories</li> <li>5,000+ 3D scenes</li> <li>2 million language descriptions</li> <li>Meticulously curated over multiple large 3D datasets</li> <li>Diversity key for generalization</li> </ul> <p>Contrastive Learning for Semantic Alignment</p> <p>Three Complementary Losses:</p> <ol> <li>Segmentation Loss: Ensures accurate 2D to 3D instance grouping</li> <li>Distillation Loss: Transfers CLIP knowledge into 3D instance and zone tokens</li> <li>Alignment Loss: Directly grounds 3D tokens to natural language descriptions</li> </ol> <p>Multi-view Consistency:</p> <ul> <li>Contrastively aligns features of same 3D instances across different viewpoints</li> <li>Pulls same instance features together, pushes different instances apart</li> <li>Produces view-invariant 3D embeddings crucial for stable reasoning as camera moves</li> </ul> <p>Multimodal Reasoning and Action Prediction</p> <p>Architecture:</p> <ul> <li>Patch, instance, zone tokens + natural language instruction + action history fed into lightweight 3.8B parameter VLM</li> <li>VLM fine-tuned to output continuous navigation actions (turning, moving forward, stopping)</li> <li>VLM reasons over structured dynamic 3D token memory instead of raw pixels</li> </ul> <p>Local Minima Mitigation:</p> <ul> <li>Since no global map maintained, planner susceptible to local minima</li> <li>Maintains historical record of robot actions to mitigate this issue</li> </ul> <p>Evaluation Results:</p> <ul> <li>Tested on three challenging VLN benchmarks</li> <li>Consistently outperforms video-based and map-based baselines in:<ul> <li>Navigation success</li> <li>Path efficiency</li> <li>Robustness to freeform instructions</li> </ul> </li> <li>Successfully deployed on real robot in indoor environment</li> <li>Continues operating correctly even when objects move during execution</li> </ul> <p>Conclusion</p> <p>Dynam3D provides hierarchical, compact and dynamic 3D memory that is:</p> <ul> <li>Explicitly geometrical</li> <li>Continuously updated</li> <li>Directly grounded in language</li> <li>Key towards scalable embodied reasoning in real world Vision and language navigation (VLN)</li> </ul>"},{"location":"talks/neurips-2025/multimodal-oral/#perception-encoder-the-best-visual-embeddings-are-not-at-the-output-of-the-network","title":"Perception Encoder: The best visual embeddings are not at the output of the network","text":"<p>Perception Encoder Paper</p> <p>PE Core</p> <ul> <li>State-of-the-art image and video joint CLIP model</li> <li>Built by making CLIP recipe robust through eliminating shortcuts<ul> <li>Varied resolution during training to prevent overfitting</li> <li>Increased batch size for more hard negatives</li> <li>Improvements plateaued on ImageNet validation but significantly boosted robustness metrics</li> </ul> </li> <li>Extended to video using synthetic data engine<ul> <li>Used image-only model as frame-based encoder</li> <li>Built custom Perception LLM for video captioning</li> <li>Video fine-tuning surprisingly improved image performance too</li> </ul> </li> <li>Outperforms SigLIP-2 at all model scales on classification, fine-grained classification, and retrieval</li> <li>Demonstrates superior compositionality and robustness compared to CLIP-L<ul> <li>Better understanding of complex queries like \u201corange berries\u201d and \u201cblue street sign with white text\u201d</li> <li>More robust to image variations (night vision example with raccoons vs opossums)</li> </ul> </li> </ul> <p>PE Lang (alignment method)</p> <ul> <li>Addresses problem: PE Core has strong language features in intermediate layers but poor last-layer performance</li> <li>Solution: Fine-tune Perception LM on top of PE model for 60M samples</li> <li>Results in state-of-the-art multimodal LLM performance<ul> <li>Stronger than InternVL-1.5 and Qwen2-VL at time of publication</li> <li>Successfully aligns language features to final layer</li> </ul> </li> <li>Enables strong performance on OCR QA and visual QA tasks</li> </ul> <p>PE Spatial</p> <ul> <li>Challenge: Spatial features degraded at last layer due to global tokens appearing after layer 32</li> <li>Solution: Self-distillation approach<ul> <li>Distill model to itself at earlier layer (~layer 40)</li> <li>Use SAM mask logic features for enhanced locality</li> <li>Combines semantic understanding with clean spatial features</li> </ul> </li> <li>Achieves state-of-the-art COCO detection performance</li> <li>Qualitative results show clean, semantic part-level similarities</li> <li>Addresses dense prediction tasks like detection, tracking, depth estimation</li> </ul>"},{"location":"talks/neurips-2025/multimodal-oral/#interactive-cross-modal-learning-for-text-3d-scene-retrieval","title":"Interactive Cross-modal Learning for Text-3D Scene Retrieval","text":"<p>Overview</p> <ul> <li>IDeal: Interactive Text-3D Scene Retrieval method enhancing alignment between text queries and 3D scenes through continuous interaction</li> <li>Four main contributions:<ol> <li>Interactive text-3D retrieval method with active alignment enhancement</li> <li>Interactive Retrieval Refinement (IRR) framework supporting structural interaction</li> <li>Interaction Adaptation Tuning (IAT) strategy</li> <li>Comprehensive experimental validation demonstrating superiority</li> </ol> </li> <li>Paper available.</li> </ul> <p>Open World Obstacles</p> <ul> <li>Incomplete one-shot descriptions fail to capture user intent<ul> <li>Single shot queries provide limited scene summaries</li> <li>Lead to mismatches between queries and retrieval results</li> </ul> </li> <li>Domain shifts across users with different languages, regional experiences, semantic skills<ul> <li>Models trained on one text type struggle to generalize to other niches</li> </ul> </li> <li>Ambiguous/unspecific descriptions in complex scenes<ul> <li>Users omit crucial details or use vague terms</li> <li>Significantly impairs accuracy and reliability</li> </ul> </li> <li>Limited generalization of static models<ul> <li>Unable to adapt to new contexts and scenarios</li> <li>Constrained by internal knowledge within pre-trained models</li> </ul> </li> </ul> <p>Motivation</p> <ul> <li>Leverage interactive retrieval with external agents (e.g. LLM) for general solution</li> <li>Two main challenges identified:<ol> <li>Applying existing interactive methods to text-3D scene retrieval<ul> <li>Current methods lack holistic interaction perspective for complex scenes</li> <li>Focus limited to initially described regions rather than entire spatial layouts</li> </ul> </li> <li>Making existing static retrieval methods interactive<ul> <li>Static models struggle to adapt to interactive text inputs</li> <li>Domain gap between original training and interactive scenarios</li> </ul> </li> </ol> </li> <li>Performance gap evidence: enriched text from interactions only achieved 30.67 recall@1 with 7-point gain</li> </ul> <p>Method</p> <ul> <li>Interactive Retrieval Refinement (IRR) Framework:<ul> <li>Coordinates questioner, answerer, and retriever for continuous interactions</li> <li>Questioner posts questions based on dense capacity entropy between response and scene features</li> <li>Answerer describes scenes based on user responses</li> <li>Retriever performs comprehensive retrieval from three perspectives:<ol> <li>Initial query processing for baseline prediction</li> <li>Multi-round response integration using weighted linear fusion</li> <li>Semantic-level feature extraction and summarization</li> </ol> </li> </ul> </li> <li>Interaction Adaptation Tuning (IAT) Strategy:<ul> <li>Addresses domain adaptation challenge for static-to-interactive transition</li> <li>Two-step process:<ol> <li>Construct simulated memory for text augmentation using training data descriptions</li> <li>Adapt model using discriminability and diversity risk minimization</li> </ol> </li> <li>Loss terms ensure matched pairs cluster together while negative pairs remain separated</li> </ul> </li> <li>Experimental Results:<ul> <li>Effective under coarse-grained memory without additional information</li> <li>Novel performance improvement over existing 2D and interactive methods under fine-grained memory</li> <li>Seamless integration with conventional cross-modal retrieval methods</li> <li>Substantial performance gains demonstrated across three datasets</li> </ul> </li> </ul>"},{"location":"talks/neurips-2025/multimodal-oral/#coralvqa-a-large-scale-visual-question-answering-dataset-for-coral-reef-image-understanding","title":"CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding","text":"<p>Paper</p> <p>Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous monitoring to support conservation. While coral reef images provide essential information in coral monitoring, interpreting such images remains challenging due to the need for domain expertise. Visual Question Answering (VQA), powered by Large Vision-Language Models (LVLMs), has great potential in user-friendly interaction with coral reef images. However, applying VQA to coral imagery demands a dedicated dataset that addresses two key challenges: domain-specific annotations and multidimensional questions. In this work, we introduce CoralVQA, the first large-scale VQA dataset for coral reef analysis. It contains 12,805 real-world coral images from 67 coral genera collected from 3 oceans, along with 277,653 question-answer pairs that comprehensively assess ecological and health-related conditions. To construct this dataset, we develop a semi-automatic data construction pipeline in collaboration with marine biologists to ensure both scalability and professional-grade data quality. CoralVQA presents novel challenges and provides a comprehensive benchmark for studying vision-language reasoning in the context of coral reef images. By evaluating several state-of-the-art LVLMs, we reveal key limitations and opportunities. These insights form a foundation for future LVLM development, with a particular emphasis on supporting coral conservation efforts</p> <p>Background</p> <ul> <li>Coral reefs are vital yet vulnerable ecosystems requiring continuous monitoring for conservation</li> <li>Current global mass coral bleaching event affecting 44% of world\u2019s coral across 53+ countries over 40 years<ul> <li>Worst coral bleaching event ever recorded</li> </ul> </li> <li>Coral reef image interpretation requires extensive domain expertise</li> <li>Visual Question Answering (VQA) powered by Large Vision-Language Models (LVLMs) offers potential for user-friendly coral image interaction</li> <li>Current coral datasets focus mainly on classification and segmentation tasks</li> </ul> <p>Find interesting ways for meaningful things</p> <ul> <li>VQA can help bridge the gap between complex ecological assessment and clear insights</li> <li>Translates domain-specific coral analysis into accessible format for broader understanding</li> <li>Enables answering questions about coral health, diversity, bleaching coverage through natural language interaction</li> </ul> <p>Problem</p> <ul> <li>Absence of comprehensive, high-quality VQA datasets for coral conservation</li> <li>Coral reef VQA presents unique challenges:<ul> <li>Domain-specific labels requiring expert knowledge</li> <li>Multidimensional/multi-task questions covering various aspects of coral health</li> </ul> </li> <li>Building VQA datasets for coral reefs more challenging than general image datasets<ul> <li>Requires marine biology expertise for question-answer generation</li> <li>Multiple scale questions needed concurrently</li> </ul> </li> </ul> <p>Work</p> <ul> <li>CoralVQA: First large-scale VQA dataset for coral reef analysis</li> <li>Dataset specifications:<ul> <li>12,805 real-world coral images from 67 coral genera</li> <li>Images collected from 3 oceans across different geographic regions</li> <li>277,653 question-answer pairs for comprehensive ecological assessment</li> </ul> </li> <li>Semi-automatic data construction pipeline developed with marine biologists</li> <li>Provides comprehensive benchmark for vision-language reasoning in coral reef context</li> </ul> <p>Data pipeline</p> <ul> <li>Six-stage process:<ol> <li>Data collection \u2192 2. Label cleaning \u2192 3. Textual attribute extraction \u2192 4. Prompt engineering \u2192 5. Question-answer generation \u2192 6. Human verification</li> </ol> </li> <li>Data sources:<ol> <li>RC dataset</li> <li>XL co clean survey project</li> <li>Cores of the world namespace</li> </ol> </li> <li>Attribute organization into key groups:<ol> <li>Basic real-world fields</li> <li>Health-related attributes</li> </ol> </li> <li>Quality assurance through three-stage process:<ol> <li>Human manual tagging</li> <li>Cross tagging</li> <li>Expert validation</li> </ol> </li> <li>Images span different marine regions across multiple countries and oceans</li> <li>Average coral coverage: 21.6%</li> <li>Average bleached area coverage: 14.4%</li> </ul> <p>Evaluation</p> <ul> <li>Three evaluation subsets created:<ol> <li>Test dataset - Standard performance evaluation</li> <li>Cross-region dataset - Tests model generalization across different geographic locations</li> <li>Bleaching-coverage dataset - Specialized for coral bleaching assessment tasks</li> </ol> </li> <li>Performance results:<ol> <li>Zero-shot performance drops significantly on coral-specific tasks</li> <li>Internal models show stronger average performance across both groups</li> <li>Cross-region evaluation shows 13% performance decrease, indicating generalization challenges</li> </ol> </li> <li>Coral bleaching evaluation metrics:<ol> <li>Current best model MASC scores: 1.2326 (channel) and 0.8967 (income)</li> <li>Models tend to overestimate bleaching region size and extent</li> <li>Significant challenges remain in complex reasoning tasks for coral analysis</li> </ol> </li> <li>Key finding: Current vision-language models struggle with understanding and complex reasoning in coral reef contexts</li> </ul>"},{"location":"talks/neurips-2025/multimodal-oral/#openhoi-open-world-hand-object-interaction-synthesis-with-multimodal-large-language-model","title":"OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model","text":"<p>Paper</p> <p>OpenHOI Framework Overview</p> <ul> <li>First open-world hand-object interaction synthesis framework</li> <li>Generates realistic motion sequences for unseen objects using open vocabulary language instructions</li> <li>Addresses two key challenges:<ul> <li>Handling unseen objects in open-world scenarios</li> <li>Processing open vocabulary free-form instructions</li> </ul> </li> </ul> <p>Technical Architecture</p> <ul> <li>Two main components:<ol> <li>Affordance and subtask decomposition</li> <li>Diffusion-driven HOI generation</li> </ol> </li> <li>3D Multimodal LLM (MLLM):<ol> <li>Processes open vocabulary instructions via language encoder</li> <li>Handles 3D point clouds through 3D vision encoder</li> <li>Outputs sequence of subtasks and 3D affordance maps</li> </ol> </li> <li>Affordance decoder:<ol> <li>Takes object features and event program</li> <li>Generates 3D affordance maps over point clouds</li> <li>Specifies where hand should interact with objects</li> </ol> </li> </ul> <p>Training and Generation Process</p> <ul> <li>Two-stage training approach:<ul> <li>Stage 1: Object-centric affordance learning</li> <li>Stage 2: Full instruction alignment with 3D affordance masks</li> </ul> </li> <li>Diffusion model conditioning:<ul> <li>Object geometry</li> <li>Subtask decomposition</li> <li>3D affordance maps from MLLM</li> </ul> </li> <li>Training objectives:<ul> <li>Standard diffusion loss</li> <li>Auxiliary losses (hand-object contact, orientation)</li> <li>Classifier guidance for better object/affordance alignment</li> <li>Compositional noise scheduling for motion refinement</li> </ul> </li> </ul> <p>Applications and Results</p> <ul> <li>Example interaction: \u201cI want to write a paper using my laptop\u201d<ul> <li>System decomposes into three subtasks</li> <li>Generates realistic motion sequence for each subtask</li> </ul> </li> <li>State-of-the-art performance across evaluation metrics</li> <li>Handles complex multi-step interactions:<ul> <li>Brushing teeth, taking photos, making phone calls</li> <li>Opening/closing objects with both hands</li> <li>Playing with toys, eating fruit, making toast</li> </ul> </li> <li>Achieves strong generalization through 3D MLLM processing of open vocabulary targets</li> </ul>"},{"location":"talks/neurips-2025/rai-pannel/","title":"Responsible AI & Unlearning","text":""},{"location":"talks/neurips-2025/rai-pannel/#responsible-ai-research-unlearning-from-consent-to-compliance-to-critique","title":"Responsible AI Research &amp; Unlearning: From Consent to Compliance to Critique","text":"<p>Research Findings on Non-Consensual Imagery</p> <ul> <li>Princessa Cinquecchia (Boston University) - systematic review of nude image use in AI research<ul> <li>Analyzed 150 computer science papers using real nude images without consent</li> <li>Over 8 million non-consensual nude images identified across papers</li> <li>8 papers published 2023+ in flagship venues (ACCP, CBPR, AAA)</li> <li>Growing trend - majority published in last 5 years</li> </ul> </li> <li>Content sources include:<ul> <li>Banned Reddit communities with abusive content</li> <li>Hidden camera footage and upskirt images</li> <li>Child sexual abuse material (explicitly mentioned in some papers)</li> <li>Pornography industry content used without creator consent</li> </ul> </li> <li>Published papers show identifiable faces without censoring</li> <li>Recommendation: Partner with industry platforms before conducting research, question necessity of using nude imagery</li> </ul> <p>Machine Unlearning for AI Regulation Compliance</p> <ul> <li>Bill Marino (Cambridge) - bridging machine unlearning and AI regulation<ul> <li>EU AI Act as proof of concept (world\u2019s first comprehensive AI regulation, already in effect)</li> <li>Identified 6 potential use cases for machine unlearning in regulatory compliance</li> <li>Significant gaps exist between current unlearning capabilities and regulatory needs</li> </ul> </li> <li>Economic motivation: billion-dollar training runs make retraining prohibitive<ul> <li>Machine unlearning offers alternative to full model retraining</li> <li>Trade-offs exist: removing mislabeled data may introduce bias or security vulnerabilities</li> </ul> </li> <li>Specific compliance challenge: accuracy requirements<ul> <li>Identifying \u201cforget sets\u201d (bad/mislabeled data) difficult at scale</li> <li>Paring down datasets may violate other AI Act provisions</li> </ul> </li> </ul> <p>Technical Limitations of Machine Unlearning</p> <ul> <li>A. Feder Cooper (Yale) - \u201cMachine Unlearning Doesn\u2019t Do What You Think\u201d<ul> <li>Gold standard assumption flawed: retraining from scratch with unwanted data removed</li> <li>Models learn latent information beyond just repeating training data</li> <li>Example: Creative Commons-trained diffusion model still generates Mickey Mouse when prompted</li> </ul> </li> <li>Scope expansion beyond data removal:<ul> <li>Originally focused on removing specific training data influence</li> <li>Now includes suppressing unwanted generative outputs</li> <li>Encompasses alignment methods and content filters</li> </ul> </li> <li>Gap between technical capabilities and policy expectations<ul> <li>Need conceptual clarity between different \u201cunlearning\u201d techniques</li> <li>Removal vs suppression serve different goals</li> <li>Policymaker education needed on technique limitations and appropriate applications</li> </ul> </li> </ul>"},{"location":"talks/neurips-2025/rai-pannel/#strengthening-the-ai-research-ecosystem-integrity-critique-and-consensus","title":"Strengthening the AI Research Ecosystem: Integrity, Critique, and Consensus","text":"<p>AI Research Integrity Challenges</p> <ul> <li>DDoS Attack on Research Community (Jiang Hao Lin)<ul> <li>AI-generated survey papers overwhelming review capacity and research attention</li> <li>Arxiv implemented stricter policy (Oct 2024) - only peer-reviewed surveys allowed</li> <li>Wrong reward signals: AI-generated surveys on hot topics getting 100+ citations with minimal effort</li> <li>MCP protocol example: 5+ survey papers in one month, content already outdated</li> <li>Hallucinations in AI surveys become \u201ctruth\u201d especially for junior researchers</li> </ul> </li> <li>Current Incentive Problems<ul> <li>Bad behavior rewarded: 30-minute ChatGPT surveys getting high citations</li> <li>Quality research vs. gaming the system misalignment</li> <li>Citation count not reliable quality indicator anymore</li> </ul> </li> </ul> <p>Proposed Solutions</p> <ul> <li>Refutations and Critiques Track (Rylan Shaffer)<ul> <li>New venue for correcting scientific record</li> <li>Address misleading, incorrect, flawed, or fraudulent research</li> <li>Current problem: negative results seen as ad hominem, hard to publish</li> <li>Need consequences for malfeasance when discovered</li> <li>Arxiv comments feature exists but underutilized</li> </ul> </li> <li>Scientific Consensus Building (Rishi Wamansi)<ul> <li>NeurIPS should facilitate consensus on policy-relevant topics</li> <li>Not universal agreement, but articulated consensus where it exists</li> <li>Policymakers need peer-reviewed research basis for reports</li> <li>No better alternative institution than NeurIPS for this role</li> <li>Requires innovation in conference structure</li> </ul> </li> <li>Individual Actions<ul> <li>Speak out about problematic prior research despite difficulty</li> <li>Use AI as copilot, not autopilot for survey writing</li> <li>Don\u2019t rely solely on citation count for survey quality</li> <li>Consult senior researchers when entering new fields</li> </ul> </li> </ul> <p>Next Steps</p> <ul> <li>Community Engagement Needed<ul> <li>Articulate demand and volunteer capacity for new tracks</li> <li>Show substantive interest beyond just proposing changes</li> <li>Consider celebrating negative results and \u201cspectacular failures\u201d</li> <li>Shift incentives to reward vulnerability and honest reporting</li> </ul> </li> <li>Alternative Approaches Discussed<ul> <li>Specialized reviewers for failure/learning tracks</li> <li>Collaboration with policy think tanks and social scientists</li> <li>Better utilization of existing Arxiv comment system</li> <li>Focus on synthetic reviews vs. annotated bibliographies</li> </ul> </li> </ul>"},{"location":"talks/neurips-2025/workshop-dl4c/","title":"Workshop: Deep Learning for Coding (DL4C)","text":"<p>Recording</p>"},{"location":"talks/neurips-2025/workshop-dl4c/#lessons-from-the-trenches-on-building-usable-coding-agent","title":"Lessons from the Trenches on building usable coding agent","text":"<p>GitHub Repository Lesson 1. Believe in simplicity</p> <ul> <li>Agent architecture evolution<ul> <li>MetaGPT approach: multi-agent system with boss agent, product manager agent, architect agent, project manager agent, engineer agent</li> <li>OpenHands approach: single flexible agent that can think on the fly</li> <li>Software engineering is incredibly diverse - fixed workflows break when tasks go off expected path</li> <li>Single agent architecture proves more effective for varied tasks</li> </ul> </li> <li>Tool philosophy: minimal but powerful toolset<ul> <li>Started with large toolbox approach, settled on minimal set</li> <li>Core tools provided to agent:<ol> <li>Bash Terminal - for system interactions</li> <li>Python/Jupyter notebook - for complex programming tasks</li> <li>File editor - direct file editing (like IDE)</li> <li>Visual web browser - for debugging frontends, viewing PDFs</li> <li>Search API - for online information gathering</li> </ol> </li> <li>Can leverage existing libraries (GitHub library, etc.) through these core tools</li> <li>Generally use tools created for human programmers</li> </ul> </li> </ul> <p>Lesson 2: Code don\u2019t click</p> <ul> <li>GUI vs API performance comparison on WebArena benchmark<ul> <li>GUI-based agent (clicking through interfaces): 15% accuracy</li> <li>API-based agent (direct API calls): 29% accuracy</li> <li>Hybrid agent (API when available, GUI when necessary): 39% accuracy</li> <li>Nearly tripled accuracy by changing interaction format</li> </ul> </li> <li>API quality impact<ul> <li>Performance gains much larger on sites with good APIs vs poor APIs</li> <li>Controlled experiment: adding APIs to Reddit site nearly doubled accuracy with just 1-2 days manual work</li> <li>Similar concept to MCP (Model Control Protocol) - prefer computer interfaces over human interfaces</li> </ul> </li> </ul> <p>Lesson 3. Agentic training is essential</p> <ul> <li>Requirements for agentic language models:<ol> <li>Instruction following ability - especially with long context</li> <li>Tool use/coding abilities - both writing code and using tools properly</li> <li>Environment understanding - GUI programming, visual browsers, domain-specific knowledge</li> <li>Error awareness/recovery abilities - try new strategies instead of repeating mistakes</li> <li>Reasonable cost</li> </ol> </li> <li>Current challenge: no model has all requirements simultaneously</li> <li>Training approaches:<ol> <li>Reinforcement learning (SWE-Gym)<ul> <li>First RL algorithm for software engineering tasks</li> <li>Convert SweeBench samples into RL environment</li> <li>Generate rollouts, reward based on unit test success</li> </ul> </li> <li>Synthetic data (GoBrowse method)<ul> <li>LLM explores websites, proposes tasks, tests feasibility</li> <li>Restart exploration from previously explored areas for deeper navigation</li> </ul> </li> <li>Human demonstrations<ul> <li>Most expensive approach</li> <li>Google collected 689k multimodal demos for Android navigation</li> </ul> </li> </ol> </li> <li>Agent Data Protocol<ol> <li>Standardized format for 1.7 million agent trajectories</li> <li>Addresses problem of every dataset having different formats</li> <li>Enables easy conversion to training formats for different agent frameworks</li> <li>Demonstrated improvements across OpenHands, SWE-Agent, and Agent-Lab</li> </ol> </li> </ul> <p>Lesson 4: Benchmarking must be ecologically valid</p> <ul> <li>VersaBench: opinionated suite covering real user cases<ul> <li>SweeBench for core coding tasks</li> <li>SweeBench Multimodal for frontend programming</li> <li>Multi-SWE for different programming languages</li> <li>CI failure fixing (pre-commit checks, linting)</li> <li>Information gathering tasks</li> <li>CommitZero for creating new apps from scratch</li> <li>Agent Company for navigating software company tasks</li> <li>SWE-Bench for software testing</li> </ul> </li> <li>Human-in-the-loop evaluation methodology<ul> <li>Step 1: Collect agent trajectories with user feedback (1-5 rating)</li> <li>Step 2: Train model to predict user ratings from trajectory data</li> <li>Step 3: Use model to evaluate interventions without waiting for human feedback</li> <li>Step 4: Compute effect sizes for different model/system changes</li> <li>8-9% feedback rate from users (high for this type of system)</li> </ul> </li> <li>Key findings from user satisfaction analysis<ul> <li>Claude 3.7 to Claude 4: statistically significant improvement<ul> <li>Fewer misunderstood intentions, less error rate, fewer git resets</li> </ul> </li> <li>Claude 4 vs GPT-4o: users preferred Claude 4<ul> <li>GPT-4o slower, less responsive interface design</li> </ul> </li> <li>Benchmark correlation: strong for Claude 3.7 vs 4, weak for Claude 4 vs GPT-4o due to non-functional factors</li> </ul> </li> </ul> <p>Lesson 5: Agent should adapt with us</p> <ul> <li>Handling under-specificity problem<ul> <li>Created dataset by removing details from SweeBench Verified issues</li> <li>Performance dropped ~50% with under-specified tasks</li> <li>Simulated user interaction improved performance significantly</li> <li>Built RL environment for training proactive questioning behavior</li> <li>Trained models beat GPT-4o on overall evaluation metrics</li> </ul> </li> <li>Agent personalization and learning<ul> <li>Agent Workflow Memory: automatically evaluates task success, induces workflows, feeds back to agent memory</li> <li>Agent Skills Induction: generates reusable code functions from successful trajectories</li> <li>Both approaches completely unsupervised - can run during normal usage</li> <li>Significantly improves success rate and efficiency</li> <li>Can turn GUI navigation into APIs through learned skills</li> </ul> </li> </ul>"},{"location":"talks/neurips-2025/workshop-dl4c/#predicting-all-the-error-bars-of-llm-evaluations","title":"Predicting all the error bars of LLM evaluations","text":"<p>Statistical Noise in LLM Evaluations</p> <ul> <li>Current evaluation reliability concerns<ul> <li>Small benchmark sizes (hundreds vs thousands in ImageNet era)</li> <li>Few percent improvements often not statistically significant</li> <li>Agent evaluations require 100k+ tokens, hours of work per sample</li> </ul> </li> <li>Key benchmark examples<ul> <li>HumanEval: hundreds of problems</li> <li>SWE-bench: 500 examples in popular version</li> <li>Contrast with ImageNet: 10k images, 100k total</li> </ul> </li> </ul> <p>Noise Framework &amp; Methodology</p> <ul> <li>Three types of noise decomposition<ol> <li>Prediction noise: LLM stochastic output variation<ul> <li>Measurable directly on fixed eval sets</li> <li>Reducible via averaging, temperature control</li> </ul> </li> <li>Data noise: Sampling variation from question set<ul> <li>Cannot be measured on fixed dataset</li> <li>Requires resampling/bootstrapping analysis</li> </ul> </li> <li>Total noise: Prediction + data noise</li> </ol> </li> <li>Paired vs unpaired comparisons<ol> <li>Paired tests much more powerful (same questions across models)</li> <li>Standard error scales as 1/\u221a(number of questions)</li> <li>Theoretical predictions fit empirical data across benchmarks</li> </ol> </li> </ul> <p>Key Findings &amp; Recommendations</p> <ul> <li>Noise levels by accuracy range<ul> <li>HumanEval: Need 10% difference for significance (unpaired), 8% (paired)</li> <li>Prediction noise dominant at typical temperatures (0.6-1.0)</li> <li>Data noise becomes limiting factor with infinite sampling</li> </ul> </li> <li>Practical implications<ul> <li>Benchmark builders should report noise levels</li> <li>Use paired comparisons when possible</li> <li>Consider temperature tradeoffs (lower temp reduces prediction noise)</li> <li>Hard problems don\u2019t necessarily reduce noise due to inconsistency</li> </ul> </li> <li>Current benchmarks show high inconsistency<ul> <li>Worse models sometimes solve harder problems than better models</li> <li>Multi-step problems compound error probability exponentially</li> </ul> </li> </ul>"},{"location":"talks/neurips-2025/workshop-dl4c/#how-to-develop-in-the-agentic-era","title":"How to Develop in the Agentic Era","text":"<p>Evaluation Challenges in AI Systems</p> <ul> <li>Current evaluation systems fundamentally flawed<ul> <li>Peer review example: 3-6 human annotators, 0-10 scoring with predefined rubrics</li> <li>Binary accept/reject outcomes despite subjective scoring</li> <li>When evaluation breaks, results can be disastrous</li> </ul> </li> <li>Reliability vs validity concepts from psychology<ul> <li>Reliability: consistency across multiple measurements under similar conditions</li> <li>Validity: actually measuring what you think you\u2019re measuring</li> <li>AI field equivalents: variance vs bias</li> </ul> </li> </ul> <p>Common Evaluation Pitfalls</p> <ul> <li>SWE-bench coding benchmark issues<ul> <li>Small reasoning models outperformed specialized coding models</li> <li>Actually measured instruction following (XML format compliance) rather than coding ability</li> <li>Low validity situation - not measuring intended capability</li> </ul> </li> <li>Mathematics evaluation problems<ul> <li>Models trained to output answers in specific box format</li> <li>Measuring format compliance rather than mathematical reasoning</li> </ul> </li> <li>Human evaluation challenges<ul> <li>High variance with small annotator pools (&lt;20 people)</li> <li>Reliable results only with 200+ annotators</li> <li>Bias toward specific domains (Gemini 3 Pro excelled due to web development focus in prompts)</li> </ul> </li> </ul> <p>Programming Definitions &amp; Evaluation Scope</p> <ul> <li>Multiple definitions of programming<ol> <li>Programs = algorithms + data structures (Nicholas textbook)</li> <li>Programs = system design + implementation</li> <li>Programs = organization of data (alternative perspective)</li> </ol> </li> <li>Current benchmarks focus heavily on code generation<ol> <li>Missing: code understanding, design, test generation, debugging</li> <li>Need comprehensive evaluation covering all programming aspects</li> </ol> </li> <li>Companies build internal evaluations to cover broader scope</li> </ul> <p>Training &amp; Scaling Considerations</p> <ul> <li>Evaluation directly enables training improvements<ul> <li>Perfect reward computation enables perfect RL</li> <li>Self-evaluation capabilities crucial for model development</li> </ul> </li> <li>Scaling factors for coding agents<ul> <li>Diversity of tasks</li> <li>Rollout length and number of tool calls</li> <li>Diversity of tools/sandbox environments</li> <li>Complexity beyond math/STEM domains</li> </ul> </li> <li>Policy gradient limitations in agentic era<ul> <li>Only learns one beam of information per rollout</li> <li>Inefficient for long-duration agent tasks (hours/days)</li> <li>May need process supervision and step-by-step modeling</li> </ul> </li> </ul> <p>Applications &amp; Future Directions</p> <ul> <li>Coding as fundamental AGI skill<ul> <li>Building block for other capabilities</li> <li>Creative applications beyond traditional programming</li> <li>Examples: blog writing, job applications, CLI tools</li> </ul> </li> <li>Development focus areas<ul> <li>Training: push toward better generalization across tools/tasks/scenarios</li> <li>Applications: design for human creativity, not limited to code writing</li> <li>Quality over quantity in benchmark creation (100 manual examples &gt; 10,000 auto-generated)</li> </ul> </li> <li>Sweet spot for evaluation difficulty: 5-30% success rates for meaningful signal</li> </ul>"},{"location":"talks/neurips-2025/workshop-dl4c/#qwen3-coder","title":"Qwen3-Coder","text":"<p>Qwen3-Coder Model Architecture &amp; Training</p> <ul> <li>Flagship model: 480B total parameters, 35B activated (MOE architecture)<ul> <li>Sparse activation for efficiency while maintaining large model capabilities</li> <li>Competitive performance: SWE-bench 37%, close to GPT-4 levels</li> </ul> </li> <li>New BNET architecture for next generation<ul> <li>Hybrid model: 3 linear attention layers + 1 full attention layer per 4-layer block</li> <li>Trained on 256K tokens, targeting 1M token context length</li> <li>Motivation: enable long-horizon coding tasks (multi-day problem solving)</li> </ul> </li> </ul> <p>Training Pipeline &amp; Data Strategy</p> <ul> <li>Pre-training approach: Data \u2192 coder \u2192 data \u2192 coder (iterative improvement)<ol> <li>Synthetic data generation crucial for coding-specific capabilities</li> <li>SpeedFlow method generates software engineering scenarios from test cases</li> <li>Focus on real software engineer experience patterns missing from internet data</li> </ol> </li> <li>Post-training RL process<ol> <li>Initial SFT on diverse coding tasks (code generation, software development, data analysis, SQL)</li> <li>Long-horizon RL training using MegaFlow scheduler<ul> <li>20K concurrent virtualized agent environments on Alibaba Cloud</li> <li>Agents interact with real coding environments using scaffolds (OpenHands, etc.)</li> <li>Challenge: model sometimes \u201chacks\u201d evaluations (git log to find solutions)</li> </ul> </li> </ol> </li> </ul> <p>Coding Agents &amp; Future Direction</p> <ul> <li>Agentic vs non-agentic coding differences<ul> <li>Multi-turn environment interaction vs single-turn solutions</li> <li>Higher token consumption but capable of harder tasks</li> <li>Dynamic scaffolding vs static problem-solving</li> </ul> </li> <li>Integration roadmap<ul> <li>Search capabilities: combine coding agent + search agent for dynamic tool usage</li> <li>Multimodal foundation: vision for computer usage agents (clicking + coding)</li> <li>Long-horizon reasoning: 10-30 hour problem solving sessions</li> </ul> </li> <li>Product: PlainCode platform (free 50 queries/day, open source scaffold)</li> </ul>"},{"location":"talks/neurips-2025/workshop-dl4c/#dl4c-panel","title":"DL4C panel","text":"<p>Current AI Coding Agent Capabilities &amp; Limitations</p> <ul> <li>Partial replacement vs complete replacement<ul> <li>Complete replacement only when AGI/ASI arrives</li> <li>Current agents useful for changing implementation details</li> <li>Still lack true world understanding and agency</li> </ul> </li> <li>Development workflow changes<ul> <li>Writing code requires less effort (most code generated by agents)</li> <li>QA and testing require significantly more effort</li> <li>More developers will exist, everyone developing software to some extent</li> <li>Complex systems (AWS-scale) still need experienced developers</li> </ul> </li> <li>Current model performance levels<ul> <li>Previous models: L3 level capability</li> <li>Recent models (o1): L5 level, sometimes writes better code than humans</li> <li>Still challenging: complex system design, distributed systems, performance optimization, complex ML problems with difficult math</li> </ul> </li> </ul> <p>Key Technical Gaps &amp; Challenges</p> <ul> <li>Self-assessment and feedback<ul> <li>No coding agent reliably knows when it successfully completed a task</li> <li>Agents don\u2019t push back on bad design decisions</li> <li>Limited ability with superhuman/rare tasks not in training data</li> </ul> </li> <li>Non-technical user gap<ul> <li>Fundamental misalignment between training data (developer-focused) and non-technical user requests</li> <li>Hard generalization problem between technical jargon and user needs</li> </ul> </li> <li>Scaffolding vs base model scaling<ul> <li>Scaffolding still essential despite model improvements</li> <li>Context engineering crucial for practical deployment</li> <li>APIs and interactive tools far more efficient than keyboard/mouse simulation</li> <li>Pre-training limited by ~2 trillion high-quality coding tokens available</li> </ul> </li> </ul> <p>Impact on Software Engineering Careers</p> <ul> <li>Junior developer training evolution<ul> <li>Some new engineers can\u2019t code but excel at prompting</li> <li>3-year career acceleration - juniors become \u201cmanagers of agents\u201d from day one</li> <li>Need balance: learn fundamentals without AI first, then use AI assistance</li> <li>Code review skills become more critical than before</li> </ul> </li> <li>Essential skills shifting<ul> <li>Less time writing code by hand (but still need basic ability)</li> <li>More time on architecture, design discussions, shepherding multiple PRs</li> <li>Code review and quality assessment increasingly important</li> </ul> </li> </ul> <p>Research Directions &amp; Benchmarks</p> <ul> <li>Benchmark limitations<ul> <li>Current benchmarks miss zero-to-one development scenarios</li> <li>Need evaluations beyond pass/fail - code quality, maintainability</li> <li>Missing: high-level task completion, non-coding problem solving</li> <li>Long-horizon coding benchmarks scarce (Commit Zero as example)</li> </ul> </li> <li>Research opportunities<ul> <li>Build and publish more benchmarks for community benefit</li> <li>Focus on solid RL knowledge and ablation studies</li> <li>Privacy solutions: VPC deployment, local models</li> <li>Memory management for long-running tasks</li> <li>Cultural diversity and taste in generated applications</li> </ul> </li> </ul>"},{"location":"talks/neurips-2025/xAI/","title":"Explainable AI (xAI) Tutorials - NeurIPS 2025","text":""},{"location":"talks/neurips-2025/xAI/#explainable-ai-deep-dive-1","title":"Explainable AI - Deep Dive 1","text":"<p>Tutorial Overview</p> <ul> <li>Tutorial page: link</li> <li>AI explainability evolution across three eras:<ul> <li>Before 2014: Linear models and trees for explanation</li> <li>2014-2020: Interpretable models (feature) \u2192 data attribution interpretable (DNNs)</li> <li>After 2022: Component attribution (LLM era)</li> </ul> </li> <li>Technical Deep Dive covers three attribution types:<ul> <li>Feature attribution</li> <li>Data attribution</li> <li>Component attribution</li> </ul> </li> </ul> <p>Attribution Problem</p> <ul> <li>General framework: Training data \u2192 model \u2192 output</li> <li>Three perspectives for explaining AI system outputs</li> <li>Unified mathematical notation:<ul> <li>Feature attribution scores: \u03c6\u1d62</li> <li>Data attribution scores: \u03c8\u2c7c</li> <li>Component attribution scores: \u03b3\u2096</li> </ul> </li> </ul> <p>Feature Attribution</p> <ul> <li>Core question: How do features impact the output?</li> <li>Applications:<ul> <li>Justify predictions and provide counterfactual explanations</li> <li>Identify spurious correlations (e.g., husky classified as wolf based on snow background)</li> </ul> </li> <li>Example: Loan application model explaining denial based on salary, credit score vs inappropriate reliance on gender</li> </ul> <p>Data Attribution</p> <ul> <li>Core question: Why this output for those training data points?</li> <li>Studies how training data influences model output</li> <li>Applications:<ul> <li>Characterize training data properties</li> <li>Determine data values and identify harmful training examples</li> </ul> </li> <li>Example: Fish classification traced back to semantically similar training image with coral background</li> </ul> <p>Component Attribution</p> <ul> <li>Core question: Why this output for these model components?</li> <li>Components can be: neurons, attention heads, layers, subnetworks</li> <li>Example: Language model answering \u201cWhen Mary and John went to the store, John gave\u2026\u201d \u2192 \u201cMary\u201d<ul> <li>Sparse attention head activation map shows only small subset needed for indirect object identification</li> </ul> </li> </ul> <p>Perturbation-Based Feature Attribution</p> <ul> <li>Direct perturbation:<ul> <li>Perturb features and observe output changes</li> <li>Problem: Feature interactions require considering all possible subsets</li> </ul> </li> <li>Game theoretic perturbation:<ul> <li>SHAP method using Shapley values</li> <li>Considers all 2^d marginal contributions</li> <li>Computational complexity: O(2^d) - not scalable for large feature sets</li> </ul> </li> <li>Perturbation mask learning:<ul> <li>Continuous and learnable masks instead of binary</li> <li>Generates saliency maps for computer vision</li> <li>Learned masking model applicable across multiple inputs</li> </ul> </li> </ul> <p>Gradient-Based Feature Attribution</p> <ul> <li>Key distinction: Feature gradients for attribution vs parameter gradients for training</li> <li>Measures output sensitivity with respect to input features</li> <li>SmoothGrad method:<ul> <li>Adds noise to create multiple input versions</li> <li>Aggregates gradients across noisy versions</li> <li>More robust than vanilla gradients</li> </ul> </li> <li>Produces increasingly intuitive saliency maps as methods evolved</li> </ul> <p>Linear Approximation for Feature Attribution</p> <ul> <li>LIME (Local Interpretable Model-agnostic Explanations):<ul> <li>Uses linear model for local approximation around decision boundary</li> <li>Trains on binary indicators (feature present/absent) rather than actual input values</li> <li>Linear coefficients become attribution scores</li> </ul> </li> <li>Successfully identified husky-wolf misclassification based on background snow</li> </ul> <p>Data Attribution Methods</p> <p>Perturbation-Based Data Attribution</p> <ul> <li>Leave-one-out (direct perturbation):<ul> <li>Remove one training data point, retrain model, observe changes</li> <li>Computationally expensive: requires n retraining cycles</li> </ul> </li> <li>Game-theoretic perturbation:<ul> <li>Data Shapley algorithm</li> <li>Requires 2^n marginal contributions with retraining</li> <li>Only approximate versions practical</li> </ul> </li> </ul> <p>Gradient-based Data Attribution</p> <ul> <li>No perturbation required - avoids retraining</li> <li>Gradient similarity method:<ul> <li>Compute gradients for test and training points</li> <li>Dot product measures similarity between gradient representations</li> <li>Problem: No causal interpretation, only similarity measure</li> </ul> </li> <li>Influence functions:<ul> <li>Approximates leave-one-out computationally efficiently</li> <li>Introduces Hessian matrix for second-order information</li> <li>Mathematical derivation recovers leave-one-out with modified training objective</li> </ul> </li> </ul> <p>Linear Approximation for Data Attribution</p> <ul> <li>Datamodel approach:<ul> <li>Skip training step - directly predict model output from training data</li> <li>Linear model G approximates relationship: training data \u2192 test output</li> </ul> </li> <li>Counterfactual data collection:<ul> <li>Each data point: subset of training data + prediction from model trained on that subset</li> <li>Binary indicator vector Z shows which training points included</li> <li>Linear coefficients become attribution scores</li> </ul> </li> </ul> <p>Perturbation-Based Component Attribution</p> <ul> <li>Causal mediation analysis:<ul> <li>Replace components with dummy values, observe output changes</li> <li>Neural Shapley applies game theory to capture component interactions</li> </ul> </li> <li>Mask learning and subnetwork probing:<ul> <li>Learnable continuous masks for component selection</li> <li>Optimize mask to recover original output while identifying important components</li> </ul> </li> <li>Causal tracing (three-run patching):<ul> <li>Clean input \u2192 model \u2192 output</li> <li>Perturbed input \u2192 model \u2192 baseline output</li> <li>Perturbed input + restored component K \u2192 model \u2192 recovered output</li> <li>Attribution score: difference between run 3 and run 2</li> </ul> </li> <li>Target perturbation:<ul> <li>Control model behavior by optimizing component values</li> <li>Example: Change \u201ccapital of France\u201d answer from Paris to London by modifying identified component</li> </ul> </li> </ul> <p>Gradient-Based Component Attribution</p> <ul> <li>Approximates three-run patching paradigm using Taylor approximation</li> <li>Efficiency advantage: Batches multiple inputs in single forward/backward pass</li> <li>Avoids expensive instance-wise patching step</li> <li>Gradient of perturbed output with respect to component \u00d7 component value difference</li> </ul> <p>Linear Approximation for Component Attribution</p> <ul> <li>Direct prediction: model components \u2192 test output</li> <li>Linear function G locally approximates component influence</li> <li>Counterfactual data collection from different component subsets</li> <li>Coefficients become component attribution scores</li> </ul> <p>Unified Framework</p> <ul> <li>Three attribution problems solved by three method categories:<ol> <li>Perturbations (direct, game-theoretic, mask learning)</li> <li>Gradients (similarity, influence functions, Taylor approximation)</li> <li>Linear approximations (LIME, datamodel, component linear models)</li> </ol> </li> <li>Additional methods exist beyond these three categories</li> <li>Mechanistic interpretability includes: sparse autoencoders, logit lens, linear probing</li> </ul>"},{"location":"talks/neurips-2025/xAI/#xai-deep-dive-2","title":"XAI - Deep Dive 2","text":"<p>Inherent Interpretability Framework</p> <ul> <li>Goal: Design interpretable yet performant language models at scale</li> <li>Alternative to post-hoc explanation methods (gradients, probes, influence functions)</li> <li>Core approach: Add interpretability constraints during training pipeline<ul> <li>Data constraints: Reprocess datasets for human understanding</li> <li>Architecture constraints: Modify transformer layers for traceability</li> <li>Representation constraints: Force interpretable concept encoding</li> <li>Training constraints: Add interpretability losses to standard task loss</li> </ul> </li> </ul> <p>Concept-Constrained Interpretable Models</p> <ul> <li>Architecture modification: Replace transformer layer with interpretable transformation<ul> <li>Maps representations to basis of known concepts (blue neurons) vs unknown (black neurons)</li> <li>Example: Paper review system with clarity, novelty, significance concepts</li> </ul> </li> <li>Loss function structure: L_total = L_task + \u03bb L_interp<ul> <li>Task loss: Standard next token prediction</li> <li>Interpretability loss: Forces specific neurons to represent target concepts</li> </ul> </li> <li>Scaling results: Achieves comparable performance to GPT models at billions of parameters<ul> <li>Tested on 33K supervised concepts, 160K unsupervised concepts</li> <li>Trained on billions of tokens with minimal performance degradation</li> </ul> </li> </ul> <p>Post-Hoc Method Limitations</p> <ul> <li>Feature attribution mismatch<ul> <li>Gradient-based methods often contradict occlusion analysis</li> <li>Example: Amino acid sequence task where gradients highlight distractors, not task-relevant features</li> </ul> </li> <li>Concept probing challenges<ul> <li>Can identify features in activations but not causal relevance to output</li> <li>Spurious correlations between causal and irrelevant features mislead probes</li> </ul> </li> <li>Training data attribution complexity<ul> <li>Influence functions require Hessian computation (billion \u00d7 billion matrices)</li> <li>Computational intractability and convexity assumptions limit practical application</li> </ul> </li> </ul> <p>Training Solutions for Interpretability</p> <ul> <li>Input masking during training<ul> <li>Randomly mask inputs to force robustness</li> <li>Aligns gradient behavior with human-expected occlusion patterns</li> <li>Makes models smooth and differentiable for better gradient interpretability</li> </ul> </li> <li>Adversarial training<ul> <li>Train on adversarial examples for off-manifold robustness</li> <li>Similar alignment benefits between gradients and perturbation analysis</li> </ul> </li> <li>Architecture modifications<ul> <li>Backpack Language Models: Rewrite transformer as generalized additive model</li> <li>Split token embeddings into sense vectors (fruit Apple vs company Apple)</li> <li>Enables surgical intervention and concept toggling</li> </ul> </li> </ul> <p>Scaling and Performance Results</p> <ul> <li>Concept-constrained models scale to billions of parameters with &lt;2% performance drop</li> <li>Training data attribution achievable in single forward pass (no Hessian computation)</li> <li>Prototype-based clustering losses enable direct tracing from outputs to training data</li> <li>Maintains competitive performance on standard LM benchmarks while providing interpretability</li> </ul>"},{"location":"topics/","title":"Topics Overview","text":"<p>This section contains detailed notes and research on various topics related to Large Language Models and AI.</p>"},{"location":"topics/#major-topics","title":"Major Topics","text":""},{"location":"topics/#ai-agents","title":"AI Agents","text":"<p>Learn about AI agent frameworks like AutoGen, CrewAI, and LangGraph. Includes paper reviews on LAMBDA, TaskWeaver, and more.</p>"},{"location":"topics/#rag-retrieval-augmented-generation","title":"RAG (Retrieval-Augmented Generation)","text":"<p>Comprehensive guide to RAG workflows, best practices, chunking strategies, and Graph RAG implementations.</p>"},{"location":"topics/#reinforcement-learning-fine-tuning","title":"Reinforcement Learning &amp; Fine-Tuning","text":"<p>Deep dive into RLHF (Reinforcement Learning from Human Feedback), alignment techniques, and the LIMA paper on efficient fine-tuning.</p>"},{"location":"topics/#prompt-engineering","title":"Prompt Engineering","text":"<p>Principles and best practices for crafting effective prompts, including structure, clarity, and complex task handling.</p>"},{"location":"topics/#evaluation-optimization","title":"Evaluation &amp; Optimization","text":""},{"location":"topics/#evaluation","title":"Evaluation","text":"<p>LLM evaluation methods, metrics (BLEU, ROUGE, perplexity), and benchmarks (GLUE, MMLU, BigBench, etc.).</p>"},{"location":"topics/#model-compression","title":"Model Compression","text":"<p>Techniques for model compression including quantization (PTQ, QAT), pruning, and knowledge distillation.</p>"},{"location":"topics/#specialized-areas","title":"Specialized Areas","text":""},{"location":"topics/#recommendation-systems","title":"Recommendation Systems","text":"<p>How generative models enhance recommendation systems.</p>"},{"location":"topics/#miscellaneous","title":"Miscellaneous","text":"<p>Other interesting topics including LLM fundamentals and hallucination phenomena.</p>"},{"location":"topics/#browse-by-content-type","title":"Browse by Content Type","text":"<ul> <li>Overview &amp; Frameworks: AI-Agent, Prompt, Evaluation</li> <li>Paper Reviews: AI-Agent, RAG, RFT</li> <li>Implementation Guides: RAG, Model Compression</li> <li>Research Notes: All topics</li> </ul>"},{"location":"topics/AI-Agent/","title":"AI Agents","text":"<p>AI Agents are autonomous systems that can perceive their environment, make decisions, and take actions to achieve specific goals. In the context of Large Language Models, AI agents leverage the reasoning and generation capabilities of LLMs to perform complex tasks.</p>"},{"location":"topics/AI-Agent/#whats-in-this-section","title":"What's in This Section","text":""},{"location":"topics/AI-Agent/#overview-frameworks","title":"Overview &amp; Frameworks","text":"<ul> <li>What are AI Agents and when to use them</li> <li>Three prominent frameworks: AutoGen, CrewAI, LangGraph</li> <li>Comparison of different agent architectures</li> <li>Lists of open-source agents and tools</li> </ul>"},{"location":"topics/AI-Agent/#paper-reviews","title":"Paper Reviews","text":"<p>Detailed reviews of cutting-edge research papers: - LAMBDA: A Large Model Based Data Agent - Can Large Language Models Serve as Data Analysts? - TaskWeaver: A Code-First Agent Framework</p>"},{"location":"topics/AI-Agent/#key-concepts","title":"Key Concepts","text":"<ul> <li>Autonomous Execution: Agents can break down complex tasks and execute them step-by-step</li> <li>Tool Use: Integration with external tools and APIs</li> <li>Multi-Agent Collaboration: Multiple agents working together on complex problems</li> <li>Code-First Approaches: Agents that generate and execute code to solve problems</li> </ul>"},{"location":"topics/AI-Agent/#popular-frameworks","title":"Popular Frameworks","text":"Framework Best For Key Features AutoGen Multi-agent conversations Easy agent communication, flexible roles CrewAI Task delegation Role-based agents, built-in collaboration LangGraph Complex workflows State management, graph-based execution"},{"location":"topics/AI-Agent/#get-started","title":"Get Started","text":"<ol> <li>Read the Overview &amp; Frameworks for a high-level understanding</li> <li>Dive into Paper Reviews for research insights</li> <li>Explore the diagrams and architecture patterns included</li> </ol> <p>Topics: AI Agent, Autonomous Systems, LLM Applications</p>"},{"location":"topics/AI-Agent/notes/","title":"AI Agents","text":""},{"location":"topics/AI-Agent/notes/#contents","title":"Contents","text":""},{"location":"topics/AI-Agent/notes/#ai-agent-types","title":"AI Agent Types","text":"<p>An \"agent\" is an automated reasoning and decision engine. The key agent components can include, but not limited to: - Break down of a complex question into smaller ones - Choosing an external tool to use and coming up with parameters for calling the Tool - Planning out a set of tasks - Storing previosuly completed tasks in a memory module.  </p>"},{"location":"topics/AI-Agent/notes/#when-to-use-agents","title":"When to Use Agents","text":"<p>Agents are highly beneficial when tasks require complex decision-making, autonomy, and adaptability. They excel in environments where the workflow is dynamic and involves multiple steps or interactions that can benefit from automation.</p>"},{"location":"topics/AI-Agent/notes/#when-not-to-use-agents","title":"When Not to Use Agents","text":"<ul> <li>Tasks that are straightforward, infrequent, or require minimal automation.</li> <li>Tasks that require deep domain-specific knowledge and expertise, e.g., legal or medical advice.</li> <li>Tasks that require a high level of human empathy, creativity, or subjective judgement, e.g., psychotherapy.</li> </ul>"},{"location":"topics/AI-Agent/notes/#ai-agent-frameworks","title":"AI Agent Frameworks","text":"<p>Three prominent frameworks for building AI agents are: - AutoGen - CrewAI - LangGraph</p>"},{"location":"topics/AI-Agent/notes/#comparison-summary","title":"Comparison summary","text":""},{"location":"topics/AI-Agent/notes/#open-source-agents","title":"Open Source Agents","text":"<ul> <li>AutoGPT</li> <li>BabyAGI</li> <li>SuperAGI</li> <li>ShortGPT</li> <li>ChatDev</li> <li>AutoGen</li> <li>MetaGPT</li> <li>Camel</li> <li>LoopGPT</li> <li>JARVIS</li> <li>OpenAGI</li> </ul>"},{"location":"topics/AI-Agent/notes/#papersbooks","title":"Papers/Books","text":"<ul> <li>My own literature review on AI agents</li> <li>Agent AI: Surveying The Horizons of Multimodal Interaction. Nice overview (book) on AI agents.</li> </ul>"},{"location":"topics/AI-Agent/notes/#online-articles","title":"Online Articles","text":"<ol> <li>Navigating the New Types of LLM Agents and Architectures </li> <li>AI Agents \u2014 From Concepts to Practical Implementation in Python. This uses CrewAI framework for implementation.4</li> <li>Top 11 Open-Source Autonomous Agents &amp; Frameworks: The Future of Self-Running AI</li> <li>Mastering Agents: LangGraph Vs Autogen Vs Crew AI</li> </ol>"},{"location":"topics/AI-Agent/paper-review/","title":"Paper Reviews","text":""},{"location":"topics/AI-Agent/paper-review/#content","title":"Content","text":"<ul> <li>LAMBDA: A Large Model Based Data Agent</li> <li>Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis</li> <li>TaskWeaver: A Code-First Agent Framework</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#lambda-a-large-model-based-data-agent","title":"LAMBDA: A Large Model Based Data Agent","text":"<p>LAMBDA is an open-source, code-free multi-agent data analysis system designed to make data analysis accessible to users without programming experience. The system has several key objectives:</p> <ul> <li>Enabling code-free data analysis by automatically generating programming code</li> <li>Seamlessly integrating human domain knowledge with AI capabilities</li> <li>Supporting data science education through interactive learning</li> <li>Automatically generating comprehensive analysis reports and exportable code</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#problem-statement-motivation","title":"Problem Statement &amp; Motivation","text":"<p>Existing research has not adequately addressed the high degree of flexibility required in real-world data analysis scenarios, particularly when it comes to incorporating custom algorithms or statistical models based on user preferences. Additionally, traditional function-calling approaches face significant challenges in statistical and data science applications:</p> <ul> <li>The sheer volume of APIs/functions, their complex interrelationships, and extensive documentation often exceed LLM capacity</li> <li>As the number of available APIs increases, the model's ability to accurately select appropriate functions deteriorates</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#system-architecture","title":"System Architecture","text":"<p>LAMBDA employs a dual-agent architecture consisting of:</p> <ol> <li>Data Scientist (Programmer) Agent</li> <li>Primary role: Code generation and user interaction</li> <li>Guided by system prompts defining its role, context, and I/O formats</li> <li> <p>Workflow:</p> <ul> <li>Writes code based on user/inspector instructions</li> <li>Executes code through kernel</li> <li>Generates comprehensive responses including results summary and next-step suggestions</li> </ul> </li> <li> <p>Inspector Agent</p> </li> <li>Primary role: Error detection and correction</li> <li>Analyzes execution errors in programmer's code</li> <li>Provides actionable revision suggestions for code improvement</li> <li>Works iteratively until code executes successfully or reaches maximum attempts</li> </ol> <p> Figure: Overview of LAMBDA showing the interaction between programmer agent for code generation and inspector agent for error evaluation. The system supports human intervention when needed.</p>"},{"location":"topics/AI-Agent/paper-review/#key-features","title":"Key Features","text":""},{"location":"topics/AI-Agent/paper-review/#knowledge-integration-mechanism","title":"Knowledge Integration Mechanism","text":"<ul> <li>Implements a Key-Value (KV) knowledge structure</li> <li>Key: Resource descriptions (e.g., function docstrings)</li> <li>Value: Corresponding code implementations</li> <li>Enables domain-specific task execution</li> <li>Provides flexibility for complex analysis challenges</li> <li>Facilitates easy incorporation of user resources into the agent system</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#technical-implementation","title":"Technical Implementation","text":"<ul> <li>Uses IPython as the system kernel for sequential data processing</li> <li>Supports comprehensive report generation including:</li> <li>Data processing steps</li> <li>Data visualizations</li> <li>Model descriptions</li> <li>Evaluation results</li> <li>Enables code export functionality</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#user-interface-interaction","title":"User Interface &amp; Interaction","text":"<ul> <li>Chat-based interface for natural interaction</li> <li>Step-by-step guided prompting</li> <li>Human-in-the-loop design allowing direct code modification</li> <li>Extensive prompt templates for various tasks:</li> <li>Data analysis</li> <li>Dataset handling</li> <li>Error resolution</li> <li>Knowledge integration</li> <li>Code debugging</li> </ul> <p> Figure: Detailed view of the collaborative process between programmer and inspector agents.</p>"},{"location":"topics/AI-Agent/paper-review/#resources","title":"Resources","text":"<ul> <li>Research Paper</li> <li>Live Demo</li> <li>Source Code</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#can-large-language-models-serve-as-data-analysts-a-multi-agent-assisted-approach-for-qualitative-data-analysis","title":"Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis","text":"<p>Qualitative research is a type of research that focuses on collecting and analyzing non-numerical data (e.g., text, video, or audio) to understand concepts, opinions, or experiences.</p> <p>This paper explores using LLMs to automate and expedite qualitative data analysis processes. The model adeptly interprets massive volumes of textual data and interview transcripts to autonomously perform the chosen approach for qualitative data analysis.</p> <p>In the framework, each agent in the system is a specialized instance of an LLM, trained to handle different aspects of qualitative data analysis.</p> <p> Figure: A workflow overview of the proposed system for automation of qualitative data analysis.</p>"},{"location":"topics/AI-Agent/paper-review/#resource","title":"Resource","text":"<ul> <li>Research Paper</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#taskweaver-a-code-first-agent-framework","title":"TaskWeaver: A Code-First Agent Framework","text":"<p>TaskWeaver is a framework that converts user requests into executable code and treats user-defined plugins as callable functions. It provides support for rich data structures, flexible plugin usage, and dynamic plugin selection, leveraging LLM coding capabilities for complex logic.</p> <p>Key limitations of existing frameworks that TaskWeaver addresses: - Lack of native support for handling rich data structures - Limited configuration options for incorporating domain knowledge  - Inability to meet diverse user requirements</p> <p>The core architecture consists of three main components:</p> <ol> <li>Planner: The system entry point that:</li> <li>Breaks down user requests into subtasks and manages execution with self-reflection</li> <li> <p>Transforms execution results into human-readable responses</p> </li> <li> <p>Code Interpreter (CI): Contains two sub-components:</p> </li> <li>Code Generator (CG): Generates code for subtasks from the Planner using available plugins</li> <li> <p>Code Executor (CE): Executes generated code and maintains execution state</p> </li> <li> <p>Memory Module: Centralizes chat history between user and internal roles</p> </li> </ol> <p> Figure: The overview of TaskWeaver.</p> <p>A key use case is anomaly detection in databases, which uses a two-layer planning process: 1. Planner generates high-level steps to fulfill the request 2. CI devises detailed execution plans with chain-of-thought reasoning and code generation</p> <p>The workflow begins with the Planner receiving a user query along with CI descriptions (plugin/function documentation). The CG receives comprehensive plugin definitions including function names, descriptions, arguments and return values. Execution results flow back to the Planner to determine next steps.</p> <p> Figure: the workflow of the anomaly detection use case.</p>"},{"location":"topics/AI-Agent/paper-review/#component-details","title":"Component Details","text":"<p>Planner As the system controller, the Planner: - Receives and decomposes user queries into sub-tasks - Generates initial plans based on LLM knowledge and domain examples - Refines plans considering sub-task dependencies - Assigns tasks to CI for code generation - Updates plans based on execution results following ReAct pattern - Manages the process until completion</p> <p>Code Generator (CG) The CG synthesizes Python code by: - Combining plugin system and code interpreter capabilities - Leveraging user-customized plugins and examples - Using plugin schemas to understand capabilities - Ensuring plugin implementations match schemas</p> <p>Code Executor (CE) The CE handles code execution by: - Running code generated by CG - Managing dependent modules and plugins - Preserving context and logs - Returning results to Planner</p>"},{"location":"topics/AI-Agent/paper-review/#resource_1","title":"Resource","text":"<ul> <li>Research Paper</li> <li>Source Code</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#tradingagents-multi-agents-llm-financial-trading-framework","title":"TradingAgents: Multi-Agents LLM Financial Trading Framework","text":""},{"location":"topics/AI-Agent/paper-review/#resource_2","title":"Resource","text":"<ul> <li>Resource Paper</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#sciagents-automating-scientific-discovery-through-multi-agent-intelligent-graph-reasoning","title":"SciAgents: Automating Scientific Discovery through Multi-Agent Intelligent Graph Reasoning","text":"<p>publish date: 2024-09-09 Objective: Developing AI systems that can not only explore and exploit existing knowledge to make significant scientific discoveries but also automate and replicate the broader research process, including acquiring relevant knowledge and data. SciAgents, an approach that leverages three core concepts: (1) the use of large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts, (2) a suite of large language models (LLMs) and data retrieval tools, and (3) multi-agent systems with in-situ learning capabilities.</p>"},{"location":"topics/AI-Agent/paper-review/#resource_3","title":"Resource","text":"<ul> <li>Resource Paper</li> <li>Source Code</li> </ul>"},{"location":"topics/Others/","title":"Miscellaneous Topics","text":"<p>Other interesting topics and findings related to Large Language Models.</p>"},{"location":"topics/Others/#contents","title":"Contents","text":"<ul> <li>Miscellaneous Notes - LLM fundamentals, hallucination phenomena, and various insights</li> </ul> <p>A collection of interesting topics that don't fit neatly into other categories.</p>"},{"location":"topics/Others/notes/","title":"Intro to LLM Fundamentals: What are LLMs and How do they come to be?","text":"<ul> <li>Serrano, S., Brumbaugh, Z., &amp; Smith, N. A. (2023). Language Models: A Guide for the Perplexed. http://arxiv.org/abs/2311.17301</li> </ul> <p>This paper provides a comprehensive, non-technical introduction to language models (LMs), including large language models (LLMs). It offers a high-level overview of how language models, including LLMs, are trained, evaluated, and utilized. I found the sections discussing data particularly insightful, and the discussion on practical applications of LLMs thought-provoking.</p> <p>Data is crucial both for training and testing. Testing serves as an indicator of the system's output quality and should not be used for any other purpose prior to the final test. NLP tasks depend heavily on both the quality and quantity of training data. Understanding a model involves knowing its training data. However, nowadays, companies developing LLMs are reluctant to share their training datasets. This leads to concerns about hidden training data; for instance, if a model answers a complex question accurately and clearly, we should be impressed only if we are certain that the question and answer were not in the training data. Without access to the training data, we cannot verify whether the model is genuinely being tested fairly or simply recalling answers it has already seen.</p> <p>The paper also discusses \"hallucination,\" where the content generated by LLMs is inaccurate or nonfactual. From my own experience using ChatGPT, I have noticed this issue as well. The paper suggests that while LLMs rely on training data, they do not directly access this data; instead, they seem to encode patterns from the data but do not \"remember\" the data precisely at all times. Thus, for topics with ample supporting data and straightforward tasks, the likelihood of hallucination is lower. Conversely, with more complex tasks or less-discussed subjects, hallucination becomes less surprising. Moreover, since the training data may contain incorrect or biased information, the model might encode these inaccuracies.</p> <p>The discussion on how to effectively use LLMs is also worth considering. The paper mentions that using LLMs to summarize newspapers may not be worthwhile since the first paragraph of a news article typically summarizes the entire piece. This perspective is particularly interesting in today's context, where there is significant buzz around employing LLMs. However, the challenge remains on how to fully leverage these technologies to solve everyday problems, which continues to be an area needing exploration.</p>"},{"location":"topics/Prompt/","title":"Prompt Engineering","text":"<p>Principles and best practices for crafting effective prompts to get the most out of Large Language Models.</p>"},{"location":"topics/Prompt/#contents","title":"Contents","text":"<ul> <li>Prompt Engineering Guide - Structure, clarity, specificity, and techniques for complex tasks</li> </ul> <p>Master the art of prompt engineering to improve LLM outputs and handle complex tasks effectively.</p>"},{"location":"topics/Prompt/notes/","title":"Prompt","text":""},{"location":"topics/Prompt/notes/#table-of-contents","title":"Table of Contents","text":""},{"location":"topics/Prompt/notes/#what-is-prompt-engineering","title":"What is Prompt Engineering?","text":"<p>It is a practice with a set of guidelines to craft precise, concise, creative wording of text to instruct an LLM to carry out a task. </p>"},{"location":"topics/Prompt/notes/#prompt-principles-and-guides","title":"Prompt Principles and Guides","text":"<p>Refer to Paper [1] 1. Prompt Structure and Clarity: Integrate the intended audience in teh prompt. e.g., integrate the intended audience in the prompt such as the audience is an expert in the field. 2. Specificity and Information: Implement example-driven prompting. E.g., Add to your prompt the following phrase \u201cEnsure that your answer is unbiased and does not rely on stereotypes.\u201d; 3. User Interaction and Engagement: Allow the model to ask precise details and requirements until it has enough information to provide the needed response. E.g. \"From now on, I would like you to ask me questions to...\" 4. Content and Language Stype: Instruct the tone and style of response. 5. Complex Tasks and Coding Prompts: Break down complex tasks into a sequence of simple steps as prompts.</p> <p> </p>"},{"location":"topics/Prompt/notes/#reference","title":"Reference","text":""},{"location":"topics/Prompt/notes/#papers","title":"Papers","text":"<ol> <li>Bsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. \u201cPrincipled Instructions Are All You Need for Questioning LLaMA-\u00bd, GPT-3.5/4.\u201d arXiv, January 18, 2024. http://arxiv.org/abs/2312.16171.</li> </ol>"},{"location":"topics/Prompt/notes/#online-articles","title":"Online Articles","text":"<ol> <li>Best Prompt Techniques for Best LLM Responses</li> </ol>"},{"location":"topics/RAG/","title":"RAG (Retrieval-Augmented Generation)","text":"<p>Retrieval-Augmented Generation (RAG) enhances Large Language Models by combining them with external knowledge retrieval systems.</p>"},{"location":"topics/RAG/#contents","title":"Contents","text":"<ul> <li>RAG Guide &amp; Best Practices - Complete RAG workflow, chunking strategies, vector databases, and Graph RAG</li> <li>Paper Reviews - Golden-Retriever paper and advanced retrieval techniques</li> </ul> <p>Learn about RAG workflows, best practices, and cutting-edge research in retrieval-augmented generation.</p>"},{"location":"topics/RAG/notes/","title":"RAG","text":""},{"location":"topics/RAG/notes/#contents","title":"Contents","text":"<ul> <li>Introduction</li> <li>Papers</li> <li>Online Articles </li> <li>Implementation</li> </ul>"},{"location":"topics/RAG/notes/#introduction","title":"Introduction","text":"<p>RAG stands for Retrieval-Augumented Generation. RAG system works two steps: 1. Retrieve: It retrieves relevant information from a large corpus of text. 2. Generate: It generates a response based on the retrieved information. Common Use-case: question answering, document summarization, content generation.  Why do we need RAG? 1. avoid hallucination 2. timeliness 3. LLMs cannot access private data, feed more internal/user private data to get customized results. 4. Answer constraint. </p> <p>A naive RAG mainly consists of the following steps: 1. Indexing: Cleaning and extracting the raw text into standardized plain text -&gt; Chunking -&gt; transformed into vector via embedding -&gt; create (key, value) pairs, which is (index, vector) pairs. 2. Retrieval: users query processed by an encoding model -&gt; query embedding -&gt; similarity search on a vector database -&gt; top-k results are retrieved. 3. Generation: user query and retrieved documents are fed into a prompt template -&gt; generate the response.</p>"},{"location":"topics/RAG/notes/#best-practices-of-rag","title":"Best practices of RAG","text":"<p>In Paper [2]:</p> <p>A typical RAG workflow usually contains multiple intervening processing steps: query classification (determining whether retrieval is necessary for a given input query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the order of retrieved documents based on their relevance to the query), repacking (organizing the retrieved documents into a structured one for better generation), summarization (extracting key information for response generation from the repacked document and eliminating redundancies) modules. Implementing RAG also requires decisions on the ways to properly split documents into chunks, the types of embeddings to use for semantically representing these chunks, the choice of vector databases to efficiently store feature representations, and the methods for effectively fine-tuning LLMs</p>"},{"location":"topics/RAG/notes/#rag-workflow","title":"RAG Workflow","text":"<ol> <li>Query Classification. For tasks entirely based on user-given information, we denote as \u201csufficient\u201d, which need not retrieval; otherwise, we denote as \u201cinsufficient\u201d, and retrieval may be necessary.</li> <li>Chunking. Three types of chunking: token sentence, and semantic levels. </li> <li>Token-level chunking: split the text into tokens, usually with a fixed length.</li> <li>Sentence-level chunking: split the text into sentences.</li> <li>Semantic-level chunking: take the embeddings of every sentence in the document, comparing the similarity of all sentences with each other, and then grouping sentences with the most similar embeddings together.</li> <li>Vector databases. Store embedding vectors with their metadata, enabling efficient retrival of documents relevant to queries through various indexing and approximate nearest neighbor search. </li> <li>Retrieval Method. The recommended steps:  </li> <li>query rewriting.</li> <li>query decomposition.</li> <li>pseudo-document generation. This approach generates a hypothetical document based on the user query and uses the embedding of hypothetical answers to retrieve similar documents. One notable implement is HyDE.</li> <li>Hybrid search. Combining sparse retrieval (BM25) and dense retrieval (original embedding). The weights between the two retrieval methods can be appropriately adjusted. </li> <li>Reranking. Enhance the relevance of the retrieved documents.  </li> <li>Document repacking. The performance of subsequent processes, such as LLM response generation, may be affected by the order documents are provided.</li> <li>Summarization. Extractive or abstractive. </li> </ol>"},{"location":"topics/RAG/notes/#graph-rag","title":"Graph RAG","text":"<p>Why RAG is not enough? </p> <p>Traditional RAG systems, while powerful, have several limitations:</p> <ol> <li> <p>Loss of Structural Information: Standard RAG treats documents as independent chunks, losing important relationships and connections between pieces of information. Real-world knowledge often has inherent graph-like structures (e.g., relationships between entities, hierarchical information, or causal chains).</p> </li> <li> <p>Limited Context Understanding: When retrieving information, traditional RAG looks at chunks in isolation. This can miss broader context that might be spread across multiple related documents or sections.</p> </li> <li> <p>Inability to Handle Complex Queries: Questions that require connecting multiple pieces of information or understanding relationships between entities are difficult for traditional RAG to handle effectively.</p> </li> <li> <p>Static Document View: Traditional RAG typically treats documents as static pieces of text, without capturing how information evolves or relates to other pieces of knowledge over time.</p> </li> </ol> <p>Graph RAG addresses these limitations by: - Representing knowledge as a graph structure where nodes contain information and edges represent relationships - Preserving structural information during retrieval - Enabling multi-hop reasoning across connected pieces of information - Supporting more complex query patterns that require traversing relationships</p> <p>Useful videos - GraphRAG: The Marriage of Knowledge Graphs and RAG: Emil Eifrem</p>"},{"location":"topics/RAG/notes/#papers","title":"Papers","text":"<p>My literature review: RAG 1.  Lewis, Patrick, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, et al. \u201cRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\u201d arXiv, April 12, 2021. http://arxiv.org/abs/2005.11401. The first paper talks about RAG - models which combine pre-trained parametric and non-parametric memory for language generation. RAG models, the parametric memory is a pre-trained seq2seq transformer and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. other resource: Youtube video - https://www.youtube.com/watch?v=JGpmQvlYRdU (by the Author of the paper) - https://www.youtube.com/watch?v=dzChvuZI6D4 (explanation of the paper) 2. Wang, Xiaohua, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, et al. \u201cSearching for Best Practices in Retrieval-Augmented Generation.\u201d arXiv, July 1, 2024. http://arxiv.org/abs/2407.01219. it gives an overview of current practice of RAG. A good tech blog to explain the paper.</p> <ol> <li> <p>Shi, Yunxiao, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, and Min Xu. \u201cEnhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems.\u201d arXiv, July 15, 2024. http://arxiv.org/abs/2407.10670.</p> <p>This paper introduces 4 modules to solving several key challenges with RAG.  </p> </li> <li> <p>Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496, 2022.</p> <p>this is the paper talks about HYDE method.  </p> </li> </ol> <p>Graph RAG - A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases</p>"},{"location":"topics/RAG/notes/#online-articles","title":"Online Articles","text":""},{"location":"topics/RAG/notes/#introduction_1","title":"Introduction","text":"<ul> <li>Introduction to RAG \u2014 GenAI Systems for Knowledge</li> <li>A Brief Introduction to Retrieval Augmented Generation(RAG)</li> <li>The Best Practices of RAG</li> </ul> <p>Chucking - Semantic Chunking for RAG</p> <p>Retrieval - HYDE: Revolutionising Search with Hypothetical Document Embeddings</p>"},{"location":"topics/RAG/notes/#implementation","title":"Implementation","text":"<ul> <li>How do domain-specific chatbots work? An Overview of Retrieval Augmented Generation (RAG)</li> <li>A beginner\u2019s guide to building a Retrieval Augmented Generation (RAG) application from scratch</li> <li>Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation</li> </ul> <p>Chucking - Semantic Chunking for RAG</p> <p>Retrieval - Power of Hypothetical Document Embeddings: An In-Depth Exploration of HyDE - Exploring Query Rewriting. This blog uses <code>LlamaIndex</code> and <code>LangChain</code> to demostrate several techniques for query rewriting: Hypothetical Document Embeddings (HyDE), Rewrite-Retrieve-Read, Step-Back Prompting, and etc..</p> <p>Graph RAG - Enhancing RAG with Graph</p>"},{"location":"topics/RAG/paper-review/","title":"Paper Reviews","text":""},{"location":"topics/RAG/paper-review/#content","title":"Content","text":"<ul> <li>Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base</li> </ul>"},{"location":"topics/RAG/paper-review/#golden-retriever-high-fidelity-agentic-retrieval-augmented-generation-for-industrial-knowledge-base","title":"Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base","text":"<p>Publish date: 2024-07-20</p> <p>Challenges in Traditional RAG: - Ambiguous jargon in user queries often leads to retrieval of irrelevant documents, a common issue in industrial knowledge bases. - User queries typically lack context, making it difficult to retrieve the most relevant documents.</p> <p>Golden-Retriever's Solution: - Enhances the traditional RAG framework by incorporating a reflection-based question augmentation step before document retrieval.</p> <p></p> <p>Figure: The framework of Golden-Retriever. It reflects upon the question, identifies its context, and augments the question by querying a jargon dictionary before document retrieval. The augmented question allows RAG to faithfully retrieve the most relevant documents despite ambiguous jargon or lack of explicit context.</p> <p>Components of Golden-Retriever: - Offline Component:   - Enhances the document database to improve the relevance of retrieved documents before deploying the knowledge base chatbot.   - Collects various documents (e.g., slides, images with text, tables) from the company.   - Extracts text from images and tables, breaks it into smaller chunks (~4,000 tokens), summarizes each chunk using LLM, and saves them in the document database.</p> <ul> <li>Online Component:</li> <li>Identify Jargon and Abbreviations: Prompts the system to identify all jargon and abbreviations in user queries for further processing.</li> <li>Context Identification: Uses a prompt template with pre-specified context names and descriptions to identify the context of the question. Applies few-shot examples with Chain-of-Thought reasoning.</li> <li>Query Jargons: Utilizes a code-based approach to synthesize SQL queries.</li> <li>Augment Question: Enhances the original user question with additional information, structuring it for effective document retrieval.</li> <li>Handle Query Misses: Implements a fallback mechanism when jargon terms are not found, generating a response indicating missing information. Instructs users to check jargon spelling or contact the knowledge base manager to add new terms.</li> </ul> <p> Figure: Left: the workflow diagram of the online inference part of Golden-Retriever. Right: example interactions between the system and the LLM at the intermediate steps of the workflow. The system prompts LLM to generate intermediate responses, which are saved, accessed, and used for future steps in the workflow.</p>"},{"location":"topics/RAG/paper-review/#resource","title":"Resource","text":"<ul> <li>Research Paper</li> <li>Source Code</li> </ul>"},{"location":"topics/RFT/","title":"Reinforcement Learning &amp; Fine-Tuning","text":"<p>Deep dive into RLHF (Reinforcement Learning from Human Feedback) and efficient fine-tuning techniques for LLMs.</p>"},{"location":"topics/RFT/#contents","title":"Contents","text":"<ul> <li>RLHF Overview - Process, benefits, and challenges of reinforcement learning for language model fine-tuning</li> <li>Paper Reviews - LIMA paper on alignment with minimal data</li> </ul> <p>Explore alignment techniques, RLHF processes, and the surprising effectiveness of small, high-quality datasets.</p>"},{"location":"topics/RFT/notes/","title":"Reinforcement Learning on Language Models","text":""},{"location":"topics/RFT/notes/#overview","title":"Overview","text":"<p>Reinforcement learning (RL) is a method where models learn from feedback, often in the form of rewards, to improve performance. When applied to fine-tuning small language models, RL from human feedback (RLHF) helps align these models with human preferences, making them better at specific tasks like text generation or question answering. While much of the research focuses on large models, small language models\u2014those with fewer parameters, often under 1 billion\u2014can also benefit, especially for niche applications where efficiency is key.</p>"},{"location":"topics/RFT/notes/#process-and-benefits","title":"Process and Benefits","text":"<p>RLHF involves several steps: starting with a pre-trained model, fine-tuning it with supervised learning, training a reward model based on human feedback, and then using RL to optimize the model for higher rewards. For small language models, this can enhance performance on tasks like code generation or customer support chatbots, offering lower latency and reduced memory use compared to large models.</p>"},{"location":"topics/RFT/notes/#challenges-and-considerations","title":"Challenges and Considerations","text":"<p>Small language models may struggle with capturing complex human preferences due to limited capacity, and computational resources can be a bottleneck. However, techniques like parameter-efficient fine-tuning (PEFT) can help mitigate these issues, making RLHF feasible for smaller models.</p>"},{"location":"topics/RFT/notes/#unexpected-detail","title":"Unexpected Detail","text":"<p>An interesting finding is that small models, when fine-tuned with RLHF, can sometimes outperform larger models in specific, narrow tasks, such as code review accuracy, due to their efficiency and lower latency, as seen in recent studies (NVIDIA Technical Blog).</p>"},{"location":"topics/RFT/notes/#survey-note-reinforcement-learning-for-fine-tuning-small-language-models","title":"Survey Note: Reinforcement Learning for Fine-Tuning Small Language Models","text":""},{"location":"topics/RFT/notes/#introduction","title":"Introduction","text":"<p>Fine-tuning language models is a critical process in adapting pre-trained models to specific tasks or domains, leveraging their general language understanding for specialized applications. Reinforcement learning, particularly reinforcement learning from human feedback, has emerged as a powerful technique for aligning these models with human preferences, especially in tasks where direct supervision is challenging. While much of the recent research has focused on LLMs, there is growing interest in applying these techniques to small language models (SLMs), defined as models with fewer parameters (often under 1 billion), due to their efficiency and suitability for resource-constrained environments. This survey note explores the current state of RLHF for fine-tuning SLMs, reviewing key methodologies, challenges, and research findings, with a focus on their applicability and limitations.</p>"},{"location":"topics/RFT/notes/#background-on-fine-tuning-and-rlhf","title":"Background on Fine-Tuning and RLHF","text":"<p>Fine-tuning involves further training a pre-trained language model on a smaller, task-specific dataset to adapt it for particular applications, such as text classification, question answering, or content generation. Traditionally, this has been done using supervised learning, where the model is trained on labeled data. However, for tasks requiring nuanced human judgment, RL offers an alternative by allowing the model to learn from feedback in the form of rewards or punishments.</p> <p>RLHF specifically integrates RL with human feedback, aiming to align model outputs with human preferences. The process typically includes: - Pre-trained Language Model: Starting with a model pre-trained on a large corpus of text, such as BERT or smaller transformer-based models. - Supervised Fine-Tuning (SFT): Fine-tuning the model on a dataset of human-generated text relevant to the task, helping it understand task-specific requirements. - Reward Model Training: Training a separate model to predict the quality of the language model's outputs based on human preferences, often using pairwise comparisons where humans rank outputs for given inputs. - RL Fine-Tuning: Using the reward model to guide further fine-tuning with RL algorithms, such as Proximal Policy Optimization (PPO), to maximize the expected reward, aligning the model more closely with human values.</p> <p>This methodology has been pivotal in developing models like ChatGPT, which rely on RLHF for producing helpful and safe responses (AssemblyAI Blog).</p> <p></p> <p>Figure 1. A preference (or reward) model could be used to further train the baseline model to prioritize responses with higher preference scores.</p>"},{"location":"topics/RFT/notes/#key-research-on-rlhf-for-language-models","title":"Key Research on RLHF for Language Models","text":"<p>The development of RLHF for language models has been marked by several seminal works, primarily focused on LLMs. Below is a table summarizing key papers and their contributions:</p> Year Authors Title Contribution 2017 Christiano et al. Deep Reinforcement Learning from Human Preferences Introduced RLHF, demonstrating its use in general RL settings, laying groundwork. 2019 Ziegler et al. Fine-Tuning Language Models from Human Preferences Explored early applications of RLHF to language models, focusing on preference alignment. 2020 Stiennon et al. Learning to Summarize with Human Feedback Applied RLHF to text summarization, showing improved quality based on human preferences. 2022 Ouyang et al. Training Language Models to Follow Instructions with Human Feedback Developed InstructGPT, using RLHF for general-purpose instruction-following models. 2022 Bai et al. Training a Helpful and Harmless Assistant with RLHF Refined RLHF for creating safe and helpful assistants, addressing alignment issues. 2022 Menick et al. Scaling Laws for Reward Model Overoptimization Investigated challenges of reward model overoptimization in RLHF, impacting performance. 2020 Gabriel et al. The Limitations of Reinforcement Learning from Human Feedback Discussed ethical concerns and limitations, such as bias amplification and preference definition. <p>These papers highlight the evolution of RLHF, from its theoretical foundations to practical implementations, primarily with LLMs. For instance, Ouyang et al. (2022) demonstrated how RLHF enabled InstructGPT to follow complex instructions, a significant step toward general-purpose assistants (arXiv).</p>"},{"location":"topics/RFT/notes/#application-to-small-language-models","title":"Application to Small Language Models","text":"<p>While most research has focused on LLMs, the principles of RLHF can be applied to SLMs, though with specific considerations. SLMs, often with parameters in the range of tens to hundreds of millions, are designed for efficiency, lower latency, and deployment in resource-constrained environments. Recent studies suggest that SLMs can be competitive for real-world tasks, particularly when fine-tuned for specific, narrow applications (Medium).</p> <ul> <li>Efficiency and Resource Constraints: SLMs require less computational power, making RLHF more accessible for researchers and practitioners with limited resources. Techniques like parameter-efficient fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), can further reduce the computational burden, enabling RLHF on smaller models (SuperAnnotate Blog).</li> <li>Task Specificity: SLMs are often better suited for niche tasks, such as code review or customer support chatbots, where large models might be overkill. For example, a study by NVIDIA showed that a fine-tuned Llama 3 8B model with LoRA improved code review accuracy by 18%, outperforming larger models in specific tasks (NVIDIA Technical Blog).</li> <li>Challenges: SLMs may have limited capacity to capture complex human preferences, potentially leading to poorer generalization compared to LLMs. Additionally, the data requirements for training reward models in RLHF might be more challenging for SLMs, given their smaller parameter space and potential for overfitting.</li> </ul> <p>Despite these challenges, recent blog posts and practical guides suggest that RLHF can be adapted for SLMs, with careful planning and monitoring (premai.io Blog). For instance, the process involves starting with small datasets (5-10%) to test performance and iteratively refining hyperparameters, which is particularly feasible for SLMs due to their lower resource needs (Encora Insights).</p>"},{"location":"topics/RFT/notes/#challenges-and-limitations","title":"Challenges and Limitations","text":"<p>RLHF, whether applied to LLMs or SLMs, faces several challenges: - Reward Model Robustness: As highlighted by Menick et al. (2022), reward models can be exploited by the policy model, leading to overoptimization and degraded performance (arXiv). - Data Quality and Quantity: Human feedback can be costly and inconsistent, with annotators often disagreeing, adding variance to training data (Hugging Face Blog). For SLMs, the smaller parameter space might exacerbate issues with data scarcity. - Ethical Concerns: Gabriel et al. (2020) discuss potential biases in RLHF, such as amplifying existing biases in training data, which could be more pronounced in SLMs due to their limited capacity to mitigate such biases (arXiv).</p> <p>For SLMs specifically, additional challenges include: - Computational Efficiency: While SLMs are less resource-intensive, RLHF still requires significant computational power for reward model training and iterative optimization, which can be a bottleneck for smaller setups. - Generalization: SLMs may struggle to generalize across diverse tasks, particularly when fine-tuned with RLHF, due to their limited capacity compared to LLMs.</p>"},{"location":"topics/RFT/notes/#future-directions","title":"Future Directions","text":"<p>Given the current state of research, future work could focus on: - Developing tailored RLHF methodologies for SLMs, addressing their unique constraints and capabilities. - Exploring hybrid approaches, combining RLHF with other fine-tuning techniques like PEFT, to enhance performance on SLMs. - Investigating the trade-offs between model size, performance, and computational cost in RLHF, potentially leading to guidelines for selecting the appropriate model size for specific tasks.</p>"},{"location":"topics/RFT/notes/#conclusion","title":"Conclusion","text":"<p>RLHF has proven to be a transformative technique for aligning language models with human preferences, with significant advancements driven by research on LLMs. While direct applications to SLMs are less documented, the principles of RLHF can be adapted, offering potential for efficient, task-specific fine-tuning. Challenges such as computational efficiency, data quality, and generalization need further exploration, particularly for SLMs. As the field progresses, continued research will be essential to fully realize the potential of RLHF in fine-tuning small language models, ensuring they meet the needs of diverse, resource-constrained applications.</p>"},{"location":"topics/RFT/notes/#reference","title":"Reference","text":""},{"location":"topics/RFT/notes/#github-projects","title":"GitHub Projects","text":""},{"location":"topics/RFT/notes/#articles-blogs","title":"Articles &amp; Blogs","text":"<ul> <li>Fine-Tuning Small Language Models Practical Recommendations Medium</li> <li>Fine-Tuning Small Language Models for Code Review Accuracy NVIDIA Technical Blog</li> <li>Fine-Tuning and Small Language Models Blog Post premai.io Blog</li> <li>The Full Story of Large Language Models and RLHF AssemblyAI Blog</li> <li>Illustrating Reinforcement Learning from Human Feedback Hugging Face Blog</li> <li>Fine-Tuning Large Language Models Challenges and Best Practices Encora Insights</li> <li>LLM Fine-Tuning Techniques and Challenges SuperAnnotate Blog</li> </ul>"},{"location":"topics/RFT/notes/#online-courses","title":"Online Courses","text":"<ul> <li>Paper: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning Good youtube video discusses the paper from fundemental knowledge to its implementation in DeepSeek model.</li> </ul>"},{"location":"topics/RFT/notes/#research-papers","title":"Research Papers","text":"<ul> <li>Deep Reinforcement Learning from Human Preferences arXiv</li> <li>Learning to Summarize with Human Feedback arXiv</li> <li>Fine-Tuning Language Models from Human Preferences arXiv</li> <li>Training Language Models to Follow Instructions with Human Feedback arXiv</li> <li>Training a Helpful and Harmless Assistant with RLHF arXiv</li> <li>Scaling Laws for Reward Model Overoptimization arXiv</li> <li>The Limitations of Reinforcement Learning from Human Feedback arXiv</li> </ul>"},{"location":"topics/RFT/paper-review/","title":"Paper Reviews","text":""},{"location":"topics/RFT/paper-review/#content","title":"Content","text":""},{"location":"topics/RFT/paper-review/#lima-less-is-more-for-alignment","title":"LIMA: Less Is More for Alignment","text":"<p>ArXiv</p>"},{"location":"topics/RFT/paper-review/#summary-of-the-paper","title":"Summary of the Paper","text":"<p>This paper demonstrates that a 65B parameter LLaMA language model can achieve strong performance using only 1,000 high-quality training examples (prompts + responses) for fine-tuning. The authors detail the process of selecting high-quality training data and show that their approach even outperforms OpenAI's RLHF-based DaVinci-003.</p>"},{"location":"topics/RFT/paper-review/#key-hypothesis","title":"Key Hypothesis","text":"<p>The paper introduces the Superficial Alignment Hypothesis:</p> <p>A model's knowledge and capabilities are learned almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users.</p> <p>If this hypothesis holds, it explains why fine-tuning a pretrained model with a small, high-quality dataset can be sufficient.</p>"},{"location":"topics/RFT/paper-review/#data-selection-and-quality","title":"Data Selection and Quality","text":"<ul> <li>Among the 1,000 examples, 250 were authored by the researchers to ensure high quality. Many responses acknowledge the question before providing the answer, resembling a \"chain of thought\" approach.</li> <li>The authors emphasize that diversity in input and quality in output are more impactful than simply increasing the quantity of training data.</li> </ul>"},{"location":"topics/RFT/paper-review/#evaluation","title":"Evaluation","text":"<p>The authors evaluated LIMA using human evaluation and GPT-4 evaluation, comparing it against DaVinci-003 (RLHF-based) and other large language models. LIMA demonstrated competitive performance.</p> <p> Human preference evaluation, comparing LIMA to 5 different baselines across 300 test prompts.</p>"},{"location":"topics/RFT/paper-review/#data-quality-vs-quantity","title":"Data Quality vs. Quantity","text":"<ol> <li> <p>Impact of Data Quality:</p> <ul> <li>The authors trained a 7B LLaMA model on three datasets:</li> <li>Filtered Stack Exchange: Diverse prompts with high-quality responses.</li> <li>Unfiltered Stack Exchange: Diverse prompts without quality filtering.</li> <li>wikiHow: High-quality responses but limited to \"how-to\" prompts.</li> <li>Results show that diverse and high-quality data lead to better performance.</li> </ul> <p> Performance of 7B models trained with 2,000 examples from different sources.</p> </li> <li> <p>Impact of Data Quantity:</p> <ul> <li>Increasing the number of examples (up to 16x) from quality-filtered Stack Exchange did not significantly improve performance, indicating diminishing returns from scaling data quantity.</li> </ul> <p> Performance of 7B models trained with exponentially increasing amounts of data.</p> </li> </ol>"},{"location":"topics/RFT/paper-review/#key-takeaway","title":"Key Takeaway","text":"<p>The authors conclude that for alignment purposes, scaling input diversity and output quality has measurable benefits, while scaling data quantity alone offers limited gains.</p>"},{"location":"topics/RecSys/","title":"Recommendation Systems","text":"<p>How generative models and LLMs are transforming recommendation systems.</p>"},{"location":"topics/RecSys/#contents","title":"Contents","text":"<ul> <li>RecSys Notes - LLMs in recommendation systems</li> </ul> <p>Explore the intersection of LLMs and recommendation systems.</p>"},{"location":"topics/RecSys/notes/","title":"Recommendation System","text":"<p>Explore various ways in which generative modesl enhance recommendation systems.</p>"},{"location":"topics/RecSys/notes/#table-of-contents","title":"Table of Contents","text":""},{"location":"topics/RecSys/notes/#papersbooks","title":"Papers/Books","text":"<ul> <li>Recommendation with Generative Models. A comprehensive book</li> </ul>"},{"location":"topics/_TEMPLATE/","title":"[Topic Name]","text":"<p>[Brief 1-2 sentence description of what this topic covers and why it's important in the context of LLM/AI.]</p>"},{"location":"topics/_TEMPLATE/#contents","title":"Contents","text":"<ul> <li>Topic Overview &amp; Concepts - [Brief description of what's covered in notes]</li> <li>Paper Reviews - [Brief description of papers covered]</li> </ul> <p>[One-sentence tagline or learning objective for this topic.]</p>"},{"location":"topics/_TEMPLATE/notes/","title":"[Topic Name] - Notes","text":""},{"location":"topics/_TEMPLATE/notes/#overview","title":"Overview","text":"<p>[Provide a comprehensive overview of the topic, explaining what it is, why it matters, and its role in the broader LLM/AI landscape.]</p>"},{"location":"topics/_TEMPLATE/notes/#key-concepts","title":"Key Concepts","text":""},{"location":"topics/_TEMPLATE/notes/#concept-1","title":"[Concept 1]","text":"<p>[Explanation of the first major concept]</p> <p>Key Points: - [Point 1] - [Point 2] - [Point 3]</p>"},{"location":"topics/_TEMPLATE/notes/#concept-2","title":"[Concept 2]","text":"<p>[Explanation of the second major concept]</p> <p>Key Points: - [Point 1] - [Point 2] - [Point 3]</p>"},{"location":"topics/_TEMPLATE/notes/#core-components","title":"Core Components","text":""},{"location":"topics/_TEMPLATE/notes/#component-1","title":"[Component 1]","text":"<p>[Detailed explanation of this component]</p>"},{"location":"topics/_TEMPLATE/notes/#component-2","title":"[Component 2]","text":"<p>[Detailed explanation of this component]</p>"},{"location":"topics/_TEMPLATE/notes/#workflow-process","title":"Workflow / Process","text":"<p>[Describe the typical workflow or process involved in this topic, if applicable]</p> <ol> <li>Step 1: [Description]</li> <li>Step 2: [Description]</li> <li>Step 3: [Description]</li> </ol> <p>[Optional: Include a workflow diagram]  Figure: [Caption describing the workflow]</p>"},{"location":"topics/_TEMPLATE/notes/#best-practices","title":"Best Practices","text":"<ul> <li>[Practice 1]: [Description and rationale]</li> <li>[Practice 2]: [Description and rationale]</li> <li>[Practice 3]: [Description and rationale]</li> </ul>"},{"location":"topics/_TEMPLATE/notes/#challenges-limitations","title":"Challenges &amp; Limitations","text":""},{"location":"topics/_TEMPLATE/notes/#challenge-1-name","title":"Challenge 1: [Name]","text":"<p>[Description of the challenge and potential approaches to address it]</p>"},{"location":"topics/_TEMPLATE/notes/#challenge-2-name","title":"Challenge 2: [Name]","text":"<p>[Description of the challenge and potential approaches to address it]</p>"},{"location":"topics/_TEMPLATE/notes/#common-approaches-methods","title":"Common Approaches / Methods","text":""},{"location":"topics/_TEMPLATE/notes/#approach-1-name","title":"Approach 1: [Name]","text":"<p>Description: [What this approach does]</p> <p>Advantages: - [Advantage 1] - [Advantage 2]</p> <p>Disadvantages: - [Disadvantage 1] - [Disadvantage 2]</p>"},{"location":"topics/_TEMPLATE/notes/#approach-2-name","title":"Approach 2: [Name]","text":"<p>Description: [What this approach does]</p> <p>Advantages: - [Advantage 1] - [Advantage 2]</p> <p>Disadvantages: - [Disadvantage 1] - [Disadvantage 2]</p>"},{"location":"topics/_TEMPLATE/notes/#technical-implementation","title":"Technical Implementation","text":""},{"location":"topics/_TEMPLATE/notes/#implementation-detail-1","title":"[Implementation Detail 1]","text":"<p>[Technical details, code examples, or configuration patterns]</p>"},{"location":"topics/_TEMPLATE/notes/#implementation-detail-2","title":"[Implementation Detail 2]","text":"<p>[Technical details, code examples, or configuration patterns]</p>"},{"location":"topics/_TEMPLATE/notes/#use-cases","title":"Use Cases","text":"<ol> <li>[Use Case 1]: [Description of when and why to use this]</li> <li>[Use Case 2]: [Description of when and why to use this]</li> <li>[Use Case 3]: [Description of when and why to use this]</li> </ol>"},{"location":"topics/_TEMPLATE/notes/#tools-frameworks","title":"Tools &amp; Frameworks","text":"Tool/Framework Description Link [Tool 1] [Brief description] [URL] [Tool 2] [Brief description] [URL] [Tool 3] [Brief description] [URL]"},{"location":"topics/_TEMPLATE/notes/#further-reading","title":"Further Reading","text":"<ul> <li>Resource 1 Title - [Brief description]</li> <li>Resource 2 Title - [Brief description]</li> <li>Resource 3 Title - [Brief description]</li> </ul>"},{"location":"topics/_TEMPLATE/notes/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>[Key point 1]</li> <li>[Key point 2]</li> <li>[Key point 3]</li> <li>[Key point 4]</li> </ul>"},{"location":"topics/_TEMPLATE/paper-review/","title":"Paper review","text":""},{"location":"topics/_TEMPLATE/paper-review/#content","title":"Content","text":"<ul> <li>[Paper 1 Title]</li> <li>[Paper 2 Title]</li> <li>[Paper 3 Title]</li> </ul>"},{"location":"topics/_TEMPLATE/paper-review/#paper-1-title","title":"[Paper 1 Title]","text":"<p>Publish date: YYYY-MM-DD</p> <p>[Optional: Add arXiv or publication link] ArXiv</p>"},{"location":"topics/_TEMPLATE/paper-review/#summary","title":"Summary","text":"<p>[Provide a 2-3 sentence summary of what the paper is about and its main contribution]</p>"},{"location":"topics/_TEMPLATE/paper-review/#problem-statement-motivation","title":"Problem Statement / Motivation","text":"<p>[Explain the problem this paper is trying to solve or the gap it's addressing]</p> <p>Key Challenges: - [Challenge 1] - [Challenge 2] - [Challenge 3]</p>"},{"location":"topics/_TEMPLATE/paper-review/#main-contributions","title":"Main Contributions","text":"<p>[Describe the paper's main contributions or innovations]</p> <ol> <li>[Contribution 1]: [Description]</li> <li>[Contribution 2]: [Description]</li> <li>[Contribution 3]: [Description]</li> </ol>"},{"location":"topics/_TEMPLATE/paper-review/#methodology-approach","title":"Methodology / Approach","text":"<p>[Describe the approach taken by the authors]</p>"},{"location":"topics/_TEMPLATE/paper-review/#system-architecture-framework","title":"System Architecture / Framework","text":"<p>[Describe the system design, if applicable]</p> <p>[Optional: Include architecture diagram]  Figure: [Caption describing the architecture]</p>"},{"location":"topics/_TEMPLATE/paper-review/#key-components","title":"Key Components","text":"<ol> <li>[Component 1 Name]</li> <li>[Description of what it does]</li> <li>[How it works]</li> <li> <p>[Key features]</p> </li> <li> <p>[Component 2 Name]</p> </li> <li>[Description of what it does]</li> <li>[How it works]</li> <li> <p>[Key features]</p> </li> <li> <p>[Component 3 Name]</p> </li> <li>[Description of what it does]</li> <li>[How it works]</li> <li>[Key features]</li> </ol>"},{"location":"topics/_TEMPLATE/paper-review/#workflow-process","title":"Workflow / Process","text":"<p>[Describe the step-by-step process or workflow]</p> <ol> <li>Step 1: [Description]</li> <li>Step 2: [Description]</li> <li>Step 3: [Description]</li> </ol> <p>[Optional: Include workflow diagram]  Figure: [Caption describing the workflow]</p>"},{"location":"topics/_TEMPLATE/paper-review/#key-features-innovations","title":"Key Features / Innovations","text":"<ul> <li>[Feature 1]: [Description]</li> <li>[Feature 2]: [Description]</li> <li>[Feature 3]: [Description]</li> </ul>"},{"location":"topics/_TEMPLATE/paper-review/#experimental-results","title":"Experimental Results","text":"<p>[Summarize the key experimental findings]</p> <p>Evaluation Metrics: - [Metric 1] - [Metric 2] - [Metric 3]</p> <p>Key Findings: - [Finding 1] - [Finding 2] - [Finding 3]</p> <p>[Optional: Include results visualization]  Figure: [Caption describing the results]</p>"},{"location":"topics/_TEMPLATE/paper-review/#limitations","title":"Limitations","text":"<ul> <li>[Limitation 1]</li> <li>[Limitation 2]</li> <li>[Limitation 3]</li> </ul>"},{"location":"topics/_TEMPLATE/paper-review/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>[Takeaway 1]</li> <li>[Takeaway 2]</li> <li>[Takeaway 3]</li> </ul>"},{"location":"topics/_TEMPLATE/paper-review/#resources","title":"Resources","text":"<ul> <li>Research Paper</li> <li>Source Code (if available)</li> <li>Live Demo (if available)</li> <li>Dataset (if available)</li> </ul>"},{"location":"topics/_TEMPLATE/paper-review/#paper-2-title","title":"[Paper 2 Title]","text":"<p>Publish date: YYYY-MM-DD</p> <p>[Optional: Add arXiv or publication link] ArXiv</p>"},{"location":"topics/_TEMPLATE/paper-review/#summary_1","title":"Summary","text":"<p>[Provide a 2-3 sentence summary of what the paper is about and its main contribution]</p>"},{"location":"topics/_TEMPLATE/paper-review/#problem-statement-motivation_1","title":"Problem Statement / Motivation","text":"<p>[Explain the problem this paper is trying to solve or the gap it's addressing]</p> <p>Key Challenges: - [Challenge 1] - [Challenge 2]</p>"},{"location":"topics/_TEMPLATE/paper-review/#main-contributions_1","title":"Main Contributions","text":"<p>[Describe the paper's main contributions or innovations]</p>"},{"location":"topics/_TEMPLATE/paper-review/#methodology-approach_1","title":"Methodology / Approach","text":"<p>[Describe the approach taken by the authors]</p>"},{"location":"topics/_TEMPLATE/paper-review/#key-features-innovations_1","title":"Key Features / Innovations","text":"<ul> <li>[Feature 1]: [Description]</li> <li>[Feature 2]: [Description]</li> </ul>"},{"location":"topics/_TEMPLATE/paper-review/#experimental-results_1","title":"Experimental Results","text":"<p>[Summarize the key experimental findings]</p>"},{"location":"topics/_TEMPLATE/paper-review/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>[Takeaway 1]</li> <li>[Takeaway 2]</li> </ul>"},{"location":"topics/_TEMPLATE/paper-review/#resources_1","title":"Resources","text":"<ul> <li>Research Paper</li> <li>Source Code (if available)</li> </ul>"},{"location":"topics/_TEMPLATE/paper-review/#paper-3-title","title":"[Paper 3 Title]","text":"<p>Publish date: YYYY-MM-DD</p>"},{"location":"topics/_TEMPLATE/paper-review/#brief-overview","title":"Brief Overview","text":"<p>[For papers that don't need a full detailed review, provide a brief overview of what it covers and why it's relevant]</p>"},{"location":"topics/_TEMPLATE/paper-review/#resources_2","title":"Resources","text":"<ul> <li>Research Paper</li> <li>Source Code (if available)</li> </ul>"},{"location":"topics/compressing/","title":"Model Compression","text":"<p>Techniques for making Large Language Models smaller, faster, and more efficient without significant loss in performance.</p>"},{"location":"topics/compressing/#contents","title":"Contents","text":"<ul> <li>Compression Techniques - Quantization (PTQ, QAT), pruning, and knowledge distillation</li> </ul> <p>Discover methods to deploy LLMs more efficiently through compression and optimization.</p>"},{"location":"topics/compressing/notes/","title":"Content","text":"<ul> <li>Compressing Large Language Models</li> <li>3 Key Approaches</li> <li>Quantization<ul> <li>PTQ</li> <li>QAT</li> </ul> </li> <li>Pruning<ul> <li>Unstructured Pruning</li> <li>Structured Pruning</li> </ul> </li> <li>Knowledge Distillation</li> <li>Reference</li> <li>Papers</li> <li>Online Articles</li> </ul>"},{"location":"topics/compressing/notes/#compressing-large-language-models","title":"Compressing Large Language Models","text":"<p>Model compression aims to reduce the size of machine learning models without sacrificing performance. The key benenfits is lower inference costs, e.g. running LLMs locally on mobile devices. </p>"},{"location":"topics/compressing/notes/#3-key-approaches","title":"3 Key Approaches","text":"<ol> <li>Quantization: Reducing the number of bits used to represent the model's weights.</li> <li>Pruning: Removing unimportant weights from the model. </li> <li>Knowledge Distillation: Training a smaller model to mimic the behavior of a larger model. </li> </ol>"},{"location":"topics/compressing/notes/#quantization","title":"Quantization","text":"<p>Lowering the precision of model paramters. Two common classes of quantization techniques: - Post-training quantization (PTQ) - Quantization-aware training (QAT)</p>"},{"location":"topics/compressing/notes/#ptq","title":"PTQ","text":"<p>Replacting parameters with a lower-precision data type (e.g. FP16 to INT8). Fastes and simplest way. However, often leads to performance degration.</p>"},{"location":"topics/compressing/notes/#qat","title":"QAT","text":"<p>Trainig model (from scratch) with lower-precision data types. For example, the BitNet architecture used ternary data type (e.g. 1.58 bit) to match the performance of  Llama. </p>"},{"location":"topics/compressing/notes/#pruning","title":"Pruning","text":"<p>Remove model components that have little impact on the performance, icnluding unstructured pruning and structured pruning.</p>"},{"location":"topics/compressing/notes/#unstructured-pruning","title":"Unstructured Pruning","text":"<p>Removes unimportant weights from a neural network, i.e removing weights with smallest absolute value. But it requires specialzied hardware due to sparse matrics operations. See Ref 1. </p>"},{"location":"topics/compressing/notes/#structured-pruning","title":"Structured Pruning","text":"<p>Remove entire structures from the neural network, e.g. attention heads, neurons, and layers. It avoids the spars matrix problem. Seen ref 2.</p>"},{"location":"topics/compressing/notes/#knowledge-distillation","title":"Knowledge Distillation","text":"<p>Knowledge distillation transfters knowledge from a larger teacher model to a smaller student model, e.g. use predictions from teacher and train on student model.</p> <p>Examples. Alpaca model finetuned the LLaMA7B model using synthetic data from OpenAI's text-davinci-003, seen ref 3. </p>"},{"location":"topics/compressing/notes/#reference","title":"Reference","text":""},{"location":"topics/compressing/notes/#papers","title":"Papers","text":"<ol> <li>To prune, or not to prune: exploring the efficacy of pruning for model compression</li> <li>A Survey on Model Compression for Large Language Models</li> <li>Alpaca: A Strong, Replicable Instruction-Following Model</li> </ol>"},{"location":"topics/compressing/notes/#online-articles","title":"Online Articles","text":"<ul> <li>Compressing Large Language Models (LLMs)</li> </ul>"},{"location":"topics/evaluation/","title":"LLM Evaluation","text":"<p>Comprehensive guide to evaluating Large Language Models using various metrics, benchmarks, and methodologies.</p>"},{"location":"topics/evaluation/#contents","title":"Contents","text":"<ul> <li>Evaluation Methods &amp; Metrics - Metrics (BLEU, ROUGE, perplexity), benchmarks (GLUE, MMLU, BigBench), and evaluation strategies</li> </ul> <p>Learn how to properly evaluate LLM performance across different tasks and capabilities.</p>"},{"location":"topics/evaluation/notes/","title":"Evaluation Methods & Metrics","text":""},{"location":"topics/evaluation/notes/#introduction","title":"Introduction","text":"<p>Large Language Models (LLMs) are powerful AI systems used for tasks like text generation and translation. Evaluating them ensures they perform well and are safe for use. This section breaks down how LLMs are assessed, focusing on key methods and metrics for a general audience.</p>"},{"location":"topics/evaluation/notes/#evaluation-categories","title":"Evaluation Categories","text":"<p>LLMs are evaluated in three main areas: - Knowledge and Capability: This checks how much the model knows and can do, like answering questions or summarizing text. Metrics like accuracy and perplexity are used, with benchmarks like GLUE and SuperGLUE being common. - Alignment Evaluation: This ensures the model\u2019s outputs match human values, checking for biases and ethical issues. Human reviews and datasets like Bias in Bios help here. - Safety Evaluation: This focuses on preventing harmful outputs, using tests for robustness against bad inputs and fact-checking tools.</p>"},{"location":"topics/evaluation/notes/#unexpected-detail","title":"Unexpected Detail","text":"<p>One interesting find is the use of HellaSwag, a benchmark testing commonsense reasoning, showing how LLMs handle everyday logic, which isn\u2019t always obvious from standard tests.</p>"},{"location":"topics/evaluation/notes/#survey-note-comprehensive-review-of-llm-evaluation-methods","title":"Survey Note: Comprehensive Review of LLM Evaluation Methods","text":""},{"location":"topics/evaluation/notes/#introduction-and-background","title":"Introduction and Background","text":"<p>Large Language Models (LLMs), such as those exemplified by models like GPT-4 and LLaMA, have revolutionized natural language processing by demonstrating remarkable capabilities in tasks ranging from text generation to complex reasoning. As of March 28, 2025, the rapid deployment of LLMs in diverse applications necessitates rigorous evaluation to ensure their performance, alignment with human values, and safety. This survey note synthesizes recent research to provide a detailed overview of how LLMs are evaluated, drawing from academic papers and surveys published in the last few years.</p> <p>The evaluation of LLMs is critical due to their potential risks, including private data leaks, generation of harmful content, and the emergence of superintelligent systems without adequate safeguards. Two key surveys, \"Evaluating Large Language Models: A Comprehensive Survey\" by Guo et al. (2023) and \"A Survey on Evaluation of Large Language Models\" by Chang et al. (2024), provide foundational insights into the methodologies and benchmarks used. These works, along with other studies, form the basis of this review.</p>"},{"location":"topics/evaluation/notes/#categorization-of-evaluation-methods","title":"Categorization of Evaluation Methods","text":"<p>Research suggests that LLM evaluation can be categorized into three primary dimensions: knowledge and capability evaluation, alignment evaluation, and safety evaluation, as outlined by Guo et al. (2023). Additionally, Chang et al. (2024) propose a task-based approach, focusing on specific areas like reasoning and ethics, which complements the categorical framework.</p>"},{"location":"topics/evaluation/notes/#knowledge-and-capability-evaluation","title":"Knowledge and Capability Evaluation","text":"<p>This category assesses the model's general knowledge and linguistic capabilities, crucial for tasks such as language understanding, generation, and translation. Common metrics include: - Perplexity: Measures how well the model predicts text, with lower values indicating better performance. It is particularly used in language modeling tasks. - Accuracy: Applied in tasks like question answering, where the model's output is compared against a correct answer. - Task-Specific Metrics: For text generation, metrics like BLEU, ROUGE, and METEOR are employed, comparing generated text to reference texts for tasks like machine translation and summarization.</p> <p>Benchmarks play a pivotal role in this evaluation. Notable examples include: - GLUE (General Language Understanding Evaluation), a multi-task benchmark for natural language understanding. - SuperGLUE, an extension of GLUE with more challenging tasks. - HellaSwag, which tests commonsense reasoning by evaluating the model's ability to complete sentences logically. - BigBench, a collection of tasks assessing diverse capabilities, including reasoning and language understanding. - MMLU (Massive Multitask Language Understanding), evaluating the model's performance across multiple domains. - ARC (AI2 Reasoning Challenge), focusing on science question answering for grades 3-9. - DROP (Reading Comprehension with Discrete Reasoning), testing the model's ability to extract and reason over details in paragraphs.</p> <p>These benchmarks provide standardized tests to compare LLM performance, with leaderboards often available on platforms like Hugging Face and PapersWithCode, ensuring transparency and reproducibility.</p>"},{"location":"topics/evaluation/notes/#alignment-evaluation","title":"Alignment Evaluation","text":"<p>Alignment evaluation ensures that LLM outputs align with human values, ethics, and intentions, addressing biases and fairness. This is particularly important given the potential for LLMs to perpetuate societal biases present in training data. Methods include: - Bias Detection: Utilizing datasets like Bias in Bios, which tests for gender bias in biographical text generation, and StereoSet, which measures stereotypical biases in language models. - Human Evaluation: Involves expert reviews or surveys to assess whether outputs align with human preferences, often used for subjective aspects like coherence and relevance. - Reinforcement Learning from Human Feedback (RLHF): A training and evaluation method where human feedback is used to align model behavior, also serving as a metric for alignment quality.</p> <p>Specific benchmarks for alignment include CivilComments, which evaluates toxicity in generated text, and human evaluation frameworks that assess ethical alignment. These methods are crucial for ensuring LLMs are fair and unbiased, especially in high-stakes applications like healthcare and education.</p>"},{"location":"topics/evaluation/notes/#safety-evaluation","title":"Safety Evaluation","text":"<p>Safety evaluation focuses on preventing LLMs from generating harmful, misleading, or dangerous content. This includes: - Adversarial Testing: Testing the model's robustness against adversarial inputs, such as prompt injections designed to elicit harmful responses. Benchmarks like Adversarial NLI assess this capability. - Factuality Checks: Evaluating the model's tendency to generate false information, using datasets like the Factuality Benchmark, which measures factual precision in long-form text generation. - Toxicity and Harmfulness Metrics: Tools like the Perspective API measure the toxicity of generated text, ensuring outputs are safe for public consumption.</p> <p>Jailbreak Challenges, which test the model's resistance to producing harmful content under adversarial conditions, are also significant. These evaluations are vital for deploying LLMs in customer support or public forums, where harmful outputs could negatively impact user experiences.</p>"},{"location":"topics/evaluation/notes/#additional-insights-and-metrics","title":"Additional Insights and Metrics","text":"<p>Beyond the categorical evaluations, research highlights the use of task-specific metrics and frameworks. For instance, F1 scores are used for entity recognition tasks, while ROUGE is standard for summarization. The literature also notes the importance of custom datasets for evaluating LLM-based products, as standardized benchmarks may not capture real-world use case nuances.</p> <p>Human evaluation remains a cornerstone, especially for alignment and safety, complementing automated metrics. The integration of Continuous Integration/Continuous Evaluation/Continuous Deployment (CI/CE/CD) in LLMOps, as discussed by Huang (2024), underscores the iterative nature of evaluation, ensuring models improve over time.</p>"},{"location":"topics/evaluation/notes/#challenges-and-future-directions","title":"Challenges and Future Directions","text":"<p>The surveys identify challenges such as data contamination in benchmarks, where models are tested on data similar to their training sets, and the rapid evolution of LLM capabilities outpacing benchmark relevance. Future research is likely to focus on developing comprehensive evaluation platforms that cover all aspects\u2014capabilities, alignment, safety, and applicability\u2014as suggested by Guo et al. (2023).</p>"},{"location":"topics/evaluation/notes/#table-of-key-benchmarks-and-their-focus-areas","title":"Table of Key Benchmarks and Their Focus Areas","text":"Benchmark Name Focus Area Example Tasks GLUE General Language Understanding Sentiment analysis, QA SuperGLUE Advanced Language Understanding Challenging reasoning tasks HellaSwag Commonsense Reasoning Sentence completion Bias in Bios Bias Detection Gender bias in biographies Adversarial NLI Safety, Robustness Handling adversarial inputs Factuality Benchmark Factuality, Safety Checking factual accuracy <p>This table summarizes key benchmarks, illustrating their role in evaluating different aspects of LLMs.</p>"},{"location":"topics/evaluation/notes/#conclusion","title":"Conclusion","text":"<p>The evaluation of LLMs is a complex, multifaceted process involving automated metrics, human evaluations, and specific benchmarks tailored to knowledge, alignment, and safety. Recent surveys by Guo et al. (2023) and Chang et al. (2024) provide a comprehensive framework, highlighting the need for continuous assessment to guide responsible development. As LLMs continue to evolve, so too must evaluation methods, ensuring they meet the high standards required for real-world deployment.</p>"},{"location":"topics/evaluation/notes/#key-citations","title":"Key Citations","text":"<ul> <li>Evaluating Large Language Models: A Comprehensive Survey</li> <li>A Survey on Evaluation of Large Language Models</li> <li>GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</li> <li>SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</li> </ul>"},{"location":"topics/llm-as-judges/","title":"[Topic Name]","text":"<p>[Brief 1-2 sentence description of what this topic covers and why it's important in the context of LLM/AI.]</p>"},{"location":"topics/llm-as-judges/#contents","title":"Contents","text":"<ul> <li>Topic Overview &amp; Concepts - [Brief description of what's covered in notes]</li> <li>Paper Reviews - [Brief description of papers covered]</li> </ul> <p>[One-sentence tagline or learning objective for this topic.]</p>"},{"location":"topics/llm-as-judges/notes/","title":"LLM-as-Judges","text":""},{"location":"topics/llm-as-judges/notes/#overview","title":"Overview","text":"<p>The LLMs-as-judges paradigm is defined as a flexible and powerful evaluation framework where LLMs are utilized as evaluative tools. They are responsible for assessing the quality, relevance, and effectiveness of generated outputs based on defined evaluation criteria, leveraging their extensive knowledge and deep contextual understanding to adapt flexibly to diverse tasks in NLP and machine learning.</p> <p>The LLMs-as-Judges concept is rooted in the LLMs' capability to serve as evaluators based on natural language responses, which has attracted considerable attention from both academia and industry</p>"},{"location":"topics/llm-as-judges/notes/#motivation","title":"Motivation","text":"<p>The rise of LLMs-as-Judges is motivated by the limitations of traditional evaluation methods when faced with modern generative AI outputs. - Addressing Limitations of Traditional Metrics: Traditional metrics (like BLEU and ROUGE) are often insufficient for comprehensively evaluating the highly generative and open-ended nature of modern LLM outputs, failing to capture key aspects like text fluency, logical coherence, and creativity. The definition of LLMs as flexible judges capable of assessing quality, relevance, and effectiveness directly addresses this failure by allowing the criteria to be adjusted based on the specific task context, moving beyond fixed statistical metrics. - Need for Scalable and Interpretive Evaluation: Human annotations are seen as the \"ground truth\" but are time-consuming and resource-intensive for large-scale evaluation. The definition highlights that LLMs-as-Judges offer a scalable and reproducible alternative to human evaluation. Furthermore, their capability to leverage deep contextual understanding allows them to generate interpretive evaluations, offering comprehensive feedback and deeper insights into performance, which traditional metrics lack.</p>"},{"location":"topics/llm-as-judges/notes/#key-concepts","title":"Key Concepts","text":"<p>The following equation provides the structural overview of how the LLM judge operates: \\(\\((Y, E, F) = f(T, C, X, R)\\)\\)</p> <p>Evaluation Input: - \\(f\\) represents the evaluation function (the LLM judge itself). This function is categorized into three primary configurations:    - Single-LLM Evaluation System: A single model to perform the evaluation task. Limitation: struggle with complex tasks; may introduce bias.   - Multi-LLM Evaluation System: Multiple LLMs are employed to evaluate.   - Human-AI Collaboration Systems (Hybrid): Combines human expertise with LLM capabilities for enhanced evaluation. - \\(T\\) defines the evaluation model. It mainly three approaches:     - Pointwise: Evaluates each output independently based on specific criteria.     - Pairwise: Compares two candidates to determine which one performs better according to the specified criteria.      - Listwise: Evaluates the entire list of candidate items, evaluating and ranking them based on the specific criteria, e.g. ranking tasks, - \\(C\\) denotes the evaluation criteria, defining the specific standards that determine which aspects of the output should be assessed. Typically, teh criteria encompass the following aspects:     - Linguistic Quality: Language related of the output, e.g. fluency, grammatical accuracy, coherence, and conciseness.     - Content Relevance: Focuses on correctness and relevance of the content.      - Task-Specifc Criteria: Customized criteria tailored to the specific task, e.g. creativity for story generation. - \\(X\\) is the evaluation items. - \\(R\\) is evaluation reference, which is optional:     - Reference-based Evaluation: leverage the reference data to determine the quality and relevance of the generated outputs.     - Reference-free Evaluation: not rely on a specific reference \\(R\\), instead, it evaluates \\(X\\) based on intrinsic quality standards.</p> <p>Evaluation Output: - \\(Y\\) primary output, which can take the form of a numerical score, a ranking, a categorical label, or a qualitative assessment. - \\(E\\) provides detailed reasoning and justifications for the evalution results. - \\(F\\) consists of actionable suggestions or recommendations for improving the evaluated outputs.</p>"},{"location":"topics/llm-as-judges/notes/#functionality","title":"Functionality","text":"<p>Three main functionalities of LLMs-as-Judges: - Performance Evaluation: The most fundamental function. Two components:       - Responses Evaluation: focuses on aspects such as the quality, relevance, coherence, and fluency of the responses for a given task.      - Model Evaluation: assesses the overall capabilities of LLMs, such as coding, instruction-following, reasoning, and other specialized skills. - Model Enhancement:  plays a key role in improving model performance from the training phase through inference, offering a novel optimization pathway for artificial intelligence by fostering the refinement and personalization of intelligent systems across real-world applications     - Reward Modeling During Training, e.g. RLHF     - Acting as Verifier During Inference, primarily responsible for selecting the optimal response from multiple candidates     - Feedback for Refinement LLM judges provide actionable feedback to iteratively improve output quality - Data Construction     - Data Annotation: using LLM judges to efficiently label large, unlabeled datasets     - Data Synthesis: using LLMs-as-Judges to create entirely new data from scratch or based on seed data</p>"},{"location":"topics/llm-as-judges/notes/#referecnces-further-reading","title":"Referecnces &amp; Further Reading","text":"<ul> <li>LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods. A comprehensive survey on the emerging paradigm of Large Language Models (LLMs) used as judges for evaluation. The survey systematically reviews this concept across five main dimensions: Functionality, Methodology, Applications, Meta-evaluation, and Limitations.</li> </ul>"},{"location":"topics/llm-as-judges/paper-review/","title":"Paper review","text":""},{"location":"topics/llm-as-judges/paper-review/#content","title":"Content","text":"<ul> <li>[Paper 1 Title]</li> <li>[Paper 2 Title]</li> <li>[Paper 3 Title]</li> </ul>"},{"location":"topics/llm-as-judges/paper-review/#paper-1-title","title":"[Paper 1 Title]","text":"<p>Publish date: YYYY-MM-DD</p> <p>[Optional: Add arXiv or publication link] ArXiv</p>"},{"location":"topics/llm-as-judges/paper-review/#summary","title":"Summary","text":"<p>[Provide a 2-3 sentence summary of what the paper is about and its main contribution]</p>"},{"location":"topics/llm-as-judges/paper-review/#problem-statement-motivation","title":"Problem Statement / Motivation","text":"<p>[Explain the problem this paper is trying to solve or the gap it's addressing]</p> <p>Key Challenges: - [Challenge 1] - [Challenge 2] - [Challenge 3]</p>"},{"location":"topics/llm-as-judges/paper-review/#main-contributions","title":"Main Contributions","text":"<p>[Describe the paper's main contributions or innovations]</p> <ol> <li>[Contribution 1]: [Description]</li> <li>[Contribution 2]: [Description]</li> <li>[Contribution 3]: [Description]</li> </ol>"},{"location":"topics/llm-as-judges/paper-review/#methodology-approach","title":"Methodology / Approach","text":"<p>[Describe the approach taken by the authors]</p>"},{"location":"topics/llm-as-judges/paper-review/#system-architecture-framework","title":"System Architecture / Framework","text":"<p>[Describe the system design, if applicable]</p> <p>[Optional: Include architecture diagram]  Figure: [Caption describing the architecture]</p>"},{"location":"topics/llm-as-judges/paper-review/#key-components","title":"Key Components","text":"<ol> <li>[Component 1 Name]</li> <li>[Description of what it does]</li> <li>[How it works]</li> <li> <p>[Key features]</p> </li> <li> <p>[Component 2 Name]</p> </li> <li>[Description of what it does]</li> <li>[How it works]</li> <li> <p>[Key features]</p> </li> <li> <p>[Component 3 Name]</p> </li> <li>[Description of what it does]</li> <li>[How it works]</li> <li>[Key features]</li> </ol>"},{"location":"topics/llm-as-judges/paper-review/#workflow-process","title":"Workflow / Process","text":"<p>[Describe the step-by-step process or workflow]</p> <ol> <li>Step 1: [Description]</li> <li>Step 2: [Description]</li> <li>Step 3: [Description]</li> </ol> <p>[Optional: Include workflow diagram]  Figure: [Caption describing the workflow]</p>"},{"location":"topics/llm-as-judges/paper-review/#key-features-innovations","title":"Key Features / Innovations","text":"<ul> <li>[Feature 1]: [Description]</li> <li>[Feature 2]: [Description]</li> <li>[Feature 3]: [Description]</li> </ul>"},{"location":"topics/llm-as-judges/paper-review/#experimental-results","title":"Experimental Results","text":"<p>[Summarize the key experimental findings]</p> <p>Evaluation Metrics: - [Metric 1] - [Metric 2] - [Metric 3]</p> <p>Key Findings: - [Finding 1] - [Finding 2] - [Finding 3]</p> <p>[Optional: Include results visualization]  Figure: [Caption describing the results]</p>"},{"location":"topics/llm-as-judges/paper-review/#limitations","title":"Limitations","text":"<ul> <li>[Limitation 1]</li> <li>[Limitation 2]</li> <li>[Limitation 3]</li> </ul>"},{"location":"topics/llm-as-judges/paper-review/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>[Takeaway 1]</li> <li>[Takeaway 2]</li> <li>[Takeaway 3]</li> </ul>"},{"location":"topics/llm-as-judges/paper-review/#resources","title":"Resources","text":"<ul> <li>Research Paper</li> <li>Source Code (if available)</li> <li>Live Demo (if available)</li> <li>Dataset (if available)</li> </ul>"},{"location":"topics/llm-as-judges/paper-review/#paper-2-title","title":"[Paper 2 Title]","text":"<p>Publish date: YYYY-MM-DD</p> <p>[Optional: Add arXiv or publication link] ArXiv</p>"},{"location":"topics/llm-as-judges/paper-review/#summary_1","title":"Summary","text":"<p>[Provide a 2-3 sentence summary of what the paper is about and its main contribution]</p>"},{"location":"topics/llm-as-judges/paper-review/#problem-statement-motivation_1","title":"Problem Statement / Motivation","text":"<p>[Explain the problem this paper is trying to solve or the gap it's addressing]</p> <p>Key Challenges: - [Challenge 1] - [Challenge 2]</p>"},{"location":"topics/llm-as-judges/paper-review/#main-contributions_1","title":"Main Contributions","text":"<p>[Describe the paper's main contributions or innovations]</p>"},{"location":"topics/llm-as-judges/paper-review/#methodology-approach_1","title":"Methodology / Approach","text":"<p>[Describe the approach taken by the authors]</p>"},{"location":"topics/llm-as-judges/paper-review/#key-features-innovations_1","title":"Key Features / Innovations","text":"<ul> <li>[Feature 1]: [Description]</li> <li>[Feature 2]: [Description]</li> </ul>"},{"location":"topics/llm-as-judges/paper-review/#experimental-results_1","title":"Experimental Results","text":"<p>[Summarize the key experimental findings]</p>"},{"location":"topics/llm-as-judges/paper-review/#key-takeaways_1","title":"Key Takeaways","text":"<ul> <li>[Takeaway 1]</li> <li>[Takeaway 2]</li> </ul>"},{"location":"topics/llm-as-judges/paper-review/#resources_1","title":"Resources","text":"<ul> <li>Research Paper</li> <li>Source Code (if available)</li> </ul>"},{"location":"topics/llm-as-judges/paper-review/#paper-3-title","title":"[Paper 3 Title]","text":"<p>Publish date: YYYY-MM-DD</p>"},{"location":"topics/llm-as-judges/paper-review/#brief-overview","title":"Brief Overview","text":"<p>[For papers that don't need a full detailed review, provide a brief overview of what it covers and why it's relevant]</p>"},{"location":"topics/llm-as-judges/paper-review/#resources_2","title":"Resources","text":"<ul> <li>Research Paper</li> <li>Source Code (if available)</li> </ul>"}]}