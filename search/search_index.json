{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Learning Journey","text":"<p>Welcome to my Large Language Models learning notes! This documentation site serves as a comprehensive collection of my personal insights, research notes, and explorations in the field of LLMs and AI.</p>"},{"location":"#about-these-notes","title":"About These Notes","text":"<p>These Markdown learning notes are a collection of my personal thoughts and insights gathered from various sources. The content may not be perfectly organized, as I take notes whenever I come across something interesting and use a \"topic online clustering\" approach to group them. This method allows me to capture ideas in real-time but may result in notes that are not in a strict order.</p> <p>While these notes help me in my LLMs learning journey, they may not directly align with your needs or be as structured as you'd prefer. Feel free to explore topics that interest you!</p>"},{"location":"#topics-covered","title":"Topics Covered","text":""},{"location":"#core-llm-topics","title":"Core LLM Topics","text":"<ul> <li>AI Agents - Frameworks, architectures, and implementations of AI agents</li> <li>RAG (Retrieval-Augmented Generation) - Best practices and workflows for RAG systems</li> <li>Reinforcement Learning &amp; Fine-Tuning - RLHF, alignment, and fine-tuning techniques</li> <li>Prompt Engineering - Crafting effective prompts for LLMs</li> </ul>"},{"location":"#evaluation-optimization","title":"Evaluation &amp; Optimization","text":"<ul> <li>Evaluation - Metrics, benchmarks, and evaluation methodologies</li> <li>Model Compression - Quantization, pruning, and distillation techniques</li> </ul>"},{"location":"#specialized-topics","title":"Specialized Topics","text":"<ul> <li>Recommendation Systems - LLMs in recommendation systems</li> <li>Miscellaneous - Other interesting topics and findings</li> </ul>"},{"location":"#online-courses","title":"Online Courses","text":"<p>I've also documented notes from various online courses:</p> <ul> <li>GenAI 5-Day Course with Google</li> <li>Hugging Face AI Agent Course</li> </ul> <p>Explore course notes \u2192</p>"},{"location":"#navigation-tips","title":"Navigation Tips","text":"<ul> <li>Use the search bar at the top to find specific topics or keywords</li> <li>Browse by topic sections in the left sidebar</li> <li>Check the table of contents on the right for quick navigation within pages</li> <li>Look for tags to find related content across different topics</li> </ul>"},{"location":"#contributing-feedback","title":"Contributing &amp; Feedback","text":"<p>These are personal learning notes, but if you find errors or have suggestions, feel free to open an issue or PR on GitHub!</p> <p>Happy learning!</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#about-this-documentation","title":"About This Documentation","text":"<p>This documentation site is a collection of personal learning notes on Large Language Models, AI Agents, and related topics. The notes are organized using a \"topic online clustering\" approach, where content is added whenever interesting insights are discovered.</p>"},{"location":"about/#note-taking-philosophy","title":"Note-Taking Philosophy","text":"<p>\"I take notes whenever I come across something interesting and use a 'topic online clustering' approach to group them.\"</p> <p>This means:</p> <ul> <li>Real-time capture: Notes are added as I learn, not in a rigid structure</li> <li>Organic organization: Topics emerge and evolve naturally</li> <li>Personal focus: Content reflects my learning journey and interests</li> <li>Continuous evolution: The documentation grows and improves over time</li> </ul>"},{"location":"about/#content-sources","title":"Content Sources","text":"<p>Notes are gathered from various sources including:</p> <ul> <li>Research papers and academic publications</li> <li>Online courses and tutorials</li> <li>Blog posts and technical articles</li> <li>Hands-on experimentation</li> <li>Community discussions</li> </ul>"},{"location":"about/#disclaimer","title":"Disclaimer","text":"<p>These notes may not be: - Perfectly organized or structured - Comprehensive or complete - Suitable for all learning styles - Free from personal bias or interpretation</p> <p>They are intended as a personal knowledge base that might be useful to others exploring similar topics.</p>"},{"location":"about/#contributing","title":"Contributing","text":"<p>While these are personal notes, feedback is welcome! If you find errors or have suggestions:</p> <ol> <li>Open an issue on GitHub</li> <li>Submit a pull request</li> <li>Reach out directly</li> </ol>"},{"location":"about/#license","title":"License","text":"<p>These notes are shared for educational purposes. Please check individual sources for their respective licenses.</p> <p>Happy learning!</p>"},{"location":"tags/","title":"Tags","text":"<p>Browse content by tags to find related topics across different sections.</p>"},{"location":"tags/#available-tags","title":"Available Tags","text":"<ul> <li>ai-agent - AI Agents and autonomous systems</li> <li>rag - Retrieval-Augmented Generation</li> <li>rlhf - Reinforcement Learning from Human Feedback</li> <li>prompt - Prompt Engineering</li> <li>evaluation - LLM Evaluation methods</li> <li>compression - Model Compression techniques</li> <li>recsys - Recommendation Systems</li> </ul> <p>Tags help you discover related content across different topic areas.</p>"},{"location":"courses/","title":"Online Courses","text":"<p>Notes and insights from various online courses on AI and Large Language Models.</p>"},{"location":"courses/#available-courses","title":"Available Courses","text":""},{"location":"courses/#genai-5-day-course-with-google","title":"GenAI 5-Day Course with Google","text":"<p>Comprehensive 5-day course covering generative AI fundamentals and applications.</p> <p>Note: Course materials are located in the parent repository.</p>"},{"location":"courses/#hugging-face-ai-agent-course","title":"Hugging Face AI Agent Course","text":"<p>Deep dive into building AI agents using Hugging Face tools and frameworks.</p> <p>Note: Course materials are located in the parent repository.</p>"},{"location":"courses/#coming-soon","title":"Coming Soon","text":"<p>More course notes will be added as I continue my learning journey!</p> <p>\u2190 Back to Topics</p>"},{"location":"topics/","title":"Topics Overview","text":"<p>This section contains detailed notes and research on various topics related to Large Language Models and AI.</p>"},{"location":"topics/#major-topics","title":"Major Topics","text":""},{"location":"topics/#ai-agents","title":"AI Agents","text":"<p>Learn about AI agent frameworks like AutoGen, CrewAI, and LangGraph. Includes paper reviews on LAMBDA, TaskWeaver, and more.</p>"},{"location":"topics/#rag-retrieval-augmented-generation","title":"RAG (Retrieval-Augmented Generation)","text":"<p>Comprehensive guide to RAG workflows, best practices, chunking strategies, and Graph RAG implementations.</p>"},{"location":"topics/#reinforcement-learning-fine-tuning","title":"Reinforcement Learning &amp; Fine-Tuning","text":"<p>Deep dive into RLHF (Reinforcement Learning from Human Feedback), alignment techniques, and the LIMA paper on efficient fine-tuning.</p>"},{"location":"topics/#prompt-engineering","title":"Prompt Engineering","text":"<p>Principles and best practices for crafting effective prompts, including structure, clarity, and complex task handling.</p>"},{"location":"topics/#evaluation-optimization","title":"Evaluation &amp; Optimization","text":""},{"location":"topics/#evaluation","title":"Evaluation","text":"<p>LLM evaluation methods, metrics (BLEU, ROUGE, perplexity), and benchmarks (GLUE, MMLU, BigBench, etc.).</p>"},{"location":"topics/#model-compression","title":"Model Compression","text":"<p>Techniques for model compression including quantization (PTQ, QAT), pruning, and knowledge distillation.</p>"},{"location":"topics/#specialized-areas","title":"Specialized Areas","text":""},{"location":"topics/#recommendation-systems","title":"Recommendation Systems","text":"<p>How generative models enhance recommendation systems.</p>"},{"location":"topics/#miscellaneous","title":"Miscellaneous","text":"<p>Other interesting topics including LLM fundamentals and hallucination phenomena.</p>"},{"location":"topics/#browse-by-content-type","title":"Browse by Content Type","text":"<ul> <li>Overview &amp; Frameworks: AI-Agent, Prompt, Evaluation</li> <li>Paper Reviews: AI-Agent, RAG, RFT</li> <li>Implementation Guides: RAG, Model Compression</li> <li>Research Notes: All topics</li> </ul>"},{"location":"topics/AI-Agent/","title":"AI Agents","text":"<p>AI Agents are autonomous systems that can perceive their environment, make decisions, and take actions to achieve specific goals. In the context of Large Language Models, AI agents leverage the reasoning and generation capabilities of LLMs to perform complex tasks.</p>"},{"location":"topics/AI-Agent/#whats-in-this-section","title":"What's in This Section","text":""},{"location":"topics/AI-Agent/#overview-frameworks","title":"Overview &amp; Frameworks","text":"<ul> <li>What are AI Agents and when to use them</li> <li>Three prominent frameworks: AutoGen, CrewAI, LangGraph</li> <li>Comparison of different agent architectures</li> <li>Lists of open-source agents and tools</li> </ul>"},{"location":"topics/AI-Agent/#paper-reviews","title":"Paper Reviews","text":"<p>Detailed reviews of cutting-edge research papers: - LAMBDA: A Large Model Based Data Agent - Can Large Language Models Serve as Data Analysts? - TaskWeaver: A Code-First Agent Framework</p>"},{"location":"topics/AI-Agent/#key-concepts","title":"Key Concepts","text":"<ul> <li>Autonomous Execution: Agents can break down complex tasks and execute them step-by-step</li> <li>Tool Use: Integration with external tools and APIs</li> <li>Multi-Agent Collaboration: Multiple agents working together on complex problems</li> <li>Code-First Approaches: Agents that generate and execute code to solve problems</li> </ul>"},{"location":"topics/AI-Agent/#popular-frameworks","title":"Popular Frameworks","text":"Framework Best For Key Features AutoGen Multi-agent conversations Easy agent communication, flexible roles CrewAI Task delegation Role-based agents, built-in collaboration LangGraph Complex workflows State management, graph-based execution"},{"location":"topics/AI-Agent/#get-started","title":"Get Started","text":"<ol> <li>Read the Overview &amp; Frameworks for a high-level understanding</li> <li>Dive into Paper Reviews for research insights</li> <li>Explore the diagrams and architecture patterns included</li> </ol> <p>Topics: AI Agent, Autonomous Systems, LLM Applications</p>"},{"location":"topics/AI-Agent/notes/","title":"AI Agents","text":""},{"location":"topics/AI-Agent/notes/#contents","title":"Contents","text":""},{"location":"topics/AI-Agent/notes/#ai-agent-types","title":"AI Agent Types","text":"<p>An \"agent\" is an automated reasoning and decision engine. The key agent components can include, but not limited to: - Break down of a complex question into smaller ones - Choosing an external tool to use and coming up with parameters for calling the Tool - Planning out a set of tasks - Storing previosuly completed tasks in a memory module.  </p>"},{"location":"topics/AI-Agent/notes/#when-to-use-agents","title":"When to Use Agents","text":"<p>Agents are highly beneficial when tasks require complex decision-making, autonomy, and adaptability. They excel in environments where the workflow is dynamic and involves multiple steps or interactions that can benefit from automation.</p>"},{"location":"topics/AI-Agent/notes/#when-not-to-use-agents","title":"When Not to Use Agents","text":"<ul> <li>Tasks that are straightforward, infrequent, or require minimal automation.</li> <li>Tasks that require deep domain-specific knowledge and expertise, e.g., legal or medical advice.</li> <li>Tasks that require a high level of human empathy, creativity, or subjective judgement, e.g., psychotherapy.</li> </ul>"},{"location":"topics/AI-Agent/notes/#ai-agent-frameworks","title":"AI Agent Frameworks","text":"<p>Three prominent frameworks for building AI agents are: - AutoGen - CrewAI - LangGraph</p>"},{"location":"topics/AI-Agent/notes/#comparison-summary","title":"Comparison summary","text":""},{"location":"topics/AI-Agent/notes/#open-source-agents","title":"Open Source Agents","text":"<ul> <li>AutoGPT</li> <li>BabyAGI</li> <li>SuperAGI</li> <li>ShortGPT</li> <li>ChatDev</li> <li>AutoGen</li> <li>MetaGPT</li> <li>Camel</li> <li>LoopGPT</li> <li>JARVIS</li> <li>OpenAGI</li> </ul>"},{"location":"topics/AI-Agent/notes/#papersbooks","title":"Papers/Books","text":"<ul> <li>My own literature review on AI agents</li> <li>Agent AI: Surveying The Horizons of Multimodal Interaction. Nice overview (book) on AI agents.</li> </ul>"},{"location":"topics/AI-Agent/notes/#online-articles","title":"Online Articles","text":"<ol> <li>Navigating the New Types of LLM Agents and Architectures </li> <li>AI Agents \u2014 From Concepts to Practical Implementation in Python. This uses CrewAI framework for implementation.4</li> <li>Top 11 Open-Source Autonomous Agents &amp; Frameworks: The Future of Self-Running AI</li> <li>Mastering Agents: LangGraph Vs Autogen Vs Crew AI</li> </ol>"},{"location":"topics/AI-Agent/paper-review/","title":"Paper Reviews","text":""},{"location":"topics/AI-Agent/paper-review/#content","title":"Content","text":"<ul> <li>LAMBDA: A Large Model Based Data Agent</li> <li>Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis</li> <li>TaskWeaver: A Code-First Agent Framework</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#lambda-a-large-model-based-data-agent","title":"LAMBDA: A Large Model Based Data Agent","text":"<p>LAMBDA is an open-source, code-free multi-agent data analysis system designed to make data analysis accessible to users without programming experience. The system has several key objectives:</p> <ul> <li>Enabling code-free data analysis by automatically generating programming code</li> <li>Seamlessly integrating human domain knowledge with AI capabilities</li> <li>Supporting data science education through interactive learning</li> <li>Automatically generating comprehensive analysis reports and exportable code</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#problem-statement-motivation","title":"Problem Statement &amp; Motivation","text":"<p>Existing research has not adequately addressed the high degree of flexibility required in real-world data analysis scenarios, particularly when it comes to incorporating custom algorithms or statistical models based on user preferences. Additionally, traditional function-calling approaches face significant challenges in statistical and data science applications:</p> <ul> <li>The sheer volume of APIs/functions, their complex interrelationships, and extensive documentation often exceed LLM capacity</li> <li>As the number of available APIs increases, the model's ability to accurately select appropriate functions deteriorates</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#system-architecture","title":"System Architecture","text":"<p>LAMBDA employs a dual-agent architecture consisting of:</p> <ol> <li>Data Scientist (Programmer) Agent</li> <li>Primary role: Code generation and user interaction</li> <li>Guided by system prompts defining its role, context, and I/O formats</li> <li> <p>Workflow:</p> <ul> <li>Writes code based on user/inspector instructions</li> <li>Executes code through kernel</li> <li>Generates comprehensive responses including results summary and next-step suggestions</li> </ul> </li> <li> <p>Inspector Agent</p> </li> <li>Primary role: Error detection and correction</li> <li>Analyzes execution errors in programmer's code</li> <li>Provides actionable revision suggestions for code improvement</li> <li>Works iteratively until code executes successfully or reaches maximum attempts</li> </ol> <p> Figure: Overview of LAMBDA showing the interaction between programmer agent for code generation and inspector agent for error evaluation. The system supports human intervention when needed.</p>"},{"location":"topics/AI-Agent/paper-review/#key-features","title":"Key Features","text":""},{"location":"topics/AI-Agent/paper-review/#knowledge-integration-mechanism","title":"Knowledge Integration Mechanism","text":"<ul> <li>Implements a Key-Value (KV) knowledge structure</li> <li>Key: Resource descriptions (e.g., function docstrings)</li> <li>Value: Corresponding code implementations</li> <li>Enables domain-specific task execution</li> <li>Provides flexibility for complex analysis challenges</li> <li>Facilitates easy incorporation of user resources into the agent system</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#technical-implementation","title":"Technical Implementation","text":"<ul> <li>Uses IPython as the system kernel for sequential data processing</li> <li>Supports comprehensive report generation including:</li> <li>Data processing steps</li> <li>Data visualizations</li> <li>Model descriptions</li> <li>Evaluation results</li> <li>Enables code export functionality</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#user-interface-interaction","title":"User Interface &amp; Interaction","text":"<ul> <li>Chat-based interface for natural interaction</li> <li>Step-by-step guided prompting</li> <li>Human-in-the-loop design allowing direct code modification</li> <li>Extensive prompt templates for various tasks:</li> <li>Data analysis</li> <li>Dataset handling</li> <li>Error resolution</li> <li>Knowledge integration</li> <li>Code debugging</li> </ul> <p> Figure: Detailed view of the collaborative process between programmer and inspector agents.</p>"},{"location":"topics/AI-Agent/paper-review/#resources","title":"Resources","text":"<ul> <li>Research Paper</li> <li>Live Demo</li> <li>Source Code</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#can-large-language-models-serve-as-data-analysts-a-multi-agent-assisted-approach-for-qualitative-data-analysis","title":"Can Large Language Models Serve as Data Analysts? A Multi-Agent Assisted Approach for Qualitative Data Analysis","text":"<p>Qualitative research is a type of research that focuses on collecting and analyzing non-numerical data (e.g., text, video, or audio) to understand concepts, opinions, or experiences.</p> <p>This paper explores using LLMs to automate and expedite qualitative data analysis processes. The model adeptly interprets massive volumes of textual data and interview transcripts to autonomously perform the chosen approach for qualitative data analysis.</p> <p>In the framework, each agent in the system is a specialized instance of an LLM, trained to handle different aspects of qualitative data analysis.</p> <p> Figure: A workflow overview of the proposed system for automation of qualitative data analysis.</p>"},{"location":"topics/AI-Agent/paper-review/#resource","title":"Resource","text":"<ul> <li>Research Paper</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#taskweaver-a-code-first-agent-framework","title":"TaskWeaver: A Code-First Agent Framework","text":"<p>TaskWeaver is a framework that converts user requests into executable code and treats user-defined plugins as callable functions. It provides support for rich data structures, flexible plugin usage, and dynamic plugin selection, leveraging LLM coding capabilities for complex logic.</p> <p>Key limitations of existing frameworks that TaskWeaver addresses: - Lack of native support for handling rich data structures - Limited configuration options for incorporating domain knowledge  - Inability to meet diverse user requirements</p> <p>The core architecture consists of three main components:</p> <ol> <li>Planner: The system entry point that:</li> <li>Breaks down user requests into subtasks and manages execution with self-reflection</li> <li> <p>Transforms execution results into human-readable responses</p> </li> <li> <p>Code Interpreter (CI): Contains two sub-components:</p> </li> <li>Code Generator (CG): Generates code for subtasks from the Planner using available plugins</li> <li> <p>Code Executor (CE): Executes generated code and maintains execution state</p> </li> <li> <p>Memory Module: Centralizes chat history between user and internal roles</p> </li> </ol> <p> Figure: The overview of TaskWeaver.</p> <p>A key use case is anomaly detection in databases, which uses a two-layer planning process: 1. Planner generates high-level steps to fulfill the request 2. CI devises detailed execution plans with chain-of-thought reasoning and code generation</p> <p>The workflow begins with the Planner receiving a user query along with CI descriptions (plugin/function documentation). The CG receives comprehensive plugin definitions including function names, descriptions, arguments and return values. Execution results flow back to the Planner to determine next steps.</p> <p> Figure: the workflow of the anomaly detection use case.</p>"},{"location":"topics/AI-Agent/paper-review/#component-details","title":"Component Details","text":"<p>Planner As the system controller, the Planner: - Receives and decomposes user queries into sub-tasks - Generates initial plans based on LLM knowledge and domain examples - Refines plans considering sub-task dependencies - Assigns tasks to CI for code generation - Updates plans based on execution results following ReAct pattern - Manages the process until completion</p> <p>Code Generator (CG) The CG synthesizes Python code by: - Combining plugin system and code interpreter capabilities - Leveraging user-customized plugins and examples - Using plugin schemas to understand capabilities - Ensuring plugin implementations match schemas</p> <p>Code Executor (CE) The CE handles code execution by: - Running code generated by CG - Managing dependent modules and plugins - Preserving context and logs - Returning results to Planner</p>"},{"location":"topics/AI-Agent/paper-review/#resource_1","title":"Resource","text":"<ul> <li>Research Paper</li> <li>Source Code</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#tradingagents-multi-agents-llm-financial-trading-framework","title":"TradingAgents: Multi-Agents LLM Financial Trading Framework","text":""},{"location":"topics/AI-Agent/paper-review/#resource_2","title":"Resource","text":"<ul> <li>Resource Paper</li> </ul>"},{"location":"topics/AI-Agent/paper-review/#sciagents-automating-scientific-discovery-through-multi-agent-intelligent-graph-reasoning","title":"SciAgents: Automating Scientific Discovery through Multi-Agent Intelligent Graph Reasoning","text":"<p>publish date: 2024-09-09 Objective: Developing AI systems that can not only explore and exploit existing knowledge to make significant scientific discoveries but also automate and replicate the broader research process, including acquiring relevant knowledge and data. SciAgents, an approach that leverages three core concepts: (1) the use of large-scale ontological knowledge graphs to organize and interconnect diverse scientific concepts, (2) a suite of large language models (LLMs) and data retrieval tools, and (3) multi-agent systems with in-situ learning capabilities.</p>"},{"location":"topics/AI-Agent/paper-review/#resource_3","title":"Resource","text":"<ul> <li>Resource Paper</li> <li>Source Code</li> </ul>"},{"location":"topics/Others/","title":"Miscellaneous Topics","text":"<p>Other interesting topics and findings related to Large Language Models.</p>"},{"location":"topics/Others/#contents","title":"Contents","text":"<ul> <li>Miscellaneous Notes - LLM fundamentals, hallucination phenomena, and various insights</li> </ul> <p>A collection of interesting topics that don't fit neatly into other categories.</p>"},{"location":"topics/Others/notes/","title":"Intro to LLM Fundamentals: What are LLMs and How do they come to be?","text":"<ul> <li>Serrano, S., Brumbaugh, Z., &amp; Smith, N. A. (2023). Language Models: A Guide for the Perplexed. http://arxiv.org/abs/2311.17301</li> </ul> <p>This paper provides a comprehensive, non-technical introduction to language models (LMs), including large language models (LLMs). It offers a high-level overview of how language models, including LLMs, are trained, evaluated, and utilized. I found the sections discussing data particularly insightful, and the discussion on practical applications of LLMs thought-provoking.</p> <p>Data is crucial both for training and testing. Testing serves as an indicator of the system's output quality and should not be used for any other purpose prior to the final test. NLP tasks depend heavily on both the quality and quantity of training data. Understanding a model involves knowing its training data. However, nowadays, companies developing LLMs are reluctant to share their training datasets. This leads to concerns about hidden training data; for instance, if a model answers a complex question accurately and clearly, we should be impressed only if we are certain that the question and answer were not in the training data. Without access to the training data, we cannot verify whether the model is genuinely being tested fairly or simply recalling answers it has already seen.</p> <p>The paper also discusses \"hallucination,\" where the content generated by LLMs is inaccurate or nonfactual. From my own experience using ChatGPT, I have noticed this issue as well. The paper suggests that while LLMs rely on training data, they do not directly access this data; instead, they seem to encode patterns from the data but do not \"remember\" the data precisely at all times. Thus, for topics with ample supporting data and straightforward tasks, the likelihood of hallucination is lower. Conversely, with more complex tasks or less-discussed subjects, hallucination becomes less surprising. Moreover, since the training data may contain incorrect or biased information, the model might encode these inaccuracies.</p> <p>The discussion on how to effectively use LLMs is also worth considering. The paper mentions that using LLMs to summarize newspapers may not be worthwhile since the first paragraph of a news article typically summarizes the entire piece. This perspective is particularly interesting in today's context, where there is significant buzz around employing LLMs. However, the challenge remains on how to fully leverage these technologies to solve everyday problems, which continues to be an area needing exploration.</p>"},{"location":"topics/Prompt/","title":"Prompt Engineering","text":"<p>Principles and best practices for crafting effective prompts to get the most out of Large Language Models.</p>"},{"location":"topics/Prompt/#contents","title":"Contents","text":"<ul> <li>Prompt Engineering Guide - Structure, clarity, specificity, and techniques for complex tasks</li> </ul> <p>Master the art of prompt engineering to improve LLM outputs and handle complex tasks effectively.</p>"},{"location":"topics/Prompt/notes/","title":"Prompt","text":""},{"location":"topics/Prompt/notes/#table-of-contents","title":"Table of Contents","text":""},{"location":"topics/Prompt/notes/#what-is-prompt-engineering","title":"What is Prompt Engineering?","text":"<p>It is a practice with a set of guidelines to craft precise, concise, creative wording of text to instruct an LLM to carry out a task. </p>"},{"location":"topics/Prompt/notes/#prompt-principles-and-guides","title":"Prompt Principles and Guides","text":"<p>Refer to Paper [1] 1. Prompt Structure and Clarity: Integrate the intended audience in teh prompt. e.g., integrate the intended audience in the prompt such as the audience is an expert in the field. 2. Specificity and Information: Implement example-driven prompting. E.g., Add to your prompt the following phrase \u201cEnsure that your answer is unbiased and does not rely on stereotypes.\u201d; 3. User Interaction and Engagement: Allow the model to ask precise details and requirements until it has enough information to provide the needed response. E.g. \"From now on, I would like you to ask me questions to...\" 4. Content and Language Stype: Instruct the tone and style of response. 5. Complex Tasks and Coding Prompts: Break down complex tasks into a sequence of simple steps as prompts.</p> <p> </p>"},{"location":"topics/Prompt/notes/#reference","title":"Reference","text":""},{"location":"topics/Prompt/notes/#papers","title":"Papers","text":"<ol> <li>Bsharat, Sondos Mahmoud, Aidar Myrzakhan, and Zhiqiang Shen. \u201cPrincipled Instructions Are All You Need for Questioning LLaMA-\u00bd, GPT-3.5/4.\u201d arXiv, January 18, 2024. http://arxiv.org/abs/2312.16171.</li> </ol>"},{"location":"topics/Prompt/notes/#online-articles","title":"Online Articles","text":"<ol> <li>Best Prompt Techniques for Best LLM Responses</li> </ol>"},{"location":"topics/RAG/","title":"RAG (Retrieval-Augmented Generation)","text":"<p>Retrieval-Augmented Generation (RAG) enhances Large Language Models by combining them with external knowledge retrieval systems.</p>"},{"location":"topics/RAG/#contents","title":"Contents","text":"<ul> <li>RAG Guide &amp; Best Practices - Complete RAG workflow, chunking strategies, vector databases, and Graph RAG</li> <li>Paper Reviews - Golden-Retriever paper and advanced retrieval techniques</li> </ul> <p>Learn about RAG workflows, best practices, and cutting-edge research in retrieval-augmented generation.</p>"},{"location":"topics/RAG/notes/","title":"RAG","text":""},{"location":"topics/RAG/notes/#contents","title":"Contents","text":"<ul> <li>Introduction</li> <li>Papers</li> <li>Online Articles </li> <li>Implementation</li> </ul>"},{"location":"topics/RAG/notes/#introduction","title":"Introduction","text":"<p>RAG stands for Retrieval-Augumented Generation. RAG system works two steps: 1. Retrieve: It retrieves relevant information from a large corpus of text. 2. Generate: It generates a response based on the retrieved information. Common Use-case: question answering, document summarization, content generation.  Why do we need RAG? 1. avoid hallucination 2. timeliness 3. LLMs cannot access private data, feed more internal/user private data to get customized results. 4. Answer constraint. </p> <p>A naive RAG mainly consists of the following steps: 1. Indexing: Cleaning and extracting the raw text into standardized plain text -&gt; Chunking -&gt; transformed into vector via embedding -&gt; create (key, value) pairs, which is (index, vector) pairs. 2. Retrieval: users query processed by an encoding model -&gt; query embedding -&gt; similarity search on a vector database -&gt; top-k results are retrieved. 3. Generation: user query and retrieved documents are fed into a prompt template -&gt; generate the response.</p>"},{"location":"topics/RAG/notes/#best-practices-of-rag","title":"Best practices of RAG","text":"<p>In Paper [2]:</p> <p>A typical RAG workflow usually contains multiple intervening processing steps: query classification (determining whether retrieval is necessary for a given input query), retrieval (efficiently obtaining relevant documents for the query), reranking (refining the order of retrieved documents based on their relevance to the query), repacking (organizing the retrieved documents into a structured one for better generation), summarization (extracting key information for response generation from the repacked document and eliminating redundancies) modules. Implementing RAG also requires decisions on the ways to properly split documents into chunks, the types of embeddings to use for semantically representing these chunks, the choice of vector databases to efficiently store feature representations, and the methods for effectively fine-tuning LLMs</p>"},{"location":"topics/RAG/notes/#rag-workflow","title":"RAG Workflow","text":"<ol> <li>Query Classification. For tasks entirely based on user-given information, we denote as \u201csufficient\u201d, which need not retrieval; otherwise, we denote as \u201cinsufficient\u201d, and retrieval may be necessary.</li> <li>Chunking. Three types of chunking: token sentence, and semantic levels. </li> <li>Token-level chunking: split the text into tokens, usually with a fixed length.</li> <li>Sentence-level chunking: split the text into sentences.</li> <li>Semantic-level chunking: take the embeddings of every sentence in the document, comparing the similarity of all sentences with each other, and then grouping sentences with the most similar embeddings together.</li> <li>Vector databases. Store embedding vectors with their metadata, enabling efficient retrival of documents relevant to queries through various indexing and approximate nearest neighbor search. </li> <li>Retrieval Method. The recommended steps:  </li> <li>query rewriting.</li> <li>query decomposition.</li> <li>pseudo-document generation. This approach generates a hypothetical document based on the user query and uses the embedding of hypothetical answers to retrieve similar documents. One notable implement is HyDE.</li> <li>Hybrid search. Combining sparse retrieval (BM25) and dense retrieval (original embedding). The weights between the two retrieval methods can be appropriately adjusted. </li> <li>Reranking. Enhance the relevance of the retrieved documents.  </li> <li>Document repacking. The performance of subsequent processes, such as LLM response generation, may be affected by the order documents are provided.</li> <li>Summarization. Extractive or abstractive. </li> </ol>"},{"location":"topics/RAG/notes/#graph-rag","title":"Graph RAG","text":"<p>Why RAG is not enough? </p> <p>Traditional RAG systems, while powerful, have several limitations:</p> <ol> <li> <p>Loss of Structural Information: Standard RAG treats documents as independent chunks, losing important relationships and connections between pieces of information. Real-world knowledge often has inherent graph-like structures (e.g., relationships between entities, hierarchical information, or causal chains).</p> </li> <li> <p>Limited Context Understanding: When retrieving information, traditional RAG looks at chunks in isolation. This can miss broader context that might be spread across multiple related documents or sections.</p> </li> <li> <p>Inability to Handle Complex Queries: Questions that require connecting multiple pieces of information or understanding relationships between entities are difficult for traditional RAG to handle effectively.</p> </li> <li> <p>Static Document View: Traditional RAG typically treats documents as static pieces of text, without capturing how information evolves or relates to other pieces of knowledge over time.</p> </li> </ol> <p>Graph RAG addresses these limitations by: - Representing knowledge as a graph structure where nodes contain information and edges represent relationships - Preserving structural information during retrieval - Enabling multi-hop reasoning across connected pieces of information - Supporting more complex query patterns that require traversing relationships</p> <p>Useful videos - GraphRAG: The Marriage of Knowledge Graphs and RAG: Emil Eifrem</p>"},{"location":"topics/RAG/notes/#papers","title":"Papers","text":"<p>My literature review: RAG 1.  Lewis, Patrick, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, et al. \u201cRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\u201d arXiv, April 12, 2021. http://arxiv.org/abs/2005.11401. The first paper talks about RAG - models which combine pre-trained parametric and non-parametric memory for language generation. RAG models, the parametric memory is a pre-trained seq2seq transformer and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. other resource: Youtube video - https://www.youtube.com/watch?v=JGpmQvlYRdU (by the Author of the paper) - https://www.youtube.com/watch?v=dzChvuZI6D4 (explanation of the paper) 2. Wang, Xiaohua, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, et al. \u201cSearching for Best Practices in Retrieval-Augmented Generation.\u201d arXiv, July 1, 2024. http://arxiv.org/abs/2407.01219. it gives an overview of current practice of RAG. A good tech blog to explain the paper.</p> <ol> <li> <p>Shi, Yunxiao, Xing Zi, Zijing Shi, Haimin Zhang, Qiang Wu, and Min Xu. \u201cEnhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems.\u201d arXiv, July 15, 2024. http://arxiv.org/abs/2407.10670.</p> <p>This paper introduces 4 modules to solving several key challenges with RAG.  </p> </li> <li> <p>Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496, 2022.</p> <p>this is the paper talks about HYDE method.  </p> </li> </ol> <p>Graph RAG - A Benchmark to Understand the Role of Knowledge Graphs on Large Language Model's Accuracy for Question Answering on Enterprise SQL Databases</p>"},{"location":"topics/RAG/notes/#online-articles","title":"Online Articles","text":""},{"location":"topics/RAG/notes/#introduction_1","title":"Introduction","text":"<ul> <li>Introduction to RAG \u2014 GenAI Systems for Knowledge</li> <li>A Brief Introduction to Retrieval Augmented Generation(RAG)</li> <li>The Best Practices of RAG</li> </ul> <p>Chucking - Semantic Chunking for RAG</p> <p>Retrieval - HYDE: Revolutionising Search with Hypothetical Document Embeddings</p>"},{"location":"topics/RAG/notes/#implementation","title":"Implementation","text":"<ul> <li>How do domain-specific chatbots work? An Overview of Retrieval Augmented Generation (RAG)</li> <li>A beginner\u2019s guide to building a Retrieval Augmented Generation (RAG) application from scratch</li> <li>Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation</li> </ul> <p>Chucking - Semantic Chunking for RAG</p> <p>Retrieval - Power of Hypothetical Document Embeddings: An In-Depth Exploration of HyDE - Exploring Query Rewriting. This blog uses <code>LlamaIndex</code> and <code>LangChain</code> to demostrate several techniques for query rewriting: Hypothetical Document Embeddings (HyDE), Rewrite-Retrieve-Read, Step-Back Prompting, and etc..</p> <p>Graph RAG - Enhancing RAG with Graph</p>"},{"location":"topics/RAG/paper-review/","title":"Paper Reviews","text":""},{"location":"topics/RAG/paper-review/#content","title":"Content","text":"<ul> <li>Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base</li> </ul>"},{"location":"topics/RAG/paper-review/#golden-retriever-high-fidelity-agentic-retrieval-augmented-generation-for-industrial-knowledge-base","title":"Golden-Retriever: High-Fidelity Agentic Retrieval Augmented Generation for Industrial Knowledge Base","text":"<p>Publish date: 2024-07-20</p> <p>Challenges in Traditional RAG: - Ambiguous jargon in user queries often leads to retrieval of irrelevant documents, a common issue in industrial knowledge bases. - User queries typically lack context, making it difficult to retrieve the most relevant documents.</p> <p>Golden-Retriever's Solution: - Enhances the traditional RAG framework by incorporating a reflection-based question augmentation step before document retrieval.</p> <p></p> <p>Figure: The framework of Golden-Retriever. It reflects upon the question, identifies its context, and augments the question by querying a jargon dictionary before document retrieval. The augmented question allows RAG to faithfully retrieve the most relevant documents despite ambiguous jargon or lack of explicit context.</p> <p>Components of Golden-Retriever: - Offline Component:   - Enhances the document database to improve the relevance of retrieved documents before deploying the knowledge base chatbot.   - Collects various documents (e.g., slides, images with text, tables) from the company.   - Extracts text from images and tables, breaks it into smaller chunks (~4,000 tokens), summarizes each chunk using LLM, and saves them in the document database.</p> <ul> <li>Online Component:</li> <li>Identify Jargon and Abbreviations: Prompts the system to identify all jargon and abbreviations in user queries for further processing.</li> <li>Context Identification: Uses a prompt template with pre-specified context names and descriptions to identify the context of the question. Applies few-shot examples with Chain-of-Thought reasoning.</li> <li>Query Jargons: Utilizes a code-based approach to synthesize SQL queries.</li> <li>Augment Question: Enhances the original user question with additional information, structuring it for effective document retrieval.</li> <li>Handle Query Misses: Implements a fallback mechanism when jargon terms are not found, generating a response indicating missing information. Instructs users to check jargon spelling or contact the knowledge base manager to add new terms.</li> </ul> <p> Figure: Left: the workflow diagram of the online inference part of Golden-Retriever. Right: example interactions between the system and the LLM at the intermediate steps of the workflow. The system prompts LLM to generate intermediate responses, which are saved, accessed, and used for future steps in the workflow.</p>"},{"location":"topics/RAG/paper-review/#resource","title":"Resource","text":"<ul> <li>Research Paper</li> <li>Source Code</li> </ul>"},{"location":"topics/RFT/","title":"Reinforcement Learning &amp; Fine-Tuning","text":"<p>Deep dive into RLHF (Reinforcement Learning from Human Feedback) and efficient fine-tuning techniques for LLMs.</p>"},{"location":"topics/RFT/#contents","title":"Contents","text":"<ul> <li>RLHF Overview - Process, benefits, and challenges of reinforcement learning for language model fine-tuning</li> <li>Paper Reviews - LIMA paper on alignment with minimal data</li> </ul> <p>Explore alignment techniques, RLHF processes, and the surprising effectiveness of small, high-quality datasets.</p>"},{"location":"topics/RFT/notes/","title":"Reinforcement Learning on Language Models","text":""},{"location":"topics/RFT/notes/#overview","title":"Overview","text":"<p>Reinforcement learning (RL) is a method where models learn from feedback, often in the form of rewards, to improve performance. When applied to fine-tuning small language models, RL from human feedback (RLHF) helps align these models with human preferences, making them better at specific tasks like text generation or question answering. While much of the research focuses on large models, small language models\u2014those with fewer parameters, often under 1 billion\u2014can also benefit, especially for niche applications where efficiency is key.</p>"},{"location":"topics/RFT/notes/#process-and-benefits","title":"Process and Benefits","text":"<p>RLHF involves several steps: starting with a pre-trained model, fine-tuning it with supervised learning, training a reward model based on human feedback, and then using RL to optimize the model for higher rewards. For small language models, this can enhance performance on tasks like code generation or customer support chatbots, offering lower latency and reduced memory use compared to large models.</p>"},{"location":"topics/RFT/notes/#challenges-and-considerations","title":"Challenges and Considerations","text":"<p>Small language models may struggle with capturing complex human preferences due to limited capacity, and computational resources can be a bottleneck. However, techniques like parameter-efficient fine-tuning (PEFT) can help mitigate these issues, making RLHF feasible for smaller models.</p>"},{"location":"topics/RFT/notes/#unexpected-detail","title":"Unexpected Detail","text":"<p>An interesting finding is that small models, when fine-tuned with RLHF, can sometimes outperform larger models in specific, narrow tasks, such as code review accuracy, due to their efficiency and lower latency, as seen in recent studies (NVIDIA Technical Blog).</p>"},{"location":"topics/RFT/notes/#survey-note-reinforcement-learning-for-fine-tuning-small-language-models","title":"Survey Note: Reinforcement Learning for Fine-Tuning Small Language Models","text":""},{"location":"topics/RFT/notes/#introduction","title":"Introduction","text":"<p>Fine-tuning language models is a critical process in adapting pre-trained models to specific tasks or domains, leveraging their general language understanding for specialized applications. Reinforcement learning, particularly reinforcement learning from human feedback, has emerged as a powerful technique for aligning these models with human preferences, especially in tasks where direct supervision is challenging. While much of the recent research has focused on LLMs, there is growing interest in applying these techniques to small language models (SLMs), defined as models with fewer parameters (often under 1 billion), due to their efficiency and suitability for resource-constrained environments. This survey note explores the current state of RLHF for fine-tuning SLMs, reviewing key methodologies, challenges, and research findings, with a focus on their applicability and limitations.</p>"},{"location":"topics/RFT/notes/#background-on-fine-tuning-and-rlhf","title":"Background on Fine-Tuning and RLHF","text":"<p>Fine-tuning involves further training a pre-trained language model on a smaller, task-specific dataset to adapt it for particular applications, such as text classification, question answering, or content generation. Traditionally, this has been done using supervised learning, where the model is trained on labeled data. However, for tasks requiring nuanced human judgment, RL offers an alternative by allowing the model to learn from feedback in the form of rewards or punishments.</p> <p>RLHF specifically integrates RL with human feedback, aiming to align model outputs with human preferences. The process typically includes: - Pre-trained Language Model: Starting with a model pre-trained on a large corpus of text, such as BERT or smaller transformer-based models. - Supervised Fine-Tuning (SFT): Fine-tuning the model on a dataset of human-generated text relevant to the task, helping it understand task-specific requirements. - Reward Model Training: Training a separate model to predict the quality of the language model's outputs based on human preferences, often using pairwise comparisons where humans rank outputs for given inputs. - RL Fine-Tuning: Using the reward model to guide further fine-tuning with RL algorithms, such as Proximal Policy Optimization (PPO), to maximize the expected reward, aligning the model more closely with human values.</p> <p>This methodology has been pivotal in developing models like ChatGPT, which rely on RLHF for producing helpful and safe responses (AssemblyAI Blog).</p> <p></p> <p>Figure 1. A preference (or reward) model could be used to further train the baseline model to prioritize responses with higher preference scores.</p>"},{"location":"topics/RFT/notes/#key-research-on-rlhf-for-language-models","title":"Key Research on RLHF for Language Models","text":"<p>The development of RLHF for language models has been marked by several seminal works, primarily focused on LLMs. Below is a table summarizing key papers and their contributions:</p> Year Authors Title Contribution 2017 Christiano et al. Deep Reinforcement Learning from Human Preferences Introduced RLHF, demonstrating its use in general RL settings, laying groundwork. 2019 Ziegler et al. Fine-Tuning Language Models from Human Preferences Explored early applications of RLHF to language models, focusing on preference alignment. 2020 Stiennon et al. Learning to Summarize with Human Feedback Applied RLHF to text summarization, showing improved quality based on human preferences. 2022 Ouyang et al. Training Language Models to Follow Instructions with Human Feedback Developed InstructGPT, using RLHF for general-purpose instruction-following models. 2022 Bai et al. Training a Helpful and Harmless Assistant with RLHF Refined RLHF for creating safe and helpful assistants, addressing alignment issues. 2022 Menick et al. Scaling Laws for Reward Model Overoptimization Investigated challenges of reward model overoptimization in RLHF, impacting performance. 2020 Gabriel et al. The Limitations of Reinforcement Learning from Human Feedback Discussed ethical concerns and limitations, such as bias amplification and preference definition. <p>These papers highlight the evolution of RLHF, from its theoretical foundations to practical implementations, primarily with LLMs. For instance, Ouyang et al. (2022) demonstrated how RLHF enabled InstructGPT to follow complex instructions, a significant step toward general-purpose assistants (arXiv).</p>"},{"location":"topics/RFT/notes/#application-to-small-language-models","title":"Application to Small Language Models","text":"<p>While most research has focused on LLMs, the principles of RLHF can be applied to SLMs, though with specific considerations. SLMs, often with parameters in the range of tens to hundreds of millions, are designed for efficiency, lower latency, and deployment in resource-constrained environments. Recent studies suggest that SLMs can be competitive for real-world tasks, particularly when fine-tuned for specific, narrow applications (Medium).</p> <ul> <li>Efficiency and Resource Constraints: SLMs require less computational power, making RLHF more accessible for researchers and practitioners with limited resources. Techniques like parameter-efficient fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), can further reduce the computational burden, enabling RLHF on smaller models (SuperAnnotate Blog).</li> <li>Task Specificity: SLMs are often better suited for niche tasks, such as code review or customer support chatbots, where large models might be overkill. For example, a study by NVIDIA showed that a fine-tuned Llama 3 8B model with LoRA improved code review accuracy by 18%, outperforming larger models in specific tasks (NVIDIA Technical Blog).</li> <li>Challenges: SLMs may have limited capacity to capture complex human preferences, potentially leading to poorer generalization compared to LLMs. Additionally, the data requirements for training reward models in RLHF might be more challenging for SLMs, given their smaller parameter space and potential for overfitting.</li> </ul> <p>Despite these challenges, recent blog posts and practical guides suggest that RLHF can be adapted for SLMs, with careful planning and monitoring (premai.io Blog). For instance, the process involves starting with small datasets (5-10%) to test performance and iteratively refining hyperparameters, which is particularly feasible for SLMs due to their lower resource needs (Encora Insights).</p>"},{"location":"topics/RFT/notes/#challenges-and-limitations","title":"Challenges and Limitations","text":"<p>RLHF, whether applied to LLMs or SLMs, faces several challenges: - Reward Model Robustness: As highlighted by Menick et al. (2022), reward models can be exploited by the policy model, leading to overoptimization and degraded performance (arXiv). - Data Quality and Quantity: Human feedback can be costly and inconsistent, with annotators often disagreeing, adding variance to training data (Hugging Face Blog). For SLMs, the smaller parameter space might exacerbate issues with data scarcity. - Ethical Concerns: Gabriel et al. (2020) discuss potential biases in RLHF, such as amplifying existing biases in training data, which could be more pronounced in SLMs due to their limited capacity to mitigate such biases (arXiv).</p> <p>For SLMs specifically, additional challenges include: - Computational Efficiency: While SLMs are less resource-intensive, RLHF still requires significant computational power for reward model training and iterative optimization, which can be a bottleneck for smaller setups. - Generalization: SLMs may struggle to generalize across diverse tasks, particularly when fine-tuned with RLHF, due to their limited capacity compared to LLMs.</p>"},{"location":"topics/RFT/notes/#future-directions","title":"Future Directions","text":"<p>Given the current state of research, future work could focus on: - Developing tailored RLHF methodologies for SLMs, addressing their unique constraints and capabilities. - Exploring hybrid approaches, combining RLHF with other fine-tuning techniques like PEFT, to enhance performance on SLMs. - Investigating the trade-offs between model size, performance, and computational cost in RLHF, potentially leading to guidelines for selecting the appropriate model size for specific tasks.</p>"},{"location":"topics/RFT/notes/#conclusion","title":"Conclusion","text":"<p>RLHF has proven to be a transformative technique for aligning language models with human preferences, with significant advancements driven by research on LLMs. While direct applications to SLMs are less documented, the principles of RLHF can be adapted, offering potential for efficient, task-specific fine-tuning. Challenges such as computational efficiency, data quality, and generalization need further exploration, particularly for SLMs. As the field progresses, continued research will be essential to fully realize the potential of RLHF in fine-tuning small language models, ensuring they meet the needs of diverse, resource-constrained applications.</p>"},{"location":"topics/RFT/notes/#reference","title":"Reference","text":""},{"location":"topics/RFT/notes/#github-projects","title":"GitHub Projects","text":""},{"location":"topics/RFT/notes/#articles-blogs","title":"Articles &amp; Blogs","text":"<ul> <li>Fine-Tuning Small Language Models Practical Recommendations Medium</li> <li>Fine-Tuning Small Language Models for Code Review Accuracy NVIDIA Technical Blog</li> <li>Fine-Tuning and Small Language Models Blog Post premai.io Blog</li> <li>The Full Story of Large Language Models and RLHF AssemblyAI Blog</li> <li>Illustrating Reinforcement Learning from Human Feedback Hugging Face Blog</li> <li>Fine-Tuning Large Language Models Challenges and Best Practices Encora Insights</li> <li>LLM Fine-Tuning Techniques and Challenges SuperAnnotate Blog</li> </ul>"},{"location":"topics/RFT/notes/#online-courses","title":"Online Courses","text":"<ul> <li>Paper: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning Good youtube video discusses the paper from fundemental knowledge to its implementation in DeepSeek model.</li> </ul>"},{"location":"topics/RFT/notes/#research-papers","title":"Research Papers","text":"<ul> <li>Deep Reinforcement Learning from Human Preferences arXiv</li> <li>Learning to Summarize with Human Feedback arXiv</li> <li>Fine-Tuning Language Models from Human Preferences arXiv</li> <li>Training Language Models to Follow Instructions with Human Feedback arXiv</li> <li>Training a Helpful and Harmless Assistant with RLHF arXiv</li> <li>Scaling Laws for Reward Model Overoptimization arXiv</li> <li>The Limitations of Reinforcement Learning from Human Feedback arXiv</li> </ul>"},{"location":"topics/RFT/paper-review/","title":"Paper Reviews","text":""},{"location":"topics/RFT/paper-review/#content","title":"Content","text":""},{"location":"topics/RFT/paper-review/#lima-less-is-more-for-alignment","title":"LIMA: Less Is More for Alignment","text":"<p>ArXiv</p>"},{"location":"topics/RFT/paper-review/#summary-of-the-paper","title":"Summary of the Paper","text":"<p>This paper demonstrates that a 65B parameter LLaMA language model can achieve strong performance using only 1,000 high-quality training examples (prompts + responses) for fine-tuning. The authors detail the process of selecting high-quality training data and show that their approach even outperforms OpenAI's RLHF-based DaVinci-003.</p>"},{"location":"topics/RFT/paper-review/#key-hypothesis","title":"Key Hypothesis","text":"<p>The paper introduces the Superficial Alignment Hypothesis:</p> <p>A model's knowledge and capabilities are learned almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users.</p> <p>If this hypothesis holds, it explains why fine-tuning a pretrained model with a small, high-quality dataset can be sufficient.</p>"},{"location":"topics/RFT/paper-review/#data-selection-and-quality","title":"Data Selection and Quality","text":"<ul> <li>Among the 1,000 examples, 250 were authored by the researchers to ensure high quality. Many responses acknowledge the question before providing the answer, resembling a \"chain of thought\" approach.</li> <li>The authors emphasize that diversity in input and quality in output are more impactful than simply increasing the quantity of training data.</li> </ul>"},{"location":"topics/RFT/paper-review/#evaluation","title":"Evaluation","text":"<p>The authors evaluated LIMA using human evaluation and GPT-4 evaluation, comparing it against DaVinci-003 (RLHF-based) and other large language models. LIMA demonstrated competitive performance.</p> <p> Human preference evaluation, comparing LIMA to 5 different baselines across 300 test prompts.</p>"},{"location":"topics/RFT/paper-review/#data-quality-vs-quantity","title":"Data Quality vs. Quantity","text":"<ol> <li> <p>Impact of Data Quality:</p> <ul> <li>The authors trained a 7B LLaMA model on three datasets:</li> <li>Filtered Stack Exchange: Diverse prompts with high-quality responses.</li> <li>Unfiltered Stack Exchange: Diverse prompts without quality filtering.</li> <li>wikiHow: High-quality responses but limited to \"how-to\" prompts.</li> <li>Results show that diverse and high-quality data lead to better performance.</li> </ul> <p> Performance of 7B models trained with 2,000 examples from different sources.</p> </li> <li> <p>Impact of Data Quantity:</p> <ul> <li>Increasing the number of examples (up to 16x) from quality-filtered Stack Exchange did not significantly improve performance, indicating diminishing returns from scaling data quantity.</li> </ul> <p> Performance of 7B models trained with exponentially increasing amounts of data.</p> </li> </ol>"},{"location":"topics/RFT/paper-review/#key-takeaway","title":"Key Takeaway","text":"<p>The authors conclude that for alignment purposes, scaling input diversity and output quality has measurable benefits, while scaling data quantity alone offers limited gains.</p>"},{"location":"topics/RecSys/","title":"Recommendation Systems","text":"<p>How generative models and LLMs are transforming recommendation systems.</p>"},{"location":"topics/RecSys/#contents","title":"Contents","text":"<ul> <li>RecSys Notes - LLMs in recommendation systems</li> </ul> <p>Explore the intersection of LLMs and recommendation systems.</p>"},{"location":"topics/RecSys/notes/","title":"Recommendation System","text":"<p>Explore various ways in which generative modesl enhance recommendation systems.</p>"},{"location":"topics/RecSys/notes/#table-of-contents","title":"Table of Contents","text":""},{"location":"topics/RecSys/notes/#papersbooks","title":"Papers/Books","text":"<ul> <li>Recommendation with Generative Models. A comprehensive book</li> </ul>"},{"location":"topics/compressing/","title":"Model Compression","text":"<p>Techniques for making Large Language Models smaller, faster, and more efficient without significant loss in performance.</p>"},{"location":"topics/compressing/#contents","title":"Contents","text":"<ul> <li>Compression Techniques - Quantization (PTQ, QAT), pruning, and knowledge distillation</li> </ul> <p>Discover methods to deploy LLMs more efficiently through compression and optimization.</p>"},{"location":"topics/compressing/notes/","title":"Content","text":"<ul> <li>Compressing Large Language Models</li> <li>3 Key Approaches</li> <li>Quantization<ul> <li>PTQ</li> <li>QAT</li> </ul> </li> <li>Pruning<ul> <li>Unstructured Pruning</li> <li>Structured Pruning</li> </ul> </li> <li>Knowledge Distillation</li> <li>Reference</li> <li>Papers</li> <li>Online Articles</li> </ul>"},{"location":"topics/compressing/notes/#compressing-large-language-models","title":"Compressing Large Language Models","text":"<p>Model compression aims to reduce the size of machine learning models without sacrificing performance. The key benenfits is lower inference costs, e.g. running LLMs locally on mobile devices. </p>"},{"location":"topics/compressing/notes/#3-key-approaches","title":"3 Key Approaches","text":"<ol> <li>Quantization: Reducing the number of bits used to represent the model's weights.</li> <li>Pruning: Removing unimportant weights from the model. </li> <li>Knowledge Distillation: Training a smaller model to mimic the behavior of a larger model. </li> </ol>"},{"location":"topics/compressing/notes/#quantization","title":"Quantization","text":"<p>Lowering the precision of model paramters. Two common classes of quantization techniques: - Post-training quantization (PTQ) - Quantization-aware training (QAT)</p>"},{"location":"topics/compressing/notes/#ptq","title":"PTQ","text":"<p>Replacting parameters with a lower-precision data type (e.g. FP16 to INT8). Fastes and simplest way. However, often leads to performance degration.</p>"},{"location":"topics/compressing/notes/#qat","title":"QAT","text":"<p>Trainig model (from scratch) with lower-precision data types. For example, the BitNet architecture used ternary data type (e.g. 1.58 bit) to match the performance of  Llama. </p>"},{"location":"topics/compressing/notes/#pruning","title":"Pruning","text":"<p>Remove model components that have little impact on the performance, icnluding unstructured pruning and structured pruning.</p>"},{"location":"topics/compressing/notes/#unstructured-pruning","title":"Unstructured Pruning","text":"<p>Removes unimportant weights from a neural network, i.e removing weights with smallest absolute value. But it requires specialzied hardware due to sparse matrics operations. See Ref 1. </p>"},{"location":"topics/compressing/notes/#structured-pruning","title":"Structured Pruning","text":"<p>Remove entire structures from the neural network, e.g. attention heads, neurons, and layers. It avoids the spars matrix problem. Seen ref 2.</p>"},{"location":"topics/compressing/notes/#knowledge-distillation","title":"Knowledge Distillation","text":"<p>Knowledge distillation transfters knowledge from a larger teacher model to a smaller student model, e.g. use predictions from teacher and train on student model.</p> <p>Examples. Alpaca model finetuned the LLaMA7B model using synthetic data from OpenAI's text-davinci-003, seen ref 3. </p>"},{"location":"topics/compressing/notes/#reference","title":"Reference","text":""},{"location":"topics/compressing/notes/#papers","title":"Papers","text":"<ol> <li>To prune, or not to prune: exploring the efficacy of pruning for model compression</li> <li>A Survey on Model Compression for Large Language Models</li> <li>Alpaca: A Strong, Replicable Instruction-Following Model</li> </ol>"},{"location":"topics/compressing/notes/#online-articles","title":"Online Articles","text":"<ul> <li>Compressing Large Language Models (LLMs)</li> </ul>"},{"location":"topics/evaluation/","title":"LLM Evaluation","text":"<p>Comprehensive guide to evaluating Large Language Models using various metrics, benchmarks, and methodologies.</p>"},{"location":"topics/evaluation/#contents","title":"Contents","text":"<ul> <li>Evaluation Methods &amp; Metrics - Metrics (BLEU, ROUGE, perplexity), benchmarks (GLUE, MMLU, BigBench), and evaluation strategies</li> </ul> <p>Learn how to properly evaluate LLM performance across different tasks and capabilities.</p>"},{"location":"topics/evaluation/notes/","title":"Evaluation Methods & Metrics","text":""},{"location":"topics/evaluation/notes/#introduction","title":"Introduction","text":"<p>Large Language Models (LLMs) are powerful AI systems used for tasks like text generation and translation. Evaluating them ensures they perform well and are safe for use. This section breaks down how LLMs are assessed, focusing on key methods and metrics for a general audience.</p>"},{"location":"topics/evaluation/notes/#evaluation-categories","title":"Evaluation Categories","text":"<p>LLMs are evaluated in three main areas: - Knowledge and Capability: This checks how much the model knows and can do, like answering questions or summarizing text. Metrics like accuracy and perplexity are used, with benchmarks like GLUE and SuperGLUE being common. - Alignment Evaluation: This ensures the model\u2019s outputs match human values, checking for biases and ethical issues. Human reviews and datasets like Bias in Bios help here. - Safety Evaluation: This focuses on preventing harmful outputs, using tests for robustness against bad inputs and fact-checking tools.</p>"},{"location":"topics/evaluation/notes/#unexpected-detail","title":"Unexpected Detail","text":"<p>One interesting find is the use of HellaSwag, a benchmark testing commonsense reasoning, showing how LLMs handle everyday logic, which isn\u2019t always obvious from standard tests.</p>"},{"location":"topics/evaluation/notes/#survey-note-comprehensive-review-of-llm-evaluation-methods","title":"Survey Note: Comprehensive Review of LLM Evaluation Methods","text":""},{"location":"topics/evaluation/notes/#introduction-and-background","title":"Introduction and Background","text":"<p>Large Language Models (LLMs), such as those exemplified by models like GPT-4 and LLaMA, have revolutionized natural language processing by demonstrating remarkable capabilities in tasks ranging from text generation to complex reasoning. As of March 28, 2025, the rapid deployment of LLMs in diverse applications necessitates rigorous evaluation to ensure their performance, alignment with human values, and safety. This survey note synthesizes recent research to provide a detailed overview of how LLMs are evaluated, drawing from academic papers and surveys published in the last few years.</p> <p>The evaluation of LLMs is critical due to their potential risks, including private data leaks, generation of harmful content, and the emergence of superintelligent systems without adequate safeguards. Two key surveys, \"Evaluating Large Language Models: A Comprehensive Survey\" by Guo et al. (2023) and \"A Survey on Evaluation of Large Language Models\" by Chang et al. (2024), provide foundational insights into the methodologies and benchmarks used. These works, along with other studies, form the basis of this review.</p>"},{"location":"topics/evaluation/notes/#categorization-of-evaluation-methods","title":"Categorization of Evaluation Methods","text":"<p>Research suggests that LLM evaluation can be categorized into three primary dimensions: knowledge and capability evaluation, alignment evaluation, and safety evaluation, as outlined by Guo et al. (2023). Additionally, Chang et al. (2024) propose a task-based approach, focusing on specific areas like reasoning and ethics, which complements the categorical framework.</p>"},{"location":"topics/evaluation/notes/#knowledge-and-capability-evaluation","title":"Knowledge and Capability Evaluation","text":"<p>This category assesses the model's general knowledge and linguistic capabilities, crucial for tasks such as language understanding, generation, and translation. Common metrics include: - Perplexity: Measures how well the model predicts text, with lower values indicating better performance. It is particularly used in language modeling tasks. - Accuracy: Applied in tasks like question answering, where the model's output is compared against a correct answer. - Task-Specific Metrics: For text generation, metrics like BLEU, ROUGE, and METEOR are employed, comparing generated text to reference texts for tasks like machine translation and summarization.</p> <p>Benchmarks play a pivotal role in this evaluation. Notable examples include: - GLUE (General Language Understanding Evaluation), a multi-task benchmark for natural language understanding. - SuperGLUE, an extension of GLUE with more challenging tasks. - HellaSwag, which tests commonsense reasoning by evaluating the model's ability to complete sentences logically. - BigBench, a collection of tasks assessing diverse capabilities, including reasoning and language understanding. - MMLU (Massive Multitask Language Understanding), evaluating the model's performance across multiple domains. - ARC (AI2 Reasoning Challenge), focusing on science question answering for grades 3-9. - DROP (Reading Comprehension with Discrete Reasoning), testing the model's ability to extract and reason over details in paragraphs.</p> <p>These benchmarks provide standardized tests to compare LLM performance, with leaderboards often available on platforms like Hugging Face and PapersWithCode, ensuring transparency and reproducibility.</p>"},{"location":"topics/evaluation/notes/#alignment-evaluation","title":"Alignment Evaluation","text":"<p>Alignment evaluation ensures that LLM outputs align with human values, ethics, and intentions, addressing biases and fairness. This is particularly important given the potential for LLMs to perpetuate societal biases present in training data. Methods include: - Bias Detection: Utilizing datasets like Bias in Bios, which tests for gender bias in biographical text generation, and StereoSet, which measures stereotypical biases in language models. - Human Evaluation: Involves expert reviews or surveys to assess whether outputs align with human preferences, often used for subjective aspects like coherence and relevance. - Reinforcement Learning from Human Feedback (RLHF): A training and evaluation method where human feedback is used to align model behavior, also serving as a metric for alignment quality.</p> <p>Specific benchmarks for alignment include CivilComments, which evaluates toxicity in generated text, and human evaluation frameworks that assess ethical alignment. These methods are crucial for ensuring LLMs are fair and unbiased, especially in high-stakes applications like healthcare and education.</p>"},{"location":"topics/evaluation/notes/#safety-evaluation","title":"Safety Evaluation","text":"<p>Safety evaluation focuses on preventing LLMs from generating harmful, misleading, or dangerous content. This includes: - Adversarial Testing: Testing the model's robustness against adversarial inputs, such as prompt injections designed to elicit harmful responses. Benchmarks like Adversarial NLI assess this capability. - Factuality Checks: Evaluating the model's tendency to generate false information, using datasets like the Factuality Benchmark, which measures factual precision in long-form text generation. - Toxicity and Harmfulness Metrics: Tools like the Perspective API measure the toxicity of generated text, ensuring outputs are safe for public consumption.</p> <p>Jailbreak Challenges, which test the model's resistance to producing harmful content under adversarial conditions, are also significant. These evaluations are vital for deploying LLMs in customer support or public forums, where harmful outputs could negatively impact user experiences.</p>"},{"location":"topics/evaluation/notes/#additional-insights-and-metrics","title":"Additional Insights and Metrics","text":"<p>Beyond the categorical evaluations, research highlights the use of task-specific metrics and frameworks. For instance, F1 scores are used for entity recognition tasks, while ROUGE is standard for summarization. The literature also notes the importance of custom datasets for evaluating LLM-based products, as standardized benchmarks may not capture real-world use case nuances.</p> <p>Human evaluation remains a cornerstone, especially for alignment and safety, complementing automated metrics. The integration of Continuous Integration/Continuous Evaluation/Continuous Deployment (CI/CE/CD) in LLMOps, as discussed by Huang (2024), underscores the iterative nature of evaluation, ensuring models improve over time.</p>"},{"location":"topics/evaluation/notes/#challenges-and-future-directions","title":"Challenges and Future Directions","text":"<p>The surveys identify challenges such as data contamination in benchmarks, where models are tested on data similar to their training sets, and the rapid evolution of LLM capabilities outpacing benchmark relevance. Future research is likely to focus on developing comprehensive evaluation platforms that cover all aspects\u2014capabilities, alignment, safety, and applicability\u2014as suggested by Guo et al. (2023).</p>"},{"location":"topics/evaluation/notes/#table-of-key-benchmarks-and-their-focus-areas","title":"Table of Key Benchmarks and Their Focus Areas","text":"Benchmark Name Focus Area Example Tasks GLUE General Language Understanding Sentiment analysis, QA SuperGLUE Advanced Language Understanding Challenging reasoning tasks HellaSwag Commonsense Reasoning Sentence completion Bias in Bios Bias Detection Gender bias in biographies Adversarial NLI Safety, Robustness Handling adversarial inputs Factuality Benchmark Factuality, Safety Checking factual accuracy <p>This table summarizes key benchmarks, illustrating their role in evaluating different aspects of LLMs.</p>"},{"location":"topics/evaluation/notes/#conclusion","title":"Conclusion","text":"<p>The evaluation of LLMs is a complex, multifaceted process involving automated metrics, human evaluations, and specific benchmarks tailored to knowledge, alignment, and safety. Recent surveys by Guo et al. (2023) and Chang et al. (2024) provide a comprehensive framework, highlighting the need for continuous assessment to guide responsible development. As LLMs continue to evolve, so too must evaluation methods, ensuring they meet the high standards required for real-world deployment.</p>"},{"location":"topics/evaluation/notes/#key-citations","title":"Key Citations","text":"<ul> <li>Evaluating Large Language Models: A Comprehensive Survey</li> <li>A Survey on Evaluation of Large Language Models</li> <li>GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</li> <li>SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems</li> </ul>"}]}