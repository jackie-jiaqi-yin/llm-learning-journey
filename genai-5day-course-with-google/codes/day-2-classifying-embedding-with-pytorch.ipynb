{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a135c3f7",
   "metadata": {},
   "source": [
    "# Day 2 -  Text Classification with Embeddings using LlamaIndex, Azure OpenAI, and PyTorch\n",
    "\n",
    "## Overview\n",
    "Welcome back to the 5-day Generative AI course. In this notebook, we'll dive deep into text classification using embeddings. We'll learn how to:\n",
    "\n",
    "1. Generate embeddings from text using Azure OpenAI\n",
    "2. Build a neural network classifier using PyTorch\n",
    "3. Train and evaluate the model\n",
    "4. Make predictions on new text\n",
    "\n",
    "This tutorial takes a hands-on approach to understanding how embeddings can be used for text classification. Instead of training on raw text, we'll use embeddings as input features, which often leads to better results with smaller datasets.\n",
    "\n",
    "## Embedding Recap\n",
    "\n",
    "Embeddings are dense vector representations of text (or other data) where similar items are mapped to nearby points in a high-dimensional space. For example, the sentences \"I love dogs\" and \"I like puppies\" would have similar embedding vectors because they express similar concepts.\n",
    "\n",
    "Benefits of using embeddings include:\n",
    "- Reduced dimensionality compared to traditional text representations\n",
    "- Capture semantic relationships between words and phrases\n",
    "- Can be generated quickly using pre-trained models\n",
    "- Work well with neural networks\n",
    "\n",
    "More can be found in [notes](../notes/embeddings-and-vector-stores.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c9d1e4",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "\n",
    "- An Azure subscription\n",
    "- Access to Azure OpenAI service\n",
    "- Python 3.7 or later\n",
    "- Basic understanding of Python and machine learning concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbab8d4",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required packages:\n",
    "\n",
    "```python\n",
    "%pip install -q llama-index llama-index-core llama-index-llms-azure-openai python-dotenv torch scikit-learn pandas numpy tqdm\n",
    "```\n",
    "\n",
    "Let's understand what each package does:\n",
    "\n",
    "- `llama-index`: Framework for building LLM applications\n",
    "- `torch`: PyTorch deep learning framework\n",
    "- `scikit-learn`: For loading our dataset and utilities\n",
    "- `pandas`: For data manipulation\n",
    "- `numpy`: For numerical computations\n",
    "- `tqdm`: For progress bars\n",
    "- `python-dotenv`: For managing environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d13657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "tqdm.pandas() # Enable progress_apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f539de0",
   "metadata": {},
   "source": [
    "\n",
    "Create `.env` file with the following content:\n",
    "\n",
    "```\n",
    "AZURE_OPENAI_ENDPOINT=\"YOUR_AZURE_ENDPOINT\"\n",
    "AZURE_OPENAI_KEY=\"YOUR_API_KEY\"\n",
    "OPENAI_API_VERSION=\"YOUR_API_VERSION\"\n",
    "```\n",
    "\n",
    "Setup OpenAI client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631c3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model= 'text-embedding-ada-002',\n",
    "    azure_deployment = 'text-embedding-ada-002',\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version = '2023-05-15',\n",
    "    max_retries=3,\n",
    "    timeout=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423ff186",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "\n",
    "The [20 Newsgroups Text Dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) contains 18,000 newsgroups posts on 20 topics divided into training and test sets. The split between the training and val datasets are based on messages posted before and after a specific date. \n",
    "\n",
    "This dataset is great for text classification because:\n",
    "\n",
    "1. It contains real-world text data\n",
    "2. It's pre-categorized into distinct topics\n",
    "3. It's a manageable size for learning\n",
    "4. It contains a good mix of topics\n",
    "\n",
    "For this tutorial, you will use sampled subsets of the training and val sets, and perform some processing using `Pandas`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1786124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available categories:\n",
      "0. alt.atheism\n",
      "1. comp.graphics\n",
      "2. comp.os.ms-windows.misc\n",
      "3. comp.sys.ibm.pc.hardware\n",
      "4. comp.sys.mac.hardware\n",
      "5. comp.windows.x\n",
      "6. misc.forsale\n",
      "7. rec.autos\n",
      "8. rec.motorcycles\n",
      "9. rec.sport.baseball\n",
      "10. rec.sport.hockey\n",
      "11. sci.crypt\n",
      "12. sci.electronics\n",
      "13. sci.med\n",
      "14. sci.space\n",
      "15. soc.religion.christian\n",
      "16. talk.politics.guns\n",
      "17. talk.politics.mideast\n",
      "18. talk.politics.misc\n",
      "19. talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset=\"train\")\n",
    "newsgroups_test = fetch_20newsgroups(subset=\"test\")\n",
    "\n",
    "# View available categories\n",
    "print(\"Available categories:\")\n",
    "for idx, name in enumerate(newsgroups_train.target_names):\n",
    "    print(f\"{idx}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8f566",
   "metadata": {},
   "source": [
    "Example of what a record from teh training set looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f063cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups_train.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cc1973",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Raw text data often contains noise and unnecessary information. To remove any sensitive information like names and email addresses, you will take only the subject and body of each message. \n",
    "\n",
    "Let's clean it up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cc35062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import re\n",
    "\n",
    "def preprocess_newsgroup_row(data: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and prepare a single newsgroup post.\n",
    "    \n",
    "    Args:\n",
    "        data (str): Raw text of newsgroup post\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text containing only subject and body\n",
    "    \"\"\"\n",
    "    # extract subject and body\n",
    "    msg = email.message_from_string(data)\n",
    "    text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"\n",
    "\n",
    "    # remove email address for privacy\n",
    "    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n",
    "\n",
    "    # truncate to manageable length\n",
    "    text = text[:5000]\n",
    "    return text\n",
    "\n",
    "def preprocess_newsgroup_data(newsgroup_dataset) -> pd.DataFrame: \n",
    "    \"\"\"\n",
    "    Convert newsgroup dataset into a pandas DataFrame and clean the text.\n",
    "    \n",
    "    Args:\n",
    "        newsgroup_dataset: Scikit-learn newsgroup dataset\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataset with text and labels\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        \"Text\": newsgroup_dataset.data,\n",
    "        \"Label\": newsgroup_dataset.target\n",
    "    })\n",
    "\n",
    "    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n",
    "\n",
    "    # add readable category names\n",
    "    df[\"Class Name\"] = df[\"Label\"].apply(lambda x: newsgroup_dataset.target_names[x])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99e9c0",
   "metadata": {},
   "source": [
    "### Sampling the Dataset\n",
    "\n",
    "To make the tutorial manageable, we'll work with a subset of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2321960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(df: pd.DataFrame,\n",
    "                num_samples: int,\n",
    "                classes_to_keep: str) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a balanced sample of the dataset, keeping only specified classes.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        num_samples (int): Number of samples per class\n",
    "        classes_to_keep (str): String to filter class names\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Sampled and filtered dataset\n",
    "    \"\"\"\n",
    "    # sample equal number of samples from each class\n",
    "    df = (\n",
    "        df.groupby(\"Label\")[df.columns]\n",
    "        .apply(lambda x: x.sample(num_samples))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # keep only specified classes\n",
    "    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n",
    "\n",
    "    # Re-encode labels starting from 0\n",
    "    df[\"Class Name\"] = df[\"Class Name\"].astype(\"category\")\n",
    "    df['Encoded Label'] = df['Class Name'].cat.codes\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9e027be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set class distribution:\n",
      "Class Name\n",
      "sci.crypt          100\n",
      "sci.electronics    100\n",
      "sci.med            100\n",
      "sci.space          100\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set class distribution:\n",
      "Class Name\n",
      "sci.crypt          25\n",
      "sci.electronics    25\n",
      "sci.med            25\n",
      "sci.space          25\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "TRAIN_NUM_SAMPLES = 100\n",
    "TEST_NUM_SAMPLES = 25\n",
    "CLASSES_TO_KEEP = \"sci\"  # Class name should contain 'sci' to keep science categories\n",
    "\n",
    "df_train = sample_data(preprocess_newsgroup_data(newsgroups_train), \n",
    "                       TRAIN_NUM_SAMPLES, \n",
    "                       CLASSES_TO_KEEP)\n",
    "df_test = sample_data(preprocess_newsgroup_data(newsgroups_test), \n",
    "                      TEST_NUM_SAMPLES, \n",
    "                      CLASSES_TO_KEEP)\n",
    "\n",
    "# Verify class distribution\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "print(df_train[\"Class Name\"].value_counts())\n",
    "print(\"\\nTest set class distribution:\")\n",
    "print(df_test[\"Class Name\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23211b37",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "\n",
    "Now we'll convert our text data into embeddings using Azure OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20e49b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_fn(text: str) -> List[float]: \n",
    "    \"\"\"\n",
    "    Generate embedding vector for a piece of text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        list[float]: Embedding vector\n",
    "    \"\"\"\n",
    "    response = embed_model.get_text_embedding(text)\n",
    "    return response\n",
    "\n",
    "def create_embeddings(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add embedding vectors to DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with 'Text' column\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added 'Embeddings' column\n",
    "    \"\"\"\n",
    "    df[\"Embeddings\"] = df[\"Text\"].progress_apply(embed_fn)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "834463a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings (this may take a few minutes)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:52<00:00,  7.56it/s]\n",
      "100%|██████████| 100/100 [00:16<00:00,  6.10it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating embeddings (this may take a few minutes)...\")\n",
    "df_train = create_embeddings(df_train) \n",
    "df_test = create_embeddings(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b025a55",
   "metadata": {},
   "source": [
    "Note: Embedding generation can take some time, especially for larger datasets. Each text needs to be processed individually through the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1c77827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Encoded Label</th>\n",
       "      <th>Embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>Re: How to detect use of an illegal cipher?\\n\\...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.000520402449183166, -0.003595346584916115, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>Re: Once tapped, your code is no good any more...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.013352726586163044, -0.0009401556453667581,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>Re: Re-inventing Crypto Policy?  An EFF Statem...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.009235640987753868, -0.008010792545974255, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>powerful \"similarity\" too\\n\\nA Unix tool of cr...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.013430314138531685, -0.015955360606312752,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>Fear, Uncertainty, Doubt\\n\\n  I suspect that t...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.012093139812350273, -0.002550253877416253, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  Label Class Name  \\\n",
       "1100  Re: How to detect use of an illegal cipher?\\n\\...     11  sci.crypt   \n",
       "1101  Re: Once tapped, your code is no good any more...     11  sci.crypt   \n",
       "1102  Re: Re-inventing Crypto Policy?  An EFF Statem...     11  sci.crypt   \n",
       "1103  powerful \"similarity\" too\\n\\nA Unix tool of cr...     11  sci.crypt   \n",
       "1104  Fear, Uncertainty, Doubt\\n\\n  I suspect that t...     11  sci.crypt   \n",
       "\n",
       "      Encoded Label                                         Embeddings  \n",
       "1100              0  [0.000520402449183166, -0.003595346584916115, ...  \n",
       "1101              0  [0.013352726586163044, -0.0009401556453667581,...  \n",
       "1102              0  [0.009235640987753868, -0.008010792545974255, ...  \n",
       "1103              0  [-0.013430314138531685, -0.015955360606312752,...  \n",
       "1104              0  [0.012093139812350273, -0.002550253877416253, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e3932",
   "metadata": {},
   "source": [
    "## Build the PyTorch Model\n",
    "\n",
    "We'll create a custom dataset class and neural network model using PyTorch.\n",
    "\n",
    "### Custom Dataset\n",
    "PyTorch's Dataset class provides a clean interface for accessing our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "078d4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for newsgroup data.\n",
    "\n",
    "    Attributes:\n",
    "        embeddings (np.ndarray): Array of embedding vectors\n",
    "        labels (np.ndarray): Array of labels\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.embeddings[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776fd44",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "Our classifier is a simple feedforward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d45ab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGroupClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for classifying newsgroup posts.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer (embedding size)\n",
    "    - Hidden layer with ReLU activation\n",
    "    - Output layer (number of classes)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.model(X)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc438a2a",
   "metadata": {},
   "source": [
    "### Set up Training Infrastructure\n",
    "\n",
    "Configure the device (CPU/GPU) and prepare data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93cf1b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# prepare data\n",
    "x_train = np.stack(df_train[\"Embeddings\"])\n",
    "y_train = df_train[\"Encoded Label\"].values\n",
    "x_val = np.stack(df_test[\"Embeddings\"])\n",
    "y_val = df_test[\"Encoded Label\"].values\n",
    "\n",
    "# create datasets\n",
    "train_dataset = NewsGroupDataset(x_train, y_train)\n",
    "val_dataset = NewsGroupDataset(x_val, y_val)\n",
    "\n",
    "# create data loaders\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)\n",
    "\n",
    "# initialize model\n",
    "input_size = len(df_train['Embeddings'].iloc[0])\n",
    "hidden_size = input_size\n",
    "num_classes = len(df_train['Class Name'].unique())\n",
    "\n",
    "model = NewsGroupClassifier(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e06cb3",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Let's implement the training loop with proper progress tracking, early stop and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33ba1b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 60.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "Train Loss: 1.2364, Train Acc: 0.7500\n",
      "Val Loss: 1.0436, Val Acc: 0.8700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 61.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20\n",
      "Train Loss: 0.7296, Train Acc: 0.9500\n",
      "Val Loss: 0.6502, Val Acc: 0.8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 213.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20\n",
      "Train Loss: 0.3062, Train Acc: 0.9700\n",
      "Val Loss: 0.5351, Val Acc: 0.8800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 253.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20\n",
      "Train Loss: 0.1432, Train Acc: 0.9775\n",
      "Val Loss: 0.4686, Val Acc: 0.8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 144.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20\n",
      "Train Loss: 0.0838, Train Acc: 0.9925\n",
      "Val Loss: 0.5533, Val Acc: 0.8700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 246.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20\n",
      "Train Loss: 0.0546, Train Acc: 0.9950\n",
      "Val Loss: 0.5786, Val Acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 222.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20\n",
      "Train Loss: 0.0368, Train Acc: 1.0000\n",
      "Val Loss: 0.5883, Val Acc: 0.8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 147.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/20\n",
      "Train Loss: 0.0236, Train Acc: 1.0000\n",
      "Val Loss: 0.6258, Val Acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 231.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/20\n",
      "Train Loss: 0.0195, Train Acc: 1.0000\n",
      "Val Loss: 0.6388, Val Acc: 0.8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 244.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/20\n",
      "Train Loss: 0.0156, Train Acc: 1.0000\n",
      "Val Loss: 0.6391, Val Acc: 0.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 147.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/20\n",
      "Train Loss: 0.0129, Train Acc: 1.0000\n",
      "Val Loss: 0.6771, Val Acc: 0.8900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 250.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/20\n",
      "Train Loss: 0.0105, Train Acc: 1.0000\n",
      "Val Loss: 0.6807, Val Acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 241.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/20\n",
      "Train Loss: 0.0090, Train Acc: 1.0000\n",
      "Val Loss: 0.6956, Val Acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 144.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/20\n",
      "Train Loss: 0.0076, Train Acc: 1.0000\n",
      "Val Loss: 0.7018, Val Acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 261.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/20\n",
      "Train Loss: 0.0066, Train Acc: 1.0000\n",
      "Val Loss: 0.7134, Val Acc: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13/13 [00:00<00:00, 258.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): Neural network model\n",
    "        train_loader (DataLoader): Training data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimization algorithm\n",
    "        device: Device to run the model on\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (average loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # track statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(train_loader), correct / total\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        test_loader: DataLoader for test data\n",
    "        criterion: Loss function\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (average loss, accuracy)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(test_loader), correct / total\n",
    "     \n",
    "# train model\n",
    "NUM_EPOCHS = 20\n",
    "PATIENCE = 5\n",
    "best_val_acc = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model = model.state_dict()\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience > PATIENCE:\n",
    "            print(\"Early stopping...\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ee2930",
   "metadata": {},
   "source": [
    "### Mark Predictions\n",
    "\n",
    "Finally, let's create a function to make predictions on new text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea72932b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making prediction on example text...\n",
      "\n",
      "Predicted probabilities:\n",
      "sci.crypt: 0.17%\n",
      "sci.electronics: 6.09%\n",
      "sci.med: 1.08%\n",
      "sci.space: 92.66%\n"
     ]
    }
   ],
   "source": [
    "def predict_category(text, model, embed_model, class_names):\n",
    "    \"\"\"\n",
    "    Predict the category of a new text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        model: Trained PyTorch model\n",
    "        embed_model: Embedding model\n",
    "        class_names: List of class names\n",
    "        \n",
    "    Prints:\n",
    "        Probability for each class\n",
    "    \"\"\"\n",
    "    # Get embedding\n",
    "    embedding = embed_fn(text)\n",
    "    # Convert to tensor\n",
    "    input_tensor = torch.tensor(embedding, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    # get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = nn.functional.softmax(output, dim=1)[0]\n",
    "\n",
    "    print(\"\\nPredicted probabilities:\")\n",
    "    for i, category in enumerate(class_names): \n",
    "        print(f\"{category}: {probabilities[i] * 100:.2f}%\")\n",
    "\n",
    "new_text = \"\"\"\n",
    "First-timer looking to get out of here.\n",
    "\n",
    "Hi, I'm writing about my interest in travelling to the outer limits!\n",
    "\n",
    "What kind of craft can I buy? What is easiest to access from this 3rd rock?\n",
    "\n",
    "Let me know how to do that please.\n",
    "\"\"\"\n",
    "print(\"Making prediction on example text...\")\n",
    "predict_category(new_text, model, embed_model, df_test[\"Class Name\"].cat.categories)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
