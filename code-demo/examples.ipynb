{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook serves my practice of LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "Transformer is introduced in the seminal paper \"Attention is All You Need\" by Vaswani et al. in 2017, transformers are particularly known for their efficiency and effectiveness in handling sequential data.\n",
    "\n",
    "### Core Components of Transformers\n",
    "\n",
    "1. **Attention Mechanism**:\n",
    "   - The key innovation of the transformer model is the attention mechanism, specifically the \"self-attention\" mechanism. This allows the model to weigh the importance of different words within the input data regardless of their position in the sequence. In essence, self-attention gives the model the ability to focus on different parts of the input when producing a specific part of the output.\n",
    "\n",
    "2. **Multi-Head Attention**:\n",
    "   - In a transformer, the attention mechanism is extended into what is known as multi-head attention. This setup allows the model to jointly attend to information from different representation subspaces at different positions, providing a richer understanding of the context.\n",
    "\n",
    "3. **Layered Architecture**:\n",
    "   - Transformers are composed of a series of identical layers, each containing two main sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. Additionally, each sub-layer has a residual connection around it followed by layer normalization.\n",
    "\n",
    "4. **Positional Encoding**:\n",
    "   - Since transformers do not inherently process sequential data as a sequence (like RNNs do), they require some form of positional encoding to maintain the order of the input. Positional encodings are added to the input embeddings to provide some information about the relative or absolute position of the tokens in the sequence.\n",
    "\n",
    "### Advantages of Transformers\n",
    "\n",
    "- **Parallelization**: Unlike recurrent neural networks (RNNs), transformers do not require that the data be processed in order. This means that operations can be parallelized, significantly speeding up training.\n",
    "- **Long-range Dependencies**: Transformers can handle long-range dependencies in text, making them effective for applications like document summarization, where understanding broader context is crucial.\n",
    "- **Flexibility**: They can be adapted for a wide range of tasks beyond NLP, such as image classification (Vision Transformer), and time-series forecasting.\n",
    "\n",
    "### Applications\n",
    "\n",
    "Transformers are the backbone of many modern NLP systems, including:\n",
    "- **Text Generation**: Models like GPT (Generative Pre-trained Transformer) can generate coherent and contextually relevant text over extensive passages.\n",
    "- **Translation**: Models like the original Transformer architecture are highly effective at translation tasks.\n",
    "- **Sentiment Analysis, Named Entity Recognition, and more**: BERT (Bidirectional Encoder Representations from Transformers) and its variants have set new standards in various NLP benchmarks.\n",
    "\n",
    "The transformer model has not only improved the performance on traditional NLP tasks but has also enabled new applications and methods in AI, leading to ongoing innovations across many domains of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, how are you today?\"\n",
    "\n",
    "# Encode text to get token ids and attention mask\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "print(inputs)\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# The last hidden state is the first element of the output tuple\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# You can take the mean of the token embeddings to get a sentence-level representation\n",
    "sentence_embedding = last_hidden_states.mean(dim=1)\n",
    "print(sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval with BERT and FAISS\n",
    "Using transformers and large language models (LLMs) for information retrieval can significantly enhance the ability to understand and retrieve relevant information based on the semantic content of queries and documents. This is often done using embedding-based retrieval, where both documents and queries are converted into dense vectors, and similarity metrics are used to find the best matches.\n",
    "\n",
    "For this example, letâ€™s use the transformers library by Hugging Face, combined with FAISS for efficient similarity search among embeddings. We'll use a BERT model to generate embeddings for a set of documents and a query, then use FAISS to find the most relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Pandas\n",
      "Top results:\n",
      "1: Hugging Face provides state-of-the-art machine learning models. (Distance: 48.62348556518555)\n",
      "2: Pandas is an open source data analysis library. (Distance: 55.538551330566406)\n",
      "3: NumPy is a fundamental package for scientific computing. (Distance: 56.726688385009766)\n",
      "4: FAISS is designed for efficient similarity search. (Distance: 69.76235961914062)\n",
      "5: Transformers are models that handle sequential data. (Distance: 86.79019165039062)\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers faiss-cpu\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the mean of the last hidden state as document representation\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Example documents\n",
    "docs = [\n",
    "    \"Pandas is an open source data analysis library.\",\n",
    "    \"Hugging Face provides state-of-the-art machine learning models.\",\n",
    "    \"Transformers are models that handle sequential data.\",\n",
    "    \"FAISS is designed for efficient similarity search.\",\n",
    "    \"NumPy is a fundamental package for scientific computing.\"\n",
    "]\n",
    "\n",
    "# Convert documents to embeddings\n",
    "doc_embeddings = np.array([get_embedding(doc) for doc in docs])\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = doc_embeddings.shape[1]  # Dimension of embeddings\n",
    "index = faiss.IndexFlatL2(dimension)  # Using L2 distance for similarity\n",
    "index.add(doc_embeddings)  # Add embeddings to index\n",
    "\n",
    "# Example query\n",
    "query = \"Pandas\"\n",
    "query_embedding = get_embedding(query)\n",
    "\n",
    "# Perform search\n",
    "k = 5  # Number of nearest neighbors\n",
    "distances, indices = index.search(np.array([query_embedding]), k)\n",
    "\n",
    "# Display search results\n",
    "print(\"Query:\", query)\n",
    "print(\"Top results:\")\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"{i+1}: {docs[idx]} (Distance: {distances[0][i]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
