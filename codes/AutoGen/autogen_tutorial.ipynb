{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from autogen.agentchat import ConversableAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comfigure the LLM\n",
    "\n",
    "You need to set up your favoriate LLM API as the backend for AutoGen. Save the API key, url, and other information in a .env file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "llm_list = [\n",
    "    {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "        \"api_version\": os.getenv(\"OPENAI_API_VERSION\"),\n",
    "        \"base_url\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"config_list\": llm_list,\n",
    "    \"timeout\": 60,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the LLM is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a joke for you:\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "assistant = ConversableAgent(\n",
    "    name='assistant',\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False, # turn off code execution\n",
    "    function_map=None, # No registered functions, by default it is None,\n",
    "    human_input_mode='NEVER', # turn off human input\n",
    "    description='helpful assistant',\n",
    ")\n",
    "\n",
    "response = assistant.generate_reply(\n",
    "    messages = [\n",
    "        {\n",
    "            \"content\": \"tell me a joke\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Input\n",
    "\n",
    "In this section, I will explain how to use human input mode in AutoGen.\n",
    "\n",
    "There are three modes:\n",
    "\n",
    "1. `NEVER`: the agent will never prompt for human input. Under this mode, the conversation stops when the number of auto reply reaches the `max_consecutive_auto_reply` or when `is_termination_msg` is True.\n",
    "2. `TERMINATE`: The agent only prompts for human input only when a termination message is received or the number of auto reply reaches the `max_consecutive_auto_reply`.\n",
    "3. `ALWAYS`: the agent prompts for human input every time a message is received. Under this mode, the conversation stops when the human input is \"exit\", or when `is_termination_msg` is True and there is no human input.\n",
    "\n",
    "The following is an example when set to `ALWAYS`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mhuman_proxy\u001b[0m (to agent_with_number):\n",
      "\n",
      "17\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to human_proxy):\n",
      "\n",
      "too low\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mhuman_proxy\u001b[0m (to agent_with_number):\n",
      "\n",
      "23\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to human_proxy):\n",
      "\n",
      "too low\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mhuman_proxy\u001b[0m (to agent_with_number):\n",
      "\n",
      "43\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to human_proxy):\n",
      "\n",
      "too low\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mhuman_proxy\u001b[0m (to agent_with_number):\n",
      "\n",
      "51\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to human_proxy):\n",
      "\n",
      "too low\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mhuman_proxy\u001b[0m (to agent_with_number):\n",
      "\n",
      "53\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': '17', 'role': 'assistant', 'name': 'human_proxy'}, {'content': 'too low', 'role': 'user', 'name': 'agent_with_number'}, {'content': '23', 'role': 'assistant', 'name': 'human_proxy'}, {'content': 'too low', 'role': 'user', 'name': 'agent_with_number'}, {'content': '43', 'role': 'assistant', 'name': 'human_proxy'}, {'content': 'too low', 'role': 'user', 'name': 'agent_with_number'}, {'content': '51', 'role': 'assistant', 'name': 'human_proxy'}, {'content': 'too low', 'role': 'user', 'name': 'agent_with_number'}, {'content': '53', 'role': 'assistant', 'name': 'human_proxy'}], summary='53', cost={'usage_including_cached_inference': {'total_cost': 0.0021200000000000004, 'gpt-4o-2024-05-13': {'cost': 0.0021200000000000004, 'prompt_tokens': 400, 'completion_tokens': 8, 'total_tokens': 408}}, 'usage_excluding_cached_inference': {'total_cost': 0.0017250000000000002, 'gpt-4o-2024-05-13': {'cost': 0.0017250000000000002, 'prompt_tokens': 327, 'completion_tokens': 6, 'total_tokens': 333}}}, human_input=['23', '43', '51', '53'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_proxy = ConversableAgent(\n",
    "    name='human_proxy',\n",
    "    llm_config=False, # no need to use llm for human input\n",
    "    human_input_mode='ALWAYS',\n",
    "    description='human proxy',\n",
    ")\n",
    "\n",
    "agent_with_number = ConversableAgent(\n",
    "    name='agent_with_number',\n",
    "    system_message=\"\"\"\n",
    "        You are playing a game of guess-my-number. You have the number 53 in you mind, and I will try to guess it.\n",
    "        If i guess too hig, you will say 'too high', if i guess too low, you will say 'too low'.\n",
    "    \"\"\",\n",
    "    llm_config=llm_config,\n",
    "    is_termination_msg=lambda x: '53' in x['content'], # The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
    "    human_input_mode='NEVER', # turn off human input\n",
    "    description='agent with number',\n",
    ")\n",
    "\n",
    "human_proxy.initiate_chat(\n",
    "    recipient=agent_with_number,\n",
    "    message=\"17\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Use\n",
    "\n",
    "Tools are predefined functions that agents can use. Instead of writing arbitrary code, agents can call tools to perform actions, such as searching the web, performing calculations, reading files, or calling remote APIs. Because you can control what tools are available to an agent, you can control what actions an agent can perform.\n",
    "\n",
    "### 1. Define the tool\n",
    "\n",
    "The tool is a regular python function.\n",
    "To let LLM better understands our tools, we need provide docstrings (description) for each tool and define the input and output type of the tool. Note that to explain the definition of the input and output, we need to use `Annotated` from `typing`. If just writing the docstring of the input and output, all the docstrings will be function description only.\n",
    "```txt\n",
    "    Generate a random number from a Gamma distribution with the given shape and scale.\n",
    "\n",
    "        Args:\n",
    "            shape (int): The shape parameter of the Gamma distribution.\n",
    "            scale (int): The scale parameter of the Gamma distribution.\n",
    "\n",
    "        Returns:\n",
    "            int: A random number from the Gamma distribution.\n",
    "```\n",
    "\n",
    "The following is an example of a tool that generates a random number from a Gamma distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Annotated\n",
    "\n",
    "def generate_random_number(\n",
    "    shape: Annotated[int, \"The shape parameter of the Gamma distribution.\"],\n",
    "    scale: Annotated[int, \"The scale parameter of the Gamma distribution.\"]\n",
    ") -> Annotated[int, \"A random number from the Gamma distribution.\"]:\n",
    "    \"\"\"\n",
    "    Generate a random number from a Gamma distribution with the given shape and scale.\n",
    "\n",
    "    Args:\n",
    "        shape (int): The shape parameter of the Gamma distribution.\n",
    "        scale (int): The scale parameter of the Gamma distribution.\n",
    "\n",
    "    Returns:\n",
    "        int: A random number from the Gamma distribution.\n",
    "    \"\"\"\n",
    "    if (shape <= 0) or (scale <= 0):\n",
    "        raise ValueError(\"Shape and scale must be positive integers.\")\n",
    "        \n",
    "    return int(np.random.gamma(shape, scale))\n",
    "\n",
    "generate_random_number(2, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Register the tool\n",
    "\n",
    "A tool must be registered with at least two agents for it to be useful in conversation. The agent registered with the tool’s signature through `register_for_llm` can call the tool; the agent registered with the tool’s function object through `register_for_execution` can execute the tool’s function.\n",
    "\n",
    "- `register_for_llm`\n",
    "    - Used when the response needs to be interpreted by the LLM\n",
    "    - Typically for natural language responses\n",
    "    - Part of the conversation flow\n",
    "    - Affects the context window size\n",
    "    \n",
    "    ```python\n",
    "    # Used for responses that should be processed by the LLM\n",
    "    @agent.register_for_llm()\n",
    "    ```\n",
    "\n",
    "- `register_for_execution`\n",
    "    - Used for executing specific actions or code\n",
    "    - Handles non-llm tasks: running code, api calls, file operations, etc.\n",
    "\n",
    "    ```python\n",
    "    # Used for executing specific actions or code\n",
    "    @agent.register_for_execution()\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autogen.tools.tool.Tool at 0x1e258200cd0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant = ConversableAgent(\n",
    "    name='Assistant',\n",
    "    system_message=\"\"\"\n",
    "        You are a helpful assistant that can use tools to perform actions.\n",
    "        You can help with generating a random number from a Gamma distribution.\n",
    "        Return 'DONE' when the task is completed.\n",
    "        \"\"\",\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "\n",
    "# the user proxy is used for interacting with the assistant agent and executes tool calls\n",
    "user_proxy = ConversableAgent(\n",
    "    name='User',\n",
    "    llm_config=False, \n",
    "    is_termination_msg=lambda x: x.get('content') is not None and 'DONE' in x.get('content'),\n",
    "    human_input_mode='NEVER'\n",
    ")\n",
    "\n",
    "# register the tool with the assistant agent\n",
    "assistant.register_for_llm(\n",
    "    name='generator'\n",
    ")(generate_random_number)\n",
    "\n",
    "# register the tool with user proxy agent\n",
    "user_proxy.register_for_execution(\n",
    "    name='generator'\n",
    ")(generate_random_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we can see the function's signature. All the docstrings and input/output explaination are included automatically, such as function description, input and output type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'description': '\\n    Generate a random number from a Gamma distribution with the given shape and scale.\\n\\n    Args:\\n        shape (int): The shape parameter of the Gamma distribution.\\n        scale (int): The scale parameter of the Gamma distribution.\\n\\n    Returns:\\n        int: A random number from the Gamma distribution.\\n    ',\n",
       "   'name': 'generator',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'shape': {'type': 'integer',\n",
       "      'description': 'The shape parameter of the Gamma distribution.'},\n",
       "     'scale': {'type': 'integer',\n",
       "      'description': 'The scale parameter of the Gamma distribution.'}},\n",
       "    'required': ['shape', 'scale']}}}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant.llm_config['tools']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the tool\n",
    "\n",
    "Once the tool is registered, we can use it in conversation. In the code below, we ask the assistant to generate a random number from a Gamma distribution with the given shape and scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "Generate a random number from a Gamma distribution with the shape 2 and scale 100.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_2Q2RmhWCxw0XgK8VFnzTBsOt): generator *****\u001b[0m\n",
      "Arguments: \n",
      "{\"shape\":2,\"scale\":100}\n",
      "\u001b[32m**************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION generator...\n",
      "Call ID: call_2Q2RmhWCxw0XgK8VFnzTBsOt\n",
      "Input arguments: {'shape': 2, 'scale': 100}\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_2Q2RmhWCxw0XgK8VFnzTBsOt) *****\u001b[0m\n",
      "127\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "The generated random number from a Gamma distribution with shape 2 and scale 100 is 127.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "DONE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "number = user_proxy.initiate_chat(\n",
    "    recipient=assistant,\n",
    "    message=\"Generate a random number from a Gamma distribution with the shape 2 and scale 100.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Patterns\n",
    "\n",
    "There are four types of conversation:\n",
    "1. **Two-agent chat**: The simplest form of conversation pattern involves two agents communicating with each other using `initiate_chat`. This basic interaction allows direct communication between agents, with one agent initiating the conversation and the other responding.\n",
    "\n",
    "2. **Sequential chat**: This pattern enables a sequence of chats between two agents. The conversations are linked through a carryover mechanism that passes the summary of previous chats into the context of subsequent chats. This allows for continuous, context-aware conversations where each interaction builds upon previous discussions.\n",
    "\n",
    "3. **Group Chat**: This advanced pattern enables multiple agents to participate in a single conversation. The key consideration is determining which agent speaks next. AutoGen provides several selection strategies:\n",
    "    - `round_robin`: Agents take turns in a fixed order\n",
    "    - `random`: Random selection of next speaker\n",
    "    - `manual`: Human user selects the next speaker\n",
    "    - `auto`: Default mode where an LLM decides the next speaker\n",
    "    \n",
    "    The system also supports constraining speaker selection and custom selection functions. This enables building StateFlow models for deterministic workflows among agents.\n",
    "\n",
    "4. **Nested Chat**: This pattern allows packaging complex multi-agent workflows into a single agent interface for modular reuse in larger systems. By encapsulating multiple agents and their interactions within a single interface, nested chats enable building hierarchical and modular conversation systems. This is particularly useful for creating reusable components in larger automated workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Chat\n",
    "\n",
    "The figure illustrates how group chat works:\n",
    "\n",
    "![group_chat_figure](https://microsoft.github.io/autogen/0.2/assets/images/group-chat-2c88a3b4769d60898d0d4d2313d2d168.png)\n",
    "\n",
    "Teh `GroupChatManager` can use several strategies to select the next agents, and those strategies are mentioned above. To help the Group Chat Manager select the next agent, the `description` of the agents need to be provided. Without the `description`, the Group Chat Manager will use the agents’ `system_message`, which may be **not** be the best choice.\n",
    "\n",
    "First, create a `GroupChat` objet and provide the list of agents. Next, create a `GroupChatManager` object and provide the `GroupChat` object.\n",
    "\n",
    "Example:\n",
    "```python\n",
    "from autogen import GroupChat, GroupChatManager\n",
    "\n",
    "group_chat = GroupChat(\n",
    "    agents=[agent1, agent2, agent3],\n",
    "    messages=[],\n",
    "    max_round=6\n",
    ")\n",
    "\n",
    "group_chat_manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "# default is auto\n",
    "chat_result = agent1.initiate_chat(\n",
    "    group_chat_manager,\n",
    "    message=\"My number is 3, I want to turn it into 13.\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")\n",
    "```\n",
    "\n",
    "In the previous example, we set the `description` of the agents to help the Group Chat Manager select the next agent. This only helps the Group Chat Manager, however, does not help the participating agents to know about each other. Sometimes it is useful have each agent introduce themselves to other agents in the group chat. This can be done by setting the `send_introductions=True`.\n",
    "\n",
    "```python\n",
    "group_chat_with_instruction = GroupChat(\n",
    "    agents=[agent1, agent2, agent3],\n",
    "    messages=[],\n",
    "    max_round=6,\n",
    "    send_introductions=True\n",
    ")\n",
    "```\n",
    "\n",
    "### Group Chat in a Sequential Chat\n",
    "Group chat can also be used as a part of a sequential chat. In this case, the Group Chat Manager is treated as a regular agent in the sequence of two-agent chats.\n",
    "\n",
    "```python\n",
    "group_chat_manager_with_intros = GroupChatManager(\n",
    "    groupchat=group_chat_with_instruction,\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "chat_result = agent4.initial_chats(\n",
    "    group_chat_manager_with_intros,\n",
    "    message=\"My number is 3, I want to turn it into 13.\",\n",
    "    summary_method=\"reflection_with_llm\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Constrained Speaker Selection\n",
    "\n",
    "When the number of agents is large, it is hard to control conversation flow. `allowed_or_disallowed_speaker_transitions` argument is a dictionary that maps a given agent to a list of agents that can (or cannot) be selected to speak next. \n",
    "\n",
    "```python\n",
    "allowed_or_disallowed_speaker_transitions = {\n",
    "    \"agent1\": [\"agent2\", \"agent3\"],\n",
    "    \"agent2\": [\"agent1\", \"agent3\"]\n",
    "    }\n",
    "\n",
    "constrained_group_chat = GroupChat(\n",
    "    agents=[agent1, agent2, agent3],\n",
    "    messages=[],\n",
    "    max_round=6,\n",
    "    allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
    "    speaker_transitions_type=\"allowed\",\n",
    "    send_introductions=True,\n",
    ")\n",
    "\n",
    "# the following is the same as the above\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Chats\n",
    "\n",
    "Package a workflow into a single agent for reuse in a larger workflow. Nested chats is powered by the nested chats handler, which is a pluggable component of `ConversableAgent`.  In each of the nested chats, the sender agent is always the same agent that triggered the nested chats. In the end, the nested chat handler uses the results of the nested chats to produce a response to the original message. By default, the nested chat handler uses the summary of the last chat as the response.\n",
    "\n",
    "![nested-chat](https://microsoft.github.io/autogen/0.2/assets/images/nested-chats-857c3a6a2115d53d588e4fea09d5daf7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
