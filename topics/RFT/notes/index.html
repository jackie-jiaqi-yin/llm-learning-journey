<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Personal learning notes on Large Language Models, AI Agents, RAG, and more"><meta name=author content="Jackie Yin"><link href=https://jackie-jiaqi-yin.github.io/llm-learning-journey/topics/RFT/notes/ rel=canonical><link href=../ rel=prev><link href=../paper-review/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.22"><title>RLHF Overview - LLM Learning Journey</title><link rel=stylesheet href=../../../assets/stylesheets/main.84d31ad4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../css/timeago.css><link rel=stylesheet href=../../../stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#reinforcement-learning-on-language-models class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="LLM Learning Journey" class="md-header__button md-logo" aria-label="LLM Learning Journey" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 2a2 2 0 0 1 2 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 0 1 7 7h1a1 1 0 0 1 1 1v3a1 1 0 0 1-1 1h-1v1a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-1H2a1 1 0 0 1-1-1v-3a1 1 0 0 1 1-1h1a7 7 0 0 1 7-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 0 1 2-2M7.5 13A2.5 2.5 0 0 0 5 15.5 2.5 2.5 0 0 0 7.5 18a2.5 2.5 0 0 0 2.5-2.5A2.5 2.5 0 0 0 7.5 13m9 0a2.5 2.5 0 0 0-2.5 2.5 2.5 2.5 0 0 0 2.5 2.5 2.5 2.5 0 0 0 2.5-2.5 2.5 2.5 0 0 0-2.5-2.5"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> LLM Learning Journey </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> RLHF Overview </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/jackie-jiaqi-yin/llm-learning-journey title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> llm-learning-journey </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Topics </a> </li> <li class=md-tabs__item> <a href=../../../courses/ class=md-tabs__link> Online Courses </a> </li> <li class=md-tabs__item> <a href=../../../tags/ class=md-tabs__link> Tags </a> </li> <li class=md-tabs__item> <a href=../../../about/ class=md-tabs__link> About </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="LLM Learning Journey" class="md-nav__button md-logo" aria-label="LLM Learning Journey" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 2a2 2 0 0 1 2 2c0 .74-.4 1.39-1 1.73V7h1a7 7 0 0 1 7 7h1a1 1 0 0 1 1 1v3a1 1 0 0 1-1 1h-1v1a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2v-1H2a1 1 0 0 1-1-1v-3a1 1 0 0 1 1-1h1a7 7 0 0 1 7-7h1V5.73c-.6-.34-1-.99-1-1.73a2 2 0 0 1 2-2M7.5 13A2.5 2.5 0 0 0 5 15.5 2.5 2.5 0 0 0 7.5 18a2.5 2.5 0 0 0 2.5-2.5A2.5 2.5 0 0 0 7.5 13m9 0a2.5 2.5 0 0 0-2.5 2.5 2.5 2.5 0 0 0 2.5 2.5 2.5 2.5 0 0 0 2.5-2.5 2.5 2.5 0 0 0-2.5-2.5"/></svg> </a> LLM Learning Journey </label> <div class=md-nav__source> <a href=https://github.com/jackie-jiaqi-yin/llm-learning-journey title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </div> <div class=md-source__repository> llm-learning-journey </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Topics </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Topics </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_2> <div class="md-nav__link md-nav__container"> <a href=../../AI-Agent/ class="md-nav__link "> <span class=md-ellipsis> AI Agents </span> </a> <label class="md-nav__link " for=__nav_2_2 id=__nav_2_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> AI Agents </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../AI-Agent/notes/ class=md-nav__link> <span class=md-ellipsis> Overview & Frameworks </span> </a> </li> <li class=md-nav__item> <a href=../../AI-Agent/paper-review/ class=md-nav__link> <span class=md-ellipsis> Paper Reviews </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_3> <div class="md-nav__link md-nav__container"> <a href=../../RAG/ class="md-nav__link "> <span class=md-ellipsis> RAG (Retrieval-Augmented Generation) </span> </a> <label class="md-nav__link " for=__nav_2_3 id=__nav_2_3_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> RAG (Retrieval-Augmented Generation) </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../RAG/notes/ class=md-nav__link> <span class=md-ellipsis> RAG Guide & Best Practices </span> </a> </li> <li class=md-nav__item> <a href=../../RAG/paper-review/ class=md-nav__link> <span class=md-ellipsis> Paper Reviews </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_4 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> Reinforcement Learning & Fine-Tuning </span> </a> <label class="md-nav__link " for=__nav_2_4 id=__nav_2_4_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=true> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Reinforcement Learning & Fine-Tuning </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> RLHF Overview </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> RLHF Overview </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#overview class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> <nav class=md-nav aria-label=Overview> <ul class=md-nav__list> <li class=md-nav__item> <a href=#process-and-benefits class=md-nav__link> <span class=md-ellipsis> Process and Benefits </span> </a> </li> <li class=md-nav__item> <a href=#challenges-and-considerations class=md-nav__link> <span class=md-ellipsis> Challenges and Considerations </span> </a> </li> <li class=md-nav__item> <a href=#unexpected-detail class=md-nav__link> <span class=md-ellipsis> Unexpected Detail </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#survey-note-reinforcement-learning-for-fine-tuning-small-language-models class=md-nav__link> <span class=md-ellipsis> Survey Note: Reinforcement Learning for Fine-Tuning Small Language Models </span> </a> <nav class=md-nav aria-label="Survey Note: Reinforcement Learning for Fine-Tuning Small Language Models"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#introduction class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=#background-on-fine-tuning-and-rlhf class=md-nav__link> <span class=md-ellipsis> Background on Fine-Tuning and RLHF </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../paper-review/ class=md-nav__link> <span class=md-ellipsis> Paper Reviews </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_5> <div class="md-nav__link md-nav__container"> <a href=../../Prompt/ class="md-nav__link "> <span class=md-ellipsis> Prompt Engineering </span> </a> <label class="md-nav__link " for=__nav_2_5 id=__nav_2_5_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_5_label aria-expanded=false> <label class=md-nav__title for=__nav_2_5> <span class="md-nav__icon md-icon"></span> Prompt Engineering </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Prompt/notes/ class=md-nav__link> <span class=md-ellipsis> Prompt Engineering Guide </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_6> <div class="md-nav__link md-nav__container"> <a href=../../evaluation/ class="md-nav__link "> <span class=md-ellipsis> Evaluation </span> </a> <label class="md-nav__link " for=__nav_2_6 id=__nav_2_6_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_6_label aria-expanded=false> <label class=md-nav__title for=__nav_2_6> <span class="md-nav__icon md-icon"></span> Evaluation </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../evaluation/notes/ class=md-nav__link> <span class=md-ellipsis> Evaluation Methods & Metrics </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_7> <div class="md-nav__link md-nav__container"> <a href=../../compressing/ class="md-nav__link "> <span class=md-ellipsis> Model Compression </span> </a> <label class="md-nav__link " for=__nav_2_7 id=__nav_2_7_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_7_label aria-expanded=false> <label class=md-nav__title for=__nav_2_7> <span class="md-nav__icon md-icon"></span> Model Compression </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../compressing/notes/ class=md-nav__link> <span class=md-ellipsis> Compression Techniques </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_8> <div class="md-nav__link md-nav__container"> <a href=../../RecSys/ class="md-nav__link "> <span class=md-ellipsis> Recommendation Systems </span> </a> <label class="md-nav__link " for=__nav_2_8 id=__nav_2_8_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_8_label aria-expanded=false> <label class=md-nav__title for=__nav_2_8> <span class="md-nav__icon md-icon"></span> Recommendation Systems </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../RecSys/notes/ class=md-nav__link> <span class=md-ellipsis> RecSys Notes </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_9> <div class="md-nav__link md-nav__container"> <a href=../../Others/ class="md-nav__link "> <span class=md-ellipsis> Others </span> </a> <label class="md-nav__link " for=__nav_2_9 id=__nav_2_9_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_9_label aria-expanded=false> <label class=md-nav__title for=__nav_2_9> <span class="md-nav__icon md-icon"></span> Others </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../Others/notes/ class=md-nav__link> <span class=md-ellipsis> Miscellaneous Notes </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../../../courses/ class="md-nav__link "> <span class=md-ellipsis> Online Courses </span> </a> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Online Courses </label> <ul class=md-nav__list data-md-scrollfix> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../tags/ class=md-nav__link> <span class=md-ellipsis> Tags </span> </a> </li> <li class=md-nav__item> <a href=../../../about/ class=md-nav__link> <span class=md-ellipsis> About </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=reinforcement-learning-on-language-models>Reinforcement Learning on Language Models<a class=headerlink href=#reinforcement-learning-on-language-models title="Permanent link">&para;</a></h1> <h2 id=overview>Overview<a class=headerlink href=#overview title="Permanent link">&para;</a></h2> <p>Reinforcement learning (RL) is a method where models learn from feedback, often in the form of rewards, to improve performance. When applied to fine-tuning small language models, RL from human feedback (RLHF) helps align these models with human preferences, making them better at specific tasks like text generation or question answering. While much of the research focuses on large models, small language models—those with fewer parameters, often under 1 billion—can also benefit, especially for niche applications where efficiency is key.</p> <h3 id=process-and-benefits>Process and Benefits<a class=headerlink href=#process-and-benefits title="Permanent link">&para;</a></h3> <p>RLHF involves several steps: starting with a pre-trained model, fine-tuning it with supervised learning, training a reward model based on human feedback, and then using RL to optimize the model for higher rewards. For small language models, this can enhance performance on tasks like code generation or customer support chatbots, offering lower latency and reduced memory use compared to large models.</p> <h3 id=challenges-and-considerations>Challenges and Considerations<a class=headerlink href=#challenges-and-considerations title="Permanent link">&para;</a></h3> <p>Small language models may struggle with capturing complex human preferences due to limited capacity, and computational resources can be a bottleneck. However, techniques like parameter-efficient fine-tuning (PEFT) can help mitigate these issues, making RLHF feasible for smaller models.</p> <h3 id=unexpected-detail>Unexpected Detail<a class=headerlink href=#unexpected-detail title="Permanent link">&para;</a></h3> <p>An interesting finding is that small models, when fine-tuned with RLHF, can sometimes outperform larger models in specific, narrow tasks, such as code review accuracy, due to their efficiency and lower latency, as seen in recent studies (<a href=https://developer.nvidia.com/blog/fine-tuning-small-language-models-to-optimize-code-review-accuracy/ >NVIDIA Technical Blog</a>).</p> <hr> <h2 id=survey-note-reinforcement-learning-for-fine-tuning-small-language-models>Survey Note: Reinforcement Learning for Fine-Tuning Small Language Models<a class=headerlink href=#survey-note-reinforcement-learning-for-fine-tuning-small-language-models title="Permanent link">&para;</a></h2> <h3 id=introduction>Introduction<a class=headerlink href=#introduction title="Permanent link">&para;</a></h3> <p>Fine-tuning language models is a critical process in adapting pre-trained models to specific tasks or domains, leveraging their general language understanding for specialized applications. Reinforcement learning, particularly reinforcement learning from human feedback, has emerged as a powerful technique for aligning these models with human preferences, especially in tasks where direct supervision is challenging. While much of the recent research has focused on LLMs, there is growing interest in applying these techniques to small language models (SLMs), defined as models with fewer parameters (often under 1 billion), due to their efficiency and suitability for resource-constrained environments. This survey note explores the current state of RLHF for fine-tuning SLMs, reviewing key methodologies, challenges, and research findings, with a focus on their applicability and limitations.</p> <h3 id=background-on-fine-tuning-and-rlhf>Background on Fine-Tuning and RLHF<a class=headerlink href=#background-on-fine-tuning-and-rlhf title="Permanent link">&para;</a></h3> <p>Fine-tuning involves further training a pre-trained language model on a smaller, task-specific dataset to adapt it for particular applications, such as text classification, question answering, or content generation. Traditionally, this has been done using supervised learning, where the model is trained on labeled data. However, for tasks requiring nuanced human judgment, RL offers an alternative by allowing the model to learn from feedback in the form of rewards or punishments.</p> <p>RLHF specifically integrates RL with human feedback, aiming to align model outputs with human preferences. The process typically includes: - <strong>Pre-trained Language Model:</strong> Starting with a model pre-trained on a large corpus of text, such as BERT or smaller transformer-based models. - <strong>Supervised Fine-Tuning (SFT):</strong> Fine-tuning the model on a dataset of human-generated text relevant to the task, helping it understand task-specific requirements. - <strong>Reward Model Training:</strong> Training a separate model to predict the quality of the language model's outputs based on human preferences, often using pairwise comparisons where humans rank outputs for given inputs. - <strong>RL Fine-Tuning:</strong> Using the reward model to guide further fine-tuning with RL algorithms, such as Proximal Policy Optimization (PPO), to maximize the expected reward, aligning the model more closely with human values.</p> <p>This methodology has been pivotal in developing models like ChatGPT, which rely on RLHF for producing helpful and safe responses (<a href=https://www.assemblyai.com/blog/the-full-story-of-large-language-models-and-rlhf>AssemblyAI Blog</a>).</p> <p><img alt=rlm src=../figs/base-rlm.png></p> <p><em>Figure 1. A preference (or reward) model could be used to further train the baseline model to prioritize responses with higher preference scores.</em></p> <h4 id=key-research-on-rlhf-for-language-models>Key Research on RLHF for Language Models<a class=headerlink href=#key-research-on-rlhf-for-language-models title="Permanent link">&para;</a></h4> <p>The development of RLHF for language models has been marked by several seminal works, primarily focused on LLMs. Below is a table summarizing key papers and their contributions:</p> <table> <thead> <tr> <th><strong>Year</strong></th> <th><strong>Authors</strong></th> <th><strong>Title</strong></th> <th><strong>Contribution</strong></th> </tr> </thead> <tbody> <tr> <td>2017</td> <td>Christiano et al.</td> <td>Deep Reinforcement Learning from Human Preferences</td> <td>Introduced RLHF, demonstrating its use in general RL settings, laying groundwork.</td> </tr> <tr> <td>2019</td> <td>Ziegler et al.</td> <td>Fine-Tuning Language Models from Human Preferences</td> <td>Explored early applications of RLHF to language models, focusing on preference alignment.</td> </tr> <tr> <td>2020</td> <td>Stiennon et al.</td> <td>Learning to Summarize with Human Feedback</td> <td>Applied RLHF to text summarization, showing improved quality based on human preferences.</td> </tr> <tr> <td>2022</td> <td>Ouyang et al.</td> <td>Training Language Models to Follow Instructions with Human Feedback</td> <td>Developed InstructGPT, using RLHF for general-purpose instruction-following models.</td> </tr> <tr> <td>2022</td> <td>Bai et al.</td> <td>Training a Helpful and Harmless Assistant with RLHF</td> <td>Refined RLHF for creating safe and helpful assistants, addressing alignment issues.</td> </tr> <tr> <td>2022</td> <td>Menick et al.</td> <td>Scaling Laws for Reward Model Overoptimization</td> <td>Investigated challenges of reward model overoptimization in RLHF, impacting performance.</td> </tr> <tr> <td>2020</td> <td>Gabriel et al.</td> <td>The Limitations of Reinforcement Learning from Human Feedback</td> <td>Discussed ethical concerns and limitations, such as bias amplification and preference definition.</td> </tr> </tbody> </table> <p>These papers highlight the evolution of RLHF, from its theoretical foundations to practical implementations, primarily with LLMs. For instance, Ouyang et al. (2022) demonstrated how RLHF enabled InstructGPT to follow complex instructions, a significant step toward general-purpose assistants (<a href=https://arxiv.org/abs/2203.02155>arXiv</a>).</p> <h4 id=application-to-small-language-models>Application to Small Language Models<a class=headerlink href=#application-to-small-language-models title="Permanent link">&para;</a></h4> <p>While most research has focused on LLMs, the principles of RLHF can be applied to SLMs, though with specific considerations. SLMs, often with parameters in the range of tens to hundreds of millions, are designed for efficiency, lower latency, and deployment in resource-constrained environments. Recent studies suggest that SLMs can be competitive for real-world tasks, particularly when fine-tuned for specific, narrow applications (<a href=https://medium.com/@liana.napalkova/fine-tuning-small-language-models-practical-recommendations-68f32b0535ca>Medium</a>).</p> <ul> <li><strong>Efficiency and Resource Constraints:</strong> SLMs require less computational power, making RLHF more accessible for researchers and practitioners with limited resources. Techniques like parameter-efficient fine-tuning (PEFT), such as Low-Rank Adaptation (LoRA), can further reduce the computational burden, enabling RLHF on smaller models (<a href=https://www.superannotate.com/blog/llm-fine-tuning>SuperAnnotate Blog</a>).</li> <li><strong>Task Specificity:</strong> SLMs are often better suited for niche tasks, such as code review or customer support chatbots, where large models might be overkill. For example, a study by NVIDIA showed that a fine-tuned Llama 3 8B model with LoRA improved code review accuracy by 18%, outperforming larger models in specific tasks (<a href=https://developer.nvidia.com/blog/fine-tuning-small-language-models-to-optimize-code-review-accuracy/ >NVIDIA Technical Blog</a>).</li> <li><strong>Challenges:</strong> SLMs may have limited capacity to capture complex human preferences, potentially leading to poorer generalization compared to LLMs. Additionally, the data requirements for training reward models in RLHF might be more challenging for SLMs, given their smaller parameter space and potential for overfitting.</li> </ul> <p>Despite these challenges, recent blog posts and practical guides suggest that RLHF can be adapted for SLMs, with careful planning and monitoring (<a href=https://blog.premai.io/fine-tuning-small-language-models/ >premai.io Blog</a>). For instance, the process involves starting with small datasets (5-10%) to test performance and iteratively refining hyperparameters, which is particularly feasible for SLMs due to their lower resource needs (<a href=https://insights.encora.com/insights/fine-tuning-small-language-models-cost-effective-performance-for-business-use-cases>Encora Insights</a>).</p> <h4 id=challenges-and-limitations>Challenges and Limitations<a class=headerlink href=#challenges-and-limitations title="Permanent link">&para;</a></h4> <p>RLHF, whether applied to LLMs or SLMs, faces several challenges: - <strong>Reward Model Robustness:</strong> As highlighted by Menick et al. (2022), reward models can be exploited by the policy model, leading to overoptimization and degraded performance (<a href=https://arxiv.org/abs/2210.13388>arXiv</a>). - <strong>Data Quality and Quantity:</strong> Human feedback can be costly and inconsistent, with annotators often disagreeing, adding variance to training data (<a href=https://huggingface.co/blog/rlhf>Hugging Face Blog</a>). For SLMs, the smaller parameter space might exacerbate issues with data scarcity. - <strong>Ethical Concerns:</strong> Gabriel et al. (2020) discuss potential biases in RLHF, such as amplifying existing biases in training data, which could be more pronounced in SLMs due to their limited capacity to mitigate such biases (<a href=https://arxiv.org/abs/2011.07783>arXiv</a>).</p> <p>For SLMs specifically, additional challenges include: - <strong>Computational Efficiency:</strong> While SLMs are less resource-intensive, RLHF still requires significant computational power for reward model training and iterative optimization, which can be a bottleneck for smaller setups. - <strong>Generalization:</strong> SLMs may struggle to generalize across diverse tasks, particularly when fine-tuned with RLHF, due to their limited capacity compared to LLMs.</p> <h4 id=future-directions>Future Directions<a class=headerlink href=#future-directions title="Permanent link">&para;</a></h4> <p>Given the current state of research, future work could focus on: - Developing tailored RLHF methodologies for SLMs, addressing their unique constraints and capabilities. - Exploring hybrid approaches, combining RLHF with other fine-tuning techniques like PEFT, to enhance performance on SLMs. - Investigating the trade-offs between model size, performance, and computational cost in RLHF, potentially leading to guidelines for selecting the appropriate model size for specific tasks.</p> <h4 id=conclusion>Conclusion<a class=headerlink href=#conclusion title="Permanent link">&para;</a></h4> <p>RLHF has proven to be a transformative technique for aligning language models with human preferences, with significant advancements driven by research on LLMs. While direct applications to SLMs are less documented, the principles of RLHF can be adapted, offering potential for efficient, task-specific fine-tuning. Challenges such as computational efficiency, data quality, and generalization need further exploration, particularly for SLMs. As the field progresses, continued research will be essential to fully realize the potential of RLHF in fine-tuning small language models, ensuring they meet the needs of diverse, resource-constrained applications.</p> <h1 id=reference>Reference<a class=headerlink href=#reference title="Permanent link">&para;</a></h1> <h2 id=github-projects>GitHub Projects<a class=headerlink href=#github-projects title="Permanent link">&para;</a></h2> <h2 id=articles-blogs>Articles &amp; Blogs<a class=headerlink href=#articles-blogs title="Permanent link">&para;</a></h2> <ul> <li>Fine-Tuning Small Language Models Practical Recommendations <a href=https://medium.com/@liana.napalkova/fine-tuning-small-language-models-practical-recommendations-68f32b0535ca>Medium</a></li> <li>Fine-Tuning Small Language Models for Code Review Accuracy <a href=https://developer.nvidia.com/blog/fine-tuning-small-language-models-to-optimize-code-review-accuracy/ >NVIDIA Technical Blog</a></li> <li>Fine-Tuning and Small Language Models Blog Post <a href=https://blog.premai.io/fine-tuning-small-language-models/ >premai.io Blog</a></li> <li>The Full Story of Large Language Models and RLHF <a href=https://www.assemblyai.com/blog/the-full-story-of-large-language-models-and-rlhf>AssemblyAI Blog</a></li> <li>Illustrating Reinforcement Learning from Human Feedback <a href=https://huggingface.co/blog/rlhf>Hugging Face Blog</a></li> <li>Fine-Tuning Large Language Models Challenges and Best Practices <a href=https://insights.encora.com/insights/fine-tuning-large-language-models-challenges-and-best-practices>Encora Insights</a></li> <li>LLM Fine-Tuning Techniques and Challenges <a href=https://www.superannotate.com/blog/llm-fine-tuning>SuperAnnotate Blog</a></li> </ul> <h2 id=online-courses>Online Courses<a class=headerlink href=#online-courses title="Permanent link">&para;</a></h2> <ul> <li><a href="https://www.youtube.com/watch?v=XMnxKGVnEUc">Paper: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a> Good youtube video discusses the paper from fundemental knowledge to its implementation in DeepSeek model.</li> </ul> <h2 id=research-papers>Research Papers<a class=headerlink href=#research-papers title="Permanent link">&para;</a></h2> <ul> <li>Deep Reinforcement Learning from Human Preferences <a href=https://arxiv.org/abs/1706.03741>arXiv</a></li> <li>Learning to Summarize with Human Feedback <a href=https://arxiv.org/abs/2009.01325>arXiv</a></li> <li>Fine-Tuning Language Models from Human Preferences <a href=https://arxiv.org/abs/1909.08593>arXiv</a></li> <li>Training Language Models to Follow Instructions with Human Feedback <a href=https://arxiv.org/abs/2203.02155>arXiv</a></li> <li>Training a Helpful and Harmless Assistant with RLHF <a href=https://arxiv.org/abs/2204.05862>arXiv</a></li> <li>Scaling Laws for Reward Model Overoptimization <a href=https://arxiv.org/abs/2210.13388>arXiv</a></li> <li>The Limitations of Reinforcement Learning from Human Feedback <a href=https://arxiv.org/abs/2011.07783>arXiv</a></li> </ul> <aside class=md-source-file> <span class=md-source-file__fact> <span class=md-icon title="Last update"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="October 26, 2025 06:29:52 UTC"><span class=timeago datetime=2025-10-26T06:29:52+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="October 26, 2025 06:29:52 UTC">2025-10-26</span> </span> <span class=md-source-file__fact> <span class=md-icon title=Created> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.47 15.08 11 13V7h1.5v5.25l3.08 1.83c-.41.28-.79.62-1.11 1m-1.39 4.84c-.36.05-.71.08-1.08.08-4.42 0-8-3.58-8-8s3.58-8 8-8 8 3.58 8 8c0 .37-.03.72-.08 1.08.69.1 1.33.32 1.92.64.1-.56.16-1.13.16-1.72 0-5.5-4.5-10-10-10S2 6.5 2 12s4.47 10 10 10c.59 0 1.16-.06 1.72-.16-.32-.59-.54-1.23-.64-1.92M18 15v3h-3v2h3v3h2v-3h3v-2h-3v-3z"/></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago" title="March 22, 2025 00:47:45 UTC"><span class=timeago datetime=2025-03-22T00:47:45+00:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date" title="March 22, 2025 00:47:45 UTC">2025-03-22</span> </span> </aside> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../ class="md-footer__link md-footer__link--prev" aria-label="Previous: Reinforcement Learning &amp; Fine-Tuning"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> Reinforcement Learning &amp; Fine-Tuning </div> </div> </a> <a href=../paper-review/ class="md-footer__link md-footer__link--next" aria-label="Next: Paper Reviews"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Paper Reviews </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2024 - Learning Notes Collection </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://jackie-jiaqi-yin.github.io target=_blank rel=noopener title="Personal Website" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M277.8 8.6c-12.3-11.4-31.3-11.4-43.5 0l-224 208c-9.6 9-12.8 22.9-8 35.1S18.8 272 32 272h16v176c0 35.3 28.7 64 64 64h288c35.3 0 64-28.7 64-64V272h16c13.2 0 25-8.1 29.8-20.3s1.6-26.2-8-35.1zM240 320h32c26.5 0 48 21.5 48 48v96H192v-96c0-26.5 21.5-48 48-48"/></svg> </a> <a href=https://github.com/jackie-jiaqi-yin target=_blank rel=noopener title="GitHub Profile" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.instant", "navigation.instant.prefetch", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.path", "navigation.indexes", "navigation.top", "navigation.footer", "toc.follow", "toc.integrate", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.tabs.link", "content.tooltips"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": {"AI Agent": "ai-agent", "Evaluation": "evaluation", "Model Compression": "compression", "Prompt Engineering": "prompt", "RAG": "rag", "RLHF": "rlhf", "RecSys": "recsys"}, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.f55a23d4.min.js></script> <script src=../../../js/timeago.min.js></script> <script src=../../../js/timeago_mkdocs_material.js></script> <script src=../../../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>