{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Agent Library\n",
    "\n",
    "Prerequesites:\n",
    "- create a token in hugging face website with read permissions\n",
    "- Request access to the model by following Meta's access requirements\n",
    "- create `.env` file with the token\n",
    "\n",
    "\n",
    "First testing the text generation (other api may use `complete`) and chat feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "# login to the Hugging Face Hub\n",
    "login(token=os.getenv(\"HF_TOKEN\"))\n",
    "client = InferenceClient(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris. The capital of Italy is Rome. The capital of Spain is Madrid. The capital of Germany is Berlin. The capital of the United Kingdom is London. The capital of Australia is Canberra. The capital of China is Beijing. The capital of Japan is Tokyo. The capital of India is New Delhi. The capital of Brazil is Bras√≠lia. The capital of Russia is Moscow. The capital of South Africa is Pretoria. The capital of Egypt is Cairo. The capital of Turkey is Ankara. The\n"
     ]
    }
   ],
   "source": [
    "output = client.text_generation(\n",
    "    \"The capital of France is\",\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the token generation continues until reaching the token limit of 100. This occurs because the model only stops when it predicts an `EOS` (End of Sequence) token.\n",
    "\n",
    "However, when the text template is applied using `template`, it will stop naturally as in a normal chat conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Paris!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "The capital of France is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "output = client.text_generation(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly use chat function and is recommended to use in order to ensure a smooth transition between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris.\n"
     ]
    }
   ],
   "source": [
    "output = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"The capital of France is\"},\n",
    "    ],\n",
    "    stream=False,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "print(output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy agent\n",
    " \n",
    "The agent's system prompt should encompass two critical components:\n",
    "\n",
    "1. **Tool Specifications**\n",
    "    - Detailed descriptions of available tools\n",
    "    - Input/output formats\n",
    "    - Usage constraints and requirements\n",
    "\n",
    "2. **Reasoning Framework**\n",
    "    - Thought: Analyze the task and plan the approach\n",
    "    - Action: Execute the chosen tool or response\n",
    "    - Observation: Process the results and determine next steps\n",
    "\n",
    "This structured approach ensures consistent and effective agent responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = '''\n",
    "    Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "    get_weather: Get the current weather in a given location\n",
    "\n",
    "    The way you use the tools is by specifying a json blob.\n",
    "    Specifically, this json should have an `action` key (with the name of the tool to use) and an `action_input` key (with the input to the tool going here).\n",
    "\n",
    "    The only values that should be in the \"action\" field are:\n",
    "    get_weather: Get the current weather in a given location, args: {\"location\": {\"type\": \"string\"}}\n",
    "    example use : \n",
    "\n",
    "    {{\n",
    "      \"action\": \"get_weather\",\n",
    "      \"action_input\": {\"location\": \"New York\"}\n",
    "    }}\n",
    "\n",
    "    ALWAYS use the following format:\n",
    "\n",
    "    Question: the input question you must answer\n",
    "    Thought: you should always think about one action to take. Only one action at a time in this format:\n",
    "    Action:\n",
    "\n",
    "    $JSON_BLOB (inside markdown cell)\n",
    "\n",
    "    Observation: the result of the action. This Observation is unique, complete, and the source of truth.\n",
    "    ... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $JSON_BLOB must be formatted as markdown and only use a SINGLE action at a time.)\n",
    "\n",
    "    You must always end your output with the following format:\n",
    "\n",
    "    Thought: I now know the final answer\n",
    "    Final Answer: the final answer to the original input question\n",
    "\n",
    "    Now begin! Reminder to ALWAYS use the exact characters `Final Answer:` when you provide a definitive answer.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{SYSTEM_PROMPT}\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "What's the weather in London ?\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather in London?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather in London ?\"},\n",
    "    ]\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", token=os.getenv(\"HF_TOKEN\"))\n",
    "\n",
    "tokenizer.apply_chat_template(messages, tokenize=False,add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
